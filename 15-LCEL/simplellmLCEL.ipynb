{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple LLM Application with LangChain Expression Language (LCEL)\n",
    "\n",
    "This quickstart guides you through building a simple LLM application using LangChain Expression Language (LCEL). Our application will perform a straightforward English-to-French text translation. While this is a relatively simple use case, it effectively demonstrates the power of LCEL for chaining components.\n",
    "\n",
    "You'll get a high-level overview of:\n",
    "\n",
    "* **Using Language Models**: Interacting with both OpenAI and Groq (for open-source models like Gemma).\n",
    "* **Prompt Templates and Output Parsers**: Structuring LLM inputs and extracting clean outputs.\n",
    "* **LangChain Expression Language (LCEL)**: Chaining components together using the intuitive `|` operator.\n",
    "* **Debugging and Tracing with LangSmith**: Observing the execution flow of your application.\n",
    "* **Deploying with LangServe (Conceptual)**: Understanding how to serve your LangChain applications as APIs.\n",
    "\n",
    "## Key Concepts:\n",
    "\n",
    "* **LangChain Expression Language (LCEL)**: A declarative way to compose \"Runnables\" (LangChain's building blocks) into chains using a concise syntax (e.g., `component1 | component2`). LCEL offers benefits like streaming, async support, parallel execution, and easier integration with LangSmith and LangServe.\n",
    "* **`ChatPromptTemplate`**: A LangChain component for creating structured prompts, especially useful for chat models, allowing the definition of system, user, and assistant messages.\n",
    "* **`StrOutputParser`**: A simple LangChain output parser that extracts the string content from an LLM's response.\n",
    "* **Groq**: A fast inference engine that provides access to open-source LLMs (like Gemma, Llama 3) at extremely high speeds via an API.\n",
    "* **LangSmith**: An observability platform for LLM applications that provides detailed traces of your chain's execution, aiding in debugging and performance analysis.\n",
    "* **LangServe**: A library for deploying LangChain applications as REST APIs, making your LLM apps accessible via HTTP endpoints.\n",
    "\n",
    "## Setup: API Keys and Environment Variables\n",
    "\n",
    "Ensure your `.env` file is configured with `OPENAI_API_KEY`, `GROQ_API_KEY`, `LANGCHAIN_API_KEY`, and `LANGCHAIN_PROJECT`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Simple LLM Application with LCEL\n",
    "In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\n",
    "\n",
    "After seeing this video, you'll have a high level overview of:\n",
    "\n",
    "- Using language models\n",
    "\n",
    "- Using PromptTemplates and OutputParsers\n",
    "\n",
    "- Using LangChain Expression Language (LCEL) to chain components together\n",
    "\n",
    "- Debugging and tracing your application using LangSmith\n",
    "\n",
    "- Deploying your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pro/python-learning/Machine Learning/15-LCEL/lcel-env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initialized Groq Model ---\n",
      "client=<groq.resources.chat.completions.Completions object at 0x1100f4f10> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x110101670> model_name='Gemma2-9b-It' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file.\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key from environment variables.\n",
    "# This is necessary for using OpenAI models.\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set Groq API key from environment variables.\n",
    "# This is necessary for using Groq's fast inference models.\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Display a part of the Groq API key to confirm it's loaded (for debugging, don't expose full key).\n",
    "# print(f\"Groq API Key (first 5 chars): {groq_api_key[:5]}*****\")\n",
    "\n",
    "\n",
    "# --- LangSmith Tracking Setup ---\n",
    "# Set LangChain API key for LangSmith.\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "# Enable LangSmith tracing. \"true\" automatically sends traces to LangSmith.\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# Define the project name under which traces will be organized in LangSmith.\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "# Import ChatOpenAI for OpenAI models and ChatGroq for Groq models.\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize a ChatGroq model using 'Gemma2-9b-It'.\n",
    "# Groq provides incredibly fast inference for open-source models.\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)\n",
    "\n",
    "# Display the initialized model object.\n",
    "print(\"--- Initialized Groq Model ---\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Language Models Directly with Messages\n",
    "\n",
    "LLMs often work best with a list of \"messages\" that define roles (System, Human, AI) to provide context and instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Direct Model Invocation Result ---\n",
      "content=\"Hello - Bonjour\\n\\nHow are you? - Comment allez-vous ? \\n\\n(More informal) How are you? - Ça va ?\\n\\n\\nLet me know if you'd like to know more translations!\\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 21, 'total_tokens': 66, 'completion_time': 0.081818182, 'prompt_time': 0.003343189, 'queue_time': 0.249786541, 'total_time': 0.085161371}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--65af6a67-09fd-4054-b3fb-27f69ad9dff7-0' usage_metadata={'input_tokens': 21, 'output_tokens': 45, 'total_tokens': 66}\n"
     ]
    }
   ],
   "source": [
    "# Import message types from langchain_core.messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Define a list of messages.\n",
    "# SystemMessage provides instructions or context to the AI.\n",
    "# HumanMessage provides the user's input.\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English to French\"),\n",
    "    HumanMessage(content=\"Hello How are you?\")\n",
    "]\n",
    "\n",
    "# Invoke the model directly with the list of messages.\n",
    "# The 'invoke' method sends the messages to the LLM and returns its response.\n",
    "result = model.invoke(messages)\n",
    "\n",
    "# Print the entire response object received from the LLM.\n",
    "# This object contains not just the content but also metadata.\n",
    "print(\"--- Direct Model Invocation Result ---\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Output with StrOutputParser\n",
    "\n",
    "The raw `result` from `model.invoke` is a complex object. Often, we only need the generated text content. `StrOutputParser` extracts this content as a simple string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Parsed Output (String) ---\n",
      "Hello - Bonjour\n",
      "\n",
      "How are you? - Comment allez-vous ? \n",
      "\n",
      "(More informal) How are you? - Ça va ?\n",
      "\n",
      "\n",
      "Let me know if you'd like to know more translations!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import StrOutputParser to extract the string content from LLM responses.\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the string output parser.\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Invoke the parser with the raw LLM result to get just the string content.\n",
    "parsed_result = parser.invoke(result)\n",
    "\n",
    "# Print the parsed string result.\n",
    "print(\"--- Parsed Output (String) ---\")\n",
    "print(parsed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LCEL: Chaining Components\n",
    "\n",
    "LangChain Expression Language (LCEL) allows you to chain Runnables together using the `|` (pipe) operator, making your code concise and readable. The output of one component becomes the input of the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Result from Chained Components (model | parser) ---\n",
      "Here are a couple of ways to say \"Hello, How are you?\" in French:\n",
      "\n",
      "* **Bonjour, comment allez-vous ?** (Formal)\n",
      "* **Salut, comment vas-tu ?** (Informal) \n",
      "\n",
      "\n",
      "Let me know if you'd like to see other variations! 😊\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a simple chain using LCEL: model | parser.\n",
    "# This means the messages go to the model, and the model's output then goes to the parser.\n",
    "chain = model | parser\n",
    "\n",
    "# Invoke the chain with the same list of messages.\n",
    "# The result will directly be the parsed string output.\n",
    "chained_result = chain.invoke(messages)\n",
    "\n",
    "# Print the result from the chained components.\n",
    "print(\"--- Result from Chained Components (model | parser) ---\")\n",
    "print(chained_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates for Dynamic Inputs\n",
    "\n",
    "`ChatPromptTemplate` allows you to create dynamic prompts with placeholders, making your applications flexible. Instead of hardcoding messages, you can define a template and fill in variables at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Prompt Template Result (Converted to Messages) ---\n",
      "[SystemMessage(content='Translate the following into French:', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Import ChatPromptTemplate for creating dynamic prompts.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a generic system template with a placeholder for the target language.\n",
    "generic_template = \"Translate the following into {language}:\"\n",
    "\n",
    "# Create a ChatPromptTemplate from messages.\n",
    "# The system message uses the generic_template.\n",
    "# The user message has a placeholder \"{text}\" for the content to be translated.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", generic_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "# Invoke the prompt template with specific values for the placeholders.\n",
    "# The `invoke` method returns a `PromptValue` object, which can be converted to messages.\n",
    "result_prompt = prompt.invoke({\"language\": \"French\", \"text\": \"Hello\"})\n",
    "\n",
    "# Convert the PromptValue object to a list of messages.\n",
    "# This shows how the template is filled out into actual messages.\n",
    "print(\"--- Prompt Template Result (Converted to Messages) ---\")\n",
    "print(result_prompt.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Together Components with LCEL (Full Translation Application)\n",
    "\n",
    "Now, let's put it all together: `PromptTemplate | Model | OutputParser`. This forms a complete, simple LLM application that takes dynamic inputs, processes them with the LLM, and provides a clean string output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Translated Output from Full Chain ---\n",
      "Here are a few ways to translate \"How are you doing today?\" into French:\n",
      "\n",
      "* **Comment vas-tu aujourd'hui ?** (This is the most common and informal way to ask.)\n",
      "* **Comment allez-vous aujourd'hui ?** (This is a more formal way to ask.)\n",
      "* **Ça va aujourd'hui ?** (This is a very casual way to ask.)\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like to see more variations!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Chaining together components with LCEL\n",
    "# Create the full chain: prompt | model | parser.\n",
    "# 1. 'prompt' formats the input into messages.\n",
    "# 2. 'model' receives the messages and generates a response.\n",
    "# 3. 'parser' extracts the string content from the model's response.\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# Invoke the full chain with a dictionary containing values for the prompt's placeholders.\n",
    "# The result is the final translated string.\n",
    "final_translation = chain.invoke({\"language\": \"French\", \"text\": \"How are you doing today?\"})\n",
    "\n",
    "# Print the final translated output.\n",
    "print(\"--- Final Translated Output from Full Chain ---\")\n",
    "print(final_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith and LangServe Overview\n",
    "\n",
    "### Debugging and Tracing Your Application with LangSmith\n",
    "\n",
    "Since you've set up `LANGCHAIN_TRACING_V2=\"true\"` and your `LANGCHAIN_API_KEY` and `LANGCHAIN_PROJECT` environment variables, every `invoke` call on your `chain` automatically sends a trace to LangSmith.\n",
    "\n",
    "**To check your usage and traces in LangSmith:**\n",
    "\n",
    "1.  **Go to the LangSmith UI**: Open your web browser and navigate to [smith.langchain.com](https://smith.langchain.com/).\n",
    "2.  **Log in**: Use the credentials you signed up with.\n",
    "3.  **Navigate to your Project**: On the left sidebar, click on \"Projects\". You should see a project listed with the name you set in your `.env` file (e.g., `my-translation-app`).\n",
    "4.  **View Runs**: Click on your project name. You will see a list of \"Runs\" (each invocation of your `chain` or `model`).\n",
    "5.  **Explore a Trace**: Click on any individual run. You'll see a detailed \"trace\" showing each step of your chain (e.g., how the prompt was formatted, the LLM call, and the parsing step). You can inspect inputs, outputs, latency, and token usage for each component. This is incredibly powerful for debugging and optimizing your LLM applications.\n",
    "\n",
    "### Deploying Your Application with LangServe (Conceptual)\n",
    "\n",
    "LangServe makes it easy to deploy your LangChain `Runnable`s (like our `chain`) as a REST API. This allows other applications or users to interact with your LLM application via HTTP requests.\n",
    "\n",
    "While we are not doing a full deployment in this quickstart, conceptually, if you had this chain defined in a Python script (e.g., `app.py`), you could expose it using LangServe:\n",
    "\n",
    "```python\n",
    "# # Example of what an app.py for LangServe might look like\n",
    "# from fastapi import FastAPI\n",
    "# from langserve import add_routes\n",
    "#\n",
    "# # Assuming 'chain' is defined as in this notebook\n",
    "# # from your_module import chain\n",
    "#\n",
    "# app = FastAPI(\n",
    "#     title=\"Translation App\",\n",
    "#     version=\"1.0\",\n",
    "#     description=\"A simple API for English to French translation.\"\n",
    "# )\n",
    "#\n",
    "# add_routes(app, chain, path=\"/translate\")\n",
    "#\n",
    "# # To run this, you would save it as app.py and run:\n",
    "# # uvicorn app:app --reload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcel-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
