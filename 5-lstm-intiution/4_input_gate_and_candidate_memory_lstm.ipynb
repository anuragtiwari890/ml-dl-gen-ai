{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissecting the LSTM Input Gate and Candidate Memory: How LSTMs Add New Information\n",
    "\n",
    "### This Jupyter Notebook continues the deep dive into the Long Short-Term Memory (LSTM) architecture, focusing on the second and third crucial components: the Input Gate and the Candidate Memory. These two parts work in conjunction to decide what *new* information from the current input should be added to the LSTM's long-term memory (cell state).\n",
    "\n",
    "## high level understanding - \n",
    "\n",
    "![Alt text for the image](images/input_gate_and_candidate_memory-1.png)\n",
    "\n",
    "![Alt text for the image](images/input_gate_and_candidate_memory.png)\n",
    "\n",
    "\n",
    "## Key Learning Objectives:\n",
    "\n",
    "### 1. **Recap: Forget Gate Equation**\n",
    "* Briefly revisit the mathematical equation for the Forget Gate ($f_t$):\n",
    "    * $f_t = \\sigma(W_f \\cdot [H_{t-1}, X_t] + b_f)$\n",
    "    * Where:\n",
    "        * $\\sigma$ is the sigmoid activation function.\n",
    "        * $W_f$ is the weight matrix for the Forget Gate.\n",
    "        * $[H_{t-1}, X_t]$ is the concatenation of the previous hidden state and current input.\n",
    "        * $b_f$ is the bias vector for the Forget Gate.\n",
    "* Understand that this equation represents the internal neural network operation (linear transformation followed by sigmoid activation) that determines the forget factors.\n",
    "\n",
    "### 2. **Focus: The Input Gate's Role**\n",
    "\n",
    "* The **Input Gate** decides which parts of the *new information* generated from the current input and previous hidden state are important enough to be *added* to the cell state. It acts as a filter for new information.\n",
    "\n",
    "#### **2.1. Input Gate Operation**\n",
    "* **Inputs**: Similar to the Forget Gate, it takes the concatenated $H_{t-1}$ and $X_t$.\n",
    "* **Neural Network Layer + Sigmoid Activation**: This input is passed through another neural network layer (with its own weights $W_i$ and bias $b_i$) and then through a sigmoid activation function.\n",
    "    * **Mathematical Representation**: $i_t = \\sigma(W_i \\cdot [H_{t-1}, X_t] + b_i)$\n",
    "    * The output, $i_t$, is a vector of values between 0 and 1, indicating the \"importance\" of each corresponding element in the *candidate memory* (discussed next).\n",
    "\n",
    "### 3. **Focus: The Candidate Memory's Role**\n",
    "\n",
    "* The **Candidate Memory** (often denoted as $\\tilde{C}_t$) creates a *new candidate* version of the cell state. This candidate state contains potential new information that *could* be added to the long-term memory.\n",
    "\n",
    "#### **3.1. Candidate Memory Operation**\n",
    "* **Inputs**: Also takes the concatenated $H_{t-1}$ and $X_t$.\n",
    "* **Neural Network Layer + Tanh Activation**: This input is passed through yet another neural network layer (with its own weights $W_c$ and bias $b_c$) and then through a **tanh activation function**.\n",
    "    * **Mathematical Representation**: $\\tilde{C}_t = \\tanh(W_c \\cdot [H_{t-1}, X_t] + b_c)$\n",
    "    * The tanh function squashes values between -1 and 1, producing a vector that represents the \"candidate\" new information.\n",
    "\n",
    "### 4. **Combining Input Gate and Candidate Memory**\n",
    "\n",
    "* The outputs of the Input Gate ($i_t$) and the Candidate Memory ($\\tilde{C}_t$) are combined via **point-wise multiplication ($i_t \\times \\tilde{C}_t$)**.\n",
    "* **Purpose**: This multiplication acts as a filter. $i_t$ (from the sigmoid) determines how much of the new candidate information ($\\tilde{C}_t$) is actually allowed to pass through and potentially be added to the cell state.\n",
    "    * If an element in $i_t$ is close to 0, the corresponding new candidate information is largely ignored.\n",
    "    * If an element in $i_t$ is close to 1, the corresponding new candidate information is fully considered.\n",
    "\n",
    "### 5. **Updating the Cell State ($C_t$)**\n",
    "\n",
    "* The final step in updating the long-term memory cell state ($C_t$) for the current time step involves combining the results from the Forget Gate and the Input Gate/Candidate Memory.\n",
    "* **Mathematical Representation**: $C_t = (f_t \\times C_{t-1}) + (i_t \\times \\tilde{C}_t)$\n",
    "* **Explanation**:\n",
    "    1.  $(f_t \\times C_{t-1})$: This part represents the *filtered* old memory – what the Forget Gate decided to keep from $C_{t-1}$.\n",
    "    2.  $(i_t \\times \\tilde{C}_t)$: This part represents the *new filtered information* – what the Input Gate decided to add from the candidate memory.\n",
    "    3.  These two results are then combined via **point-wise addition**. This effectively updates the cell state by removing old irrelevant information and adding new relevant information, allowing the LSTM to maintain relevant context over long sequences.\n",
    "\n",
    "### Conclusion:\n",
    "The Input Gate and Candidate Memory, working in tandem with the Forget Gate, are crucial for LSTMs' ability to learn long-term dependencies. They meticulously control what new information enters the long-term memory cell state, ensuring that only contextually relevant data is retained and passed forward, thereby mitigating the vanishing gradient problem.\n",
    "\n",
    "**Next Video**: The next lecture will cover the final component, the **Output Gate**, and then explain how the entire LSTM unit works together to produce the final hidden state."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
