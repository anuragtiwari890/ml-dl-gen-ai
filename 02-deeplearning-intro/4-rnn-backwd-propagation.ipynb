{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Lecture: RNN Backpropagation Through Time (BPTT)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "In this lecture, we continue our study of **Recurrent Neural Networks (RNNs)** by focusing on **Backpropagation Through Time (BPTT)** ‚Äî the fundamental algorithm by which RNNs learn from sequential data. Specifically, we'll cover:\n",
    "\n",
    "- A quick recap of **forward propagation** in simple RNNs.\n",
    "- Understanding how RNNs are **unfolded through time** to visualize the computational graph.\n",
    "- Detailed explanation of **weight updates** using backpropagation for different types of weights.\n",
    "- Understanding the application of the **chain rule** and gradient flow in a recurrent setting.\n",
    "- The role of **gradient descent** in minimizing the loss function for sequence learning.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Forward Propagation in a Simple RNN\n",
    "\n",
    "Before understanding backpropagation, let‚Äôs briefly revisit the **forward pass**, where information flows from input to output.\n",
    "\n",
    "### 1. **Basic Architecture & Unfolding in Time**\n",
    "\n",
    "A simple RNN processes sequential data by taking an input $x_t$ at each time step $t$ and combining it with a hidden state $h_{t-1}$ from the previous time step to compute a new hidden state $h_t$. This hidden state can then be used to produce an output $y_t$. The recurrent nature means the same set of weights are reused across all time steps.\n",
    "\n",
    "Let's consider a sequence with $T$ time steps (e.g., $T=3$ words in a sentence). The RNN \"unfolds\" over these time steps:\n",
    "\n",
    "* **Input sequence:** $x_1, x_2, \\dots, x_T$ (e.g., $x_{i1}, x_{i2}, x_{i3}$)\n",
    "* **Hidden states:** $h_0$ (initial), $h_1, h_2, \\dots, h_T$\n",
    "* **Outputs (optional at each step):** $\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_T$\n",
    "* **Final output (if only one prediction at the end):** $\\hat{y}_T$\n",
    "\n",
    "### 2. **Forward Computation at Each Time Step**\n",
    "\n",
    "At each time step $t$ (from $t=1$ to $T$), the hidden state $h_t$ is computed. For the first step ($t=1$), $h_{t-1}$ is the initial hidden state $h_0$.\n",
    "\n",
    "The computation involves multiplying inputs and previous hidden states by their respective weights, adding a bias, and then applying a non-linear activation function.\n",
    "\n",
    "* **Hidden State Calculation:**\n",
    "    $$h_t = \\text{Activation}_h(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$$\n",
    "    * **$x_t$**: The input vector at time step $t$.\n",
    "    * **$W_{xh}$**: The **input-to-hidden weight matrix**. It transforms the current input into the hidden state space.\n",
    "    * **$h_{t-1}$**: The hidden state vector from the previous time step. For $t=1$, this is $h_0$, the initial hidden state.\n",
    "    * **$W_{hh}$**: The **hidden-to-hidden (recurrent) weight matrix**. It captures the influence of the previous hidden state on the current one.\n",
    "    * **$b_h$**: The **hidden layer bias vector**.\n",
    "    * **$\\text{Activation}_h(\\cdot)$**: A non-linear activation function (e.g., $\\tanh$, ReLU). The lecture mentions $\\tanh$.\n",
    "\n",
    "* **Output Calculation (if a prediction is made at each step, or at the final step $T$):**\n",
    "    $$ \\hat{y}_t = \\text{Activation}_y(W_{hy}h_t + b_y) $$\n",
    "    * **$\\hat{y}_t$**: The predicted output vector at time step $t$.\n",
    "    * **$W_{hy}$**: The **hidden-to-output weight matrix**. It transforms the current hidden state into the output space.\n",
    "    * **$b_y$**: The **output layer bias vector**.\n",
    "    * **$\\text{Activation}_y(\\cdot)$**: An activation function suitable for the output task (e.g., $\\text{sigmoid}$ for binary classification, $\\text{softmax}$ for multi-class classification). The lecture uses $\\text{sigmoid}$ for the final output.\n",
    "\n",
    "For the example given in the lecture with a final output $\\hat{y}$ derived from $h_3$:\n",
    "$$ \\hat{y} = \\text{sigmoid}(W_{hy}h_3 + b_y) $$\n",
    "\n",
    "---\n",
    "\n",
    "## üîô Backpropagation Through Time (BPTT)\n",
    "\n",
    "After the forward pass, we calculate a **Loss Function** $\\mathcal{L}$ that quantifies the difference between our network's predictions $\\hat{y}$ (or $\\hat{y}_t$) and the true target values $y$ (or $y_t$).\n",
    "\n",
    "$$ \\mathcal{L} = \\text{Loss}(y, \\hat{y}) $$\n",
    "\n",
    "Our primary goal during training is to **minimize this loss**. This is achieved by iteratively adjusting the network's weights and biases ($W_{xh}, W_{hh}, W_{hy}, b_h, b_y$). We use **gradient descent** (or its variants) for this optimization.\n",
    "\n",
    "The general weight update rule for gradient descent is:\n",
    "$$ W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{old}}} $$\n",
    "* **$W_{\\text{new}}$**: The updated weight matrix.\n",
    "* **$W_{\\text{old}}$**: The current weight matrix before update.\n",
    "* **$\\eta$**: The **learning rate**, a small positive scalar hyperparameter that controls the step size of the update.\n",
    "* **$\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{old}}}$**: The **gradient** of the loss function with respect to the weight $W_{\\text{old}}$. This term tells us the direction and magnitude to change $W_{\\text{old}}$ to decrease the loss.\n",
    "\n",
    "In RNNs, calculating these gradients involves **Backpropagation Through Time (BPTT)**, which is essentially applying the standard backpropagation algorithm to the unfolded computational graph of the RNN. Since weights like $W_{xh}$ and $W_{hh}$ are shared across all time steps, their gradients must be accumulated over time.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Step-by-Step Weight Updates\n",
    "\n",
    "Let's detail how the gradients are calculated for each set of weights.\n",
    "\n",
    "### 1. üéØ Updating Output Weights ($W_{hy}$) and Bias ($b_y$)\n",
    "\n",
    "The output weights and bias are typically only involved in the final step of generating a prediction from a hidden state. Assuming a loss is computed at the final time step $T$ based on $\\hat{y}_T$:\n",
    "\n",
    "* **Gradient with respect to $W_{hy}$:**\n",
    "    The loss $\\mathcal{L}$ depends on $\\hat{y}_T$, and $\\hat{y}_T$ directly depends on $W_{hy}$.\n",
    "    Using the chain rule:\n",
    "    $$ \\frac{\\partial \\mathcal{L}}{\\partial W_{hy}} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_T} \\cdot \\frac{\\partial \\hat{y}_T}{\\partial W_{hy}} $$\n",
    "    * $\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_T}$: How much the loss changes with respect to a change in the final predicted output. This depends on the specific loss function used.\n",
    "    * $\\frac{\\partial \\hat{y}_T}{\\partial W_{hy}}$: How much the final predicted output changes with respect to a change in $W_{hy}$. This involves the derivative of the output activation function $\\text{Activation}_y'(\\cdot)$ and the hidden state $h_T$.\n",
    "\n",
    "* **Update Rule for $W_{hy}$:**\n",
    "    $$ W_{hy}^{\\text{new}} = W_{hy}^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_{hy}} $$\n",
    "\n",
    "* **Similarly for $b_y$:**\n",
    "    $$ \\frac{\\partial \\mathcal{L}}{\\partial b_y} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_T} \\cdot \\frac{\\partial \\hat{y}_T}{\\partial b_y} $$\n",
    "    $$ b_y^{\\text{new}} = b_y^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b_y} $$\n",
    "\n",
    "These gradients are relatively straightforward as they do not involve propagating the error signal backward through multiple recurrent time steps.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üîÅ Updating Recurrent (Hidden-to-Hidden) Weights ($W_{hh}$) and Bias ($b_h$)\n",
    "\n",
    "The recurrent weight $W_{hh}$ and bias $b_h$ are crucial because they are reused at **every single time step** ($t = 1, \\dots, T$) to connect hidden states. Therefore, their gradients must account for their influence on the loss through all time steps. This involves summing the gradients from each point in time where they were used.\n",
    "\n",
    "* **Total Gradient for $W_{hh}$:**\n",
    "    $$ \\frac{\\partial \\mathcal{L}}{\\partial W_{hh}} = \\sum_{k=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial W_{hh} \\text{ at time } k} $$\n",
    "    Here, $\\frac{\\partial \\mathcal{L}}{\\partial W_{hh} \\text{ at time } k}$ represents the specific contribution of $W_{hh}$ at time step $k$ to the total loss.\n",
    "\n",
    "* **Chain Rule for a single contribution (e.g., $W_{hh}$ at time $k$ affecting the final loss at $T$):**\n",
    "    The path of influence goes: $W_{hh} \\to h_k \\to h_{k+1} \\to \\dots \\to h_T \\to \\hat{y}_T \\to \\mathcal{L}$.\n",
    "    This translates to a product of derivatives:\n",
    "    $$ \\frac{\\partial \\mathcal{L}}{\\partial W_{hh} \\text{ at time } k} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_T} \\cdot \\frac{\\partial \\hat{y}_T}{\\partial h_T} \\cdot \\frac{\\partial h_T}{\\partial h_{T-1}} \\cdot \\ldots \\cdot \\frac{\\partial h_{k+1}}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial W_{hh}} $$\n",
    "    * **$\\frac{\\partial h_j}{\\partial h_{j-1}}$**: This is the critical recurrent term. As derived in the previous section (and will be expanded below), it is typically $\\text{Activation}_h'(z_j) \\cdot W_{hh}$.\n",
    "    * **$\\frac{\\partial h_k}{\\partial W_{hh}}$**: How the hidden state at time $k$ changes with respect to $W_{hh}$. This term depends on $h_{k-1}$ and $\\text{Activation}_h'(\\cdot)$.\n",
    "\n",
    "* **Update Rule for $W_{hh}$:**\n",
    "    $$ W_{hh}^{\\text{new}} = W_{hh}^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_{hh}} $$\n",
    "\n",
    "* **Similarly for $b_h$:**\n",
    "    $$ \\frac{\\partial \\mathcal{L}}{\\partial b_h} = \\sum_{k=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial b_h \\text{ at time } k} $$\n",
    "    The chain rule for each contribution is similar to $W_{hh}$ but ends with $\\frac{\\partial h_k}{\\partial b_h}$.\n",
    "    $$ b_h^{\\text{new}} = b_h^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b_h} $$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üì• Updating Input Weights ($W_{xh}$)\n",
    "\n",
    "The input weights $W_{xh}$ are also shared and used at each time step to incorporate the current input $x_t$ into the hidden state $h_t$. Their gradients must also be accumulated over time.\n",
    "\n",
    "* **Total Gradient for $W_{xh}$:**\n",
    "    $$ \\frac{\\partial \\mathcal{L}}{\\partial W_{xh}} = \\sum_{k=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial W_{xh} \\text{ at time } k} $$\n",
    "\n",
    "* **Chain Rule for a single contribution (e.g., $W_{xh}$ at time $k$ affecting the final loss at $T$):**\n",
    "    The path of influence goes: $W_{xh} \\to h_k \\to h_{k+1} \\to \\dots \\to h_T \\to \\hat{y}_T \\to \\mathcal{L}$.\n",
    "    $$ \\frac{\\partial \\mathcal{L}}{\\partial W_{xh} \\text{ at time } k} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_T} \\cdot \\frac{\\partial \\hat{y}_T}{\\partial h_T} \\cdot \\frac{\\partial h_T}{\\partial h_{T-1}} \\cdot \\ldots \\cdot \\frac{\\partial h_{k+1}}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial W_{xh}} $$\n",
    "    * **$\\frac{\\partial h_k}{\\partial W_{xh}}$**: How the hidden state at time $k$ changes with respect to $W_{xh}$. This term depends on $x_k$ and $\\text{Activation}_h'(\\cdot)$.\n",
    "\n",
    "* **Update Rule for $W_{xh}$:**\n",
    "    $$ W_{xh}^{\\text{new}} = W_{xh}^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_{xh}} $$\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Decomposing the Recurrent Gradient Term: $\\frac{\\partial h_j}{\\partial h_{j-1}}$\n",
    "\n",
    "This term is the cornerstone of understanding how gradients flow (and potentially vanish or explode) in RNNs. It represents how sensitive the hidden state at time $j$ is to the hidden state at time $j-1$.\n",
    "\n",
    "Recall the hidden state calculation:\n",
    "$$ h_j = \\text{Activation}_h(W_{xh}x_j + W_{hh}h_{j-1} + b_h) $$\n",
    "\n",
    "Let $z_j = W_{xh}x_j + W_{hh}h_{j-1} + b_h$ be the net input to the activation function at time $j$.\n",
    "\n",
    "Using the chain rule:\n",
    "$$ \\frac{\\partial h_j}{\\partial h_{j-1}} = \\frac{d \\text{Activation}_h(z_j)}{d z_j} \\cdot \\frac{\\partial z_j}{\\partial h_{j-1}} $$\n",
    "\n",
    "* **Derivative of the Activation Function:** $\\frac{d \\text{Activation}_h(z_j)}{d z_j}$ is the derivative of the activation function, evaluated at $z_j$. For $\\tanh(z)$, the derivative is $1 - \\tanh^2(z)$. For $\\text{sigmoid}(z)$, the derivative is $\\text{sigmoid}(z)(1 - \\text{sigmoid}(z))$.\n",
    "\n",
    "* **Derivative of the Net Input:** $\\frac{\\partial z_j}{\\partial h_{j-1}}$:\n",
    "    $$ \\frac{\\partial}{\\partial h_{j-1}} (W_{xh}x_j + W_{hh}h_{j-1} + b_h) $$\n",
    "    Since $W_{xh}x_j$ and $b_h$ are constant with respect to $h_{j-1}$, their derivatives are zero. The derivative of $W_{hh}h_{j-1}$ with respect to $h_{j-1}$ is $W_{hh}$.\n",
    "\n",
    "Therefore, the crucial recurrent term is:\n",
    "$$ \\frac{\\partial h_j}{\\partial h_{j-1}} = \\text{Activation}_h'(z_j) \\cdot W_{hh} $$\n",
    "Where $\\text{Activation}_h'(z_j)$ is the element-wise derivative of the activation function.\n",
    "\n",
    "This is the term that gets multiplied repeatedly in the gradient calculation for earlier time steps. If its magnitude is consistently less than 1 (as is often the case with sigmoid and tanh for inputs far from zero, and small $W_{hh}$ values), the gradient will **vanish**. If its magnitude is consistently greater than 1, the gradient will **explode**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ Gradient Descent & Convergence\n",
    "\n",
    "The entire process of BPTT, calculating these complex gradients, and then applying the weight update rules using $\\text{gradient descent}$ (or its variants like Adam, RMSprop) is performed iteratively over many training epochs.\n",
    "\n",
    "The ultimate goal of this iterative optimization process is to:\n",
    "* **Minimize the loss function**: By moving in the direction opposite to the gradient, we steadily decrease the error.\n",
    "* **Reach a global minimum (or a good local minimum)**: This signifies that the network's weights have been optimized to represent the underlying patterns in the sequential data as accurately as possible.\n",
    "* **Enable the RNN to learn accurate predictions**: A minimized loss indicates that the RNN has effectively learned the long-term and short-term dependencies necessary to make precise outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì Why Do We Need BPTT?\n",
    "\n",
    "BPTT is indispensable for training RNNs for several key reasons:\n",
    "\n",
    "* **Sequential Learning**: RNNs are specifically designed for sequential data where the order and historical context matter. BPTT provides the mechanism for the model to learn these temporal dependencies by propagating error signals across time.\n",
    "* **Shared Parameters**: Unlike feedforward networks, RNNs reuse the same set of recurrent weights ($W_{hh}$) and input weights ($W_{xh}$) at every time step. BPTT correctly accumulates the gradients from all these instances of usage to ensure effective updates.\n",
    "* **Memory in Learning**: The hidden state acts as the RNN's \"memory.\" BPTT allows the error signal to flow back into this memory mechanism, enabling the model to adjust how it retains and forgets information from previous steps in a principled and optimized way.\n",
    "* **Effective Optimization**: Without BPTT, an RNN would not be able to adjust its weights to minimize errors that depend on the entire sequence. It would be akin to training a feedforward network without backpropagation.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Conclusion\n",
    "\n",
    "In this lecture, we've taken a deep dive into **Backpropagation Through Time (BPTT)**, the core learning algorithm for simple Recurrent Neural Networks. We've:\n",
    "\n",
    "* Reviewed the computational steps of **forward propagation** in an unfolded RNN.\n",
    "* Understood how **gradients are calculated** for the output weights ($W_{hy}$), recurrent weights ($W_{hh}$), and input weights ($W_{xh}$) using the **chain rule**, accounting for their shared nature across time.\n",
    "* Examined the crucial recurrent derivative term $\\frac{\\partial h_j}{\\partial h_{j-1}}$ which explains the **vanishing (and exploding) gradient problems** in simple RNNs.\n",
    "* Reinforced the role of **gradient descent** in iteratively adjusting these weights to minimize the loss and enable effective sequence learning.\n",
    "\n",
    "> üîç BPTT is critical in making RNNs truly learn from sequential patterns. However, its very mechanism (repeated multiplication of derivatives) also reveals the inherent limitations of simple RNNs, particularly the vanishing gradient problem. This understanding forms the foundation for appreciating the advancements brought by more sophisticated architectures like LSTMs and GRUs, which were specifically designed to overcome these challenges and enable learning of truly long-term dependencies.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
