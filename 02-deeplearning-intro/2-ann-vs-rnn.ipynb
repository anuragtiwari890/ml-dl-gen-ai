{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Sentiment Analysis with ANN and RNN\n",
    "\n",
    "Hello everyone, and welcome back.\n",
    "\n",
    "Today, we're continuing our discussion on **Natural Language Processing (NLP) in Deep Learning**.  \n",
    "In the last video, I asked a question:\n",
    "\n",
    "> Can we use an Artificial Neural Network (ANN) to solve problems related to sequential data?\n",
    "\n",
    "Let’s start by understanding the **problem statement**.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Statement: Sentiment Analysis\n",
    "\n",
    "We have a dataset with **text inputs** and corresponding **output labels**.  \n",
    "Our goal is to perform **sentiment analysis**, where:\n",
    "\n",
    "- Input: A sentence (e.g., \"The food is good.\")\n",
    "- Output: A sentiment label (`1` for positive, `0` for negative)\n",
    "\n",
    "### Example Dataset\n",
    "\n",
    "| Sentence                  | Label |\n",
    "|---------------------------|-------|\n",
    "| The food is good.         | 1     |\n",
    "| The food is bad.          | 0     |\n",
    "| The food is not good.     | 0     |\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Text Preprocessing\n",
    "\n",
    "To use ANN or any model, we first need to convert text into vectors.\n",
    "\n",
    "### Common Vectorization Techniques:\n",
    "- **Bag of Words**\n",
    "- **One-Hot Encoding**\n",
    "- **TF-IDF**\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Bag of Words\n",
    "\n",
    "Let’s extract unique words from the dataset (ignoring stop words like \"is\", \"the\"):\n",
    "\n",
    "- **Vocabulary:** `food`, `good`, `bad`, `not`  \n",
    "- **Vocabulary size:** 4\n",
    "\n",
    "Now, we convert each sentence into a vector using Bag of Words:\n",
    "\n",
    "vector would lookg like this -> `[food, good, bad, not]` -> for Sentence [The **food** is **good**] -> `[1, 1, 0, 0]`\n",
    "\n",
    "| Sentence                  | Vector     |\n",
    "|---------------------------|------------|\n",
    "| The **food** is **good**          | [1, 1, 0, 0] |\n",
    "| The **food** is **bad**           | [1, 0, 1, 0] |\n",
    "| The **food** is **not** **good**      | [1, 1, 0, 1] |\n",
    "\n",
    "---\n",
    "\n",
    "## ANN Architecture for Text\n",
    "\n",
    "Let’s consider a simple **Artificial Neural Network**:\n",
    "\n",
    "- Input size: 4 (from Bag of Words)\n",
    "- Hidden layer: 3 neurons\n",
    "- Output layer: 1 neuron (binary classification)\n",
    "\n",
    "But there are two major **problems** when using ANN for text:\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 1: Loss of Sequence\n",
    "\n",
    "In text, **order matters** — e.g., \"not good\" ≠ \"good\".  \n",
    "Bag of Words ignores order. So:\n",
    "\n",
    "- Sequence is lost\n",
    "- Context is lost\n",
    "- Sentence meaning is diluted\n",
    "\n",
    "---\n",
    "\n",
    "## Problem 2: Simultaneous Input\n",
    "\n",
    "In ANN, all inputs (words) are fed **at once**. But for text:\n",
    "\n",
    "- Better approach: **One word at a time**\n",
    "- Example use-case: **Language translation**\n",
    "  - Each word builds upon the previous one\n",
    "  - Sequence must be preserved\n",
    "\n",
    "---\n",
    "\n",
    "## Why ANN Fails for Sequential Data\n",
    "\n",
    "- ANN gives all words at once → **No sense of time or order**\n",
    "- Backpropagation happens after the entire sentence is processed\n",
    "- No memory of previous words\n",
    "\n",
    "---\n",
    "\n",
    "## Enter: Recurrent Neural Networks (RNN)\n",
    "\n",
    "To solve this, we use a **Recurrent Neural Network (RNN)**.\n",
    "\n",
    "### Key Characteristics:\n",
    "- Maintains **state** (memory of previous words)\n",
    "- Takes **one word at a time** (across timestamps)\n",
    "- Preserves **context and sequence**\n",
    "\n",
    "---\n",
    "\n",
    "## RNN Architecture Overview\n",
    "\n",
    "### High-Level Structure:\n",
    "- Input → Hidden Layer → Output\n",
    "- But now: **Feedback loop in the hidden layer**\n",
    "  - Hidden output is sent back to itself at next timestamp\n",
    "  - Allows context to carry forward\n",
    "\n",
    "---\n",
    "\n",
    "## RNN vs ANN: Architecture Comparison\n",
    "\n",
    "| Feature            | ANN                          | RNN                                      |\n",
    "|--------------------|-------------------------------|-------------------------------------------|\n",
    "| Input handling     | All words at once             | One word at a time (timestamped)         |\n",
    "| Sequence memory    | None                          | Maintains previous context               |\n",
    "| Use-case support   | Static data                   | Sequential/time-dependent data           |\n",
    "\n",
    "---\n",
    "\n",
    "## RNN Example: Step-by-Step Input\n",
    "\n",
    "Suppose the sentence is: **\"The food is good\"**\n",
    "\n",
    "We label each word:\n",
    "\n",
    "- X₁₁ → \"The\"\n",
    "- X₁₂ → \"food\"\n",
    "- X₁₃ → \"is\"\n",
    "- X₁₄ → \"good\"\n",
    "\n",
    "At each timestamp `t`, one word is passed:\n",
    "\n",
    "- t = 1 → X₁₁\n",
    "- t = 2 → X₁₂\n",
    "- t = 3 → X₁₃\n",
    "- t = 4 → X₁₄\n",
    "\n",
    "Each hidden layer output carries the **previous context** to the next step.\n",
    "\n",
    "---\n",
    "\n",
    "## Unrolling an RNN\n",
    "\n",
    "Visual representation of an unrolled RNN over 4 time steps:\n",
    "\n",
    "x₁₁ → h₁ →\n",
    "\n",
    "↓\n",
    "\n",
    "x₁₂ → h₂ →\n",
    "\n",
    "↓\n",
    "\n",
    "x₁₃ → h₃ →\n",
    "\n",
    "↓\n",
    "\n",
    "x₁₄ → h₄ → ŷ\n",
    "\n",
    "\n",
    "Where:\n",
    "- x = input at timestamp t\n",
    "- h = hidden state\n",
    "- ŷ = final prediction\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- ANN is **not suitable** for sequential data like text\n",
    "  - Lacks sequence awareness\n",
    "  - Inputs processed all at once\n",
    "\n",
    "- RNN is **designed for sequential data**\n",
    "  - Maintains **order and context**\n",
    "  - Processes **one word at a time**\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next section/video, we’ll break down the **RNN architecture** in more detail and see how **forward propagation** and **backpropagation through time (BPTT)** works.\n",
    "\n",
    "Stay tuned and thank you!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
