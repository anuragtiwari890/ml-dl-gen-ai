{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Stores and Retrievers: The Foundation of RAG\n",
    "\n",
    "This video tutorial will familiarize you with LangChain's `VectorStore` and `Retriever` abstractions. These fundamental components are designed to facilitate the retrieval of data from various sources, especially vector databases, for seamless integration with Large Language Model (LLM) workflows. They are paramount for applications that need to fetch external data to augment the LLM's reasoning capabilities, as seen in Retrieval-Augmented Generation (RAG) systems.\n",
    "\n",
    "We will cover:\n",
    "* **Documents**: The basic unit of data in LangChain.\n",
    "* **Vector Stores**: Databases optimized for storing and querying vector embeddings.\n",
    "* **Retrievers**: LangChain's standardized interface for fetching relevant documents.\n",
    "* **A Simple RAG Example**: Combining these components into an end-to-end RAG chain.\n",
    "\n",
    "## Key Concepts:\n",
    "\n",
    "* **Retrieval-Augmented Generation (RAG)**: A technique where an LLM's response is improved by first retrieving relevant information from an external knowledge base (often a vector store) and then using that information as context for generation.\n",
    "* **`Document`**: LangChain's abstraction for a unit of text, comprising `page_content` (the text itself) and `metadata` (a dictionary of arbitrary information about the document).\n",
    "* **`VectorStore`**: A database (like Chroma, FAISS, Pinecone) that stores numerical vector representations (embeddings) of text, allowing for efficient similarity searches.\n",
    "* **`Embedding`**: A dense numerical vector that captures the semantic meaning of text.\n",
    "* **`HuggingFaceEmbeddings`**: LangChain's integration to generate embeddings using models hosted on Hugging Face (e.g., `all-MiniLM-L6-v2`).\n",
    "* **`Chroma`**: A popular open-source, AI-native vector database often used for local development and persistence.\n",
    "* **`Retriever`**: A LangChain component that defines a standard way to retrieve documents based on a query. Retrievers are `Runnable`s, meaning they can be easily integrated into LCEL chains.\n",
    "* **`RunnableLambda`**: An LCEL construct to turn any Python function into a `Runnable`.\n",
    "* **`as_retriever()`**: A common method on `VectorStore` objects to convert them into a standard LangChain `Retriever`.\n",
    "* **`search_type` and `search_kwargs`**: Parameters used when creating a `VectorStoreRetriever` to specify how the underlying vector store should perform its search (e.g., \"similarity\", \"mmr\") and with what parameters (e.g., `k` for number of results).\n",
    "* **LangChain Expression Language (LCEL)**: The `|` operator for chaining components (`Runnable`s) together, facilitating complex workflows.\n",
    "* **`RunnablePassthrough`**: An LCEL component that simply passes its input through to the next component in a chain. Used in RAG to pass the original question.\n",
    "* **`ChatPromptTemplate`**: Used to structure the input to a chat model, incorporating both the question and the retrieved context.\n",
    "\n",
    "## Setup: API Keys and Environment Variables\n",
    "\n",
    "Ensure your `.env` file is configured with `GROQ_API_KEY` and optionally `HF_TOKEN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores and retrievers\n",
    "This video tutorial will familiarize you with LangChain's vector store and retriever abstractions. These abstractions are designed to support retrieval of data-- from (vector) databases and other sources-- for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation.\n",
    "\n",
    "We will cover \n",
    "- Documents\n",
    "- Vector stores\n",
    "- Retrievers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents\n",
    "LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has two attributes:\n",
    "\n",
    "- page_content: a string representing the content;\n",
    "- metadata: a dict containing arbitrary metadata.\n",
    "The metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\n",
    "\n",
    "Let's generate some sample documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initialized Groq LLM ---\n",
      "client=<groq.resources.chat.completions.Completions object at 0x114302ba0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x114303770> model_name='Llama3-8b-8192'\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries (uncomment and run if not already installed)\n",
    "# !pip install langchain langchain-chroma langchain_groq langchain_huggingface python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file.\n",
    "load_dotenv()\n",
    "\n",
    "# --- API Keys ---\n",
    "# Get Groq API key from environment variables.\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "# Get HuggingFace Token from environment variables (optional for some models).\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Import ChatGroq for using Groq's fast inference models.\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize a ChatGroq model.\n",
    "# Using \"Llama3-8b-8192\" which is a powerful and fast open-source model via Groq.\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model=\"Llama3-8b-8192\")\n",
    "\n",
    "# Display the initialized LLM object.\n",
    "print(\"--- Initialized Groq LLM ---\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents: The Basic Unit of Data\n",
    "\n",
    "In LangChain, a `Document` is a fundamental abstraction. It represents a piece of text content along with associated metadata. This structure allows us to keep track of the source and other relevant information about the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Documents ---\n",
      "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'), Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'), Document(metadata={'source': 'fish-pets-doc'}, page_content='Goldfish are popular pets for beginners, requiring relatively simple care.'), Document(metadata={'source': 'bird-pets-doc'}, page_content='Parrots are intelligent birds capable of mimicking human speech.'), Document(metadata={'source': 'mammal-pets-doc'}, page_content='Rabbits are social animals that need plenty of space to hop around.')]\n"
     ]
    }
   ],
   "source": [
    "# Import the Document class from langchain_core.documents.\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create a list of sample Document objects.\n",
    "# Each Document has `page_content` (the actual text) and `metadata` (a dictionary).\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
    "        metadata={\"source\": \"fish-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
    "        metadata={\"source\": \"bird-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Display the list of created documents.\n",
    "print(\"--- Sample Documents ---\")\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings: Converting Text to Vectors\n",
    "\n",
    "Before storing documents in a vector store, they need to be converted into numerical vector embeddings. We'll use a Hugging Face embedding model for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pro/python-learning/Machine Learning/16-chat-bot-with-history/chat-bot-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Install langchain_huggingface if not already installed.\n",
    "# !pip install langchain_huggingface\n",
    "\n",
    "# Import HuggingFaceEmbeddings for creating embeddings using Hugging Face models.\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize HuggingFaceEmbeddings with a pre-trained model.\n",
    "# \"all-MiniLM-L6-v2\" is a good general-purpose embedding model.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Display the embeddings object.\n",
    "print(\"--- Initialized HuggingFace Embeddings ---\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Stores: Storing and Querying Embeddings\n",
    "\n",
    "A vector store is a database designed to store vector embeddings and efficiently perform similarity searches (find vectors close to a given query vector). Here, we'll use Chroma, an open-source vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chroma Vector Store Created ---\n",
      "<langchain_chroma.vectorstores.Chroma object at 0x3171f9400>\n"
     ]
    }
   ],
   "source": [
    "# Import Chroma as our vector store.\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Create a Chroma vector store from our documents and embedding function.\n",
    "# Chroma will embed each document using `embeddings` and store the resulting vectors.\n",
    "vectorstore = Chroma.from_documents(documents, embedding=embeddings)\n",
    "\n",
    "# Display the created vector store object.\n",
    "print(\"--- Chroma Vector Store Created ---\")\n",
    "print(vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Search\n",
    "\n",
    "You can query the vector store directly to find documents similar to a given text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Similarity Search Results for 'cat' ---\n",
      "[Document(id='9dc76707-687c-4ae1-90a0-90b6cda55f72', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'), Document(id='e88e6e6b-b762-4231-897a-beb00c6b862b', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'), Document(id='c397515b-ef86-4ac6-a89d-3f3c5cc4c682', metadata={'source': 'mammal-pets-doc'}, page_content='Rabbits are social animals that need plenty of space to hop around.'), Document(id='45da9439-feb0-4982-a46a-8a604f216e03', metadata={'source': 'bird-pets-doc'}, page_content='Parrots are intelligent birds capable of mimicking human speech.')]\n"
     ]
    }
   ],
   "source": [
    "# Perform a similarity search for the query \"cat\".\n",
    "# It will embed \"cat\" and find documents with the most similar embeddings.\n",
    "results = vectorstore.similarity_search(\"cat\")\n",
    "\n",
    "# Print the results. These are Document objects sorted by similarity.\n",
    "print(\"--- Similarity Search Results for 'cat' ---\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asynchronous Query\n",
    "\n",
    "Vector stores often support asynchronous operations for better performance in concurrent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Asynchronous Similarity Search (Awaiting Result) ---\n",
      "[Document(id='9dc76707-687c-4ae1-90a0-90b6cda55f72', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'), Document(id='e88e6e6b-b762-4231-897a-beb00c6b862b', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'), Document(id='c397515b-ef86-4ac6-a89d-3f3c5cc4c682', metadata={'source': 'mammal-pets-doc'}, page_content='Rabbits are social animals that need plenty of space to hop around.'), Document(id='45da9439-feb0-4982-a46a-8a604f216e03', metadata={'source': 'bird-pets-doc'}, page_content='Parrots are intelligent birds capable of mimicking human speech.')]\n"
     ]
    }
   ],
   "source": [
    "# To run await, this cell needs to be in an async context, like a Jupyter Notebook cell.\n",
    "# In a Python script, you'd define an async function and run it with `asyncio.run()`.\n",
    "print(\"--- Asynchronous Similarity Search (Awaiting Result) ---\")\n",
    "async_results = await vectorstore.asimilarity_search(\"cat\")\n",
    "print(async_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Search with Score\n",
    "\n",
    "Many vector stores allow you to retrieve documents along with a similarity score. The meaning of the score (higher is better vs. lower is better) depends on the distance metric used by the embedding model and vector store (e.g., cosine similarity vs. L2 distance). For cosine distance (often the default for embeddings), a lower score indicates higher similarity (0 means identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Similarity Search Results with Score for 'cat' ---\n",
      "[(Document(id='9dc76707-687c-4ae1-90a0-90b6cda55f72', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'), 0.9351055026054382), (Document(id='e88e6e6b-b762-4231-897a-beb00c6b862b', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'), 1.574090600013733), (Document(id='c397515b-ef86-4ac6-a89d-3f3c5cc4c682', metadata={'source': 'mammal-pets-doc'}, page_content='Rabbits are social animals that need plenty of space to hop around.'), 1.595691204071045), (Document(id='45da9439-feb0-4982-a46a-8a604f216e03', metadata={'source': 'bird-pets-doc'}, page_content='Parrots are intelligent birds capable of mimicking human speech.'), 1.6657930612564087)]\n"
     ]
    }
   ],
   "source": [
    "# Perform a similarity search and also return the similarity score.\n",
    "# The result is a list of tuples: (Document, score).\n",
    "results_with_score = vectorstore.similarity_search_with_score(\"cat\")\n",
    "\n",
    "# Print the results, including the scores.\n",
    "print(\"--- Similarity Search Results with Score for 'cat' ---\")\n",
    "print(results_with_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievers: Standardized Data Fetching\n",
    "\n",
    "LangChain `VectorStore` objects, by themselves, are not `Runnable`s, which means they can't be directly chained using LCEL's `|` operator. LangChain `Retrievers` are `Runnable`s, providing a standardized interface for fetching documents and enabling seamless integration into LCEL chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievers\n",
    "LangChain VectorStore objects do not subclass Runnable, and so cannot immediately be integrated into LangChain Expression Language chains.\n",
    "\n",
    "LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations) and are designed to be incorporated in LCEL chains.\n",
    "\n",
    "We can create a simple version of this ourselves, without subclassing Retriever. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Retriever Manually (using `RunnableLambda`)\n",
    "\n",
    "We can manually create a `Runnable` that acts as a retriever by wrapping a vector store's search method with `RunnableLambda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch Results using RunnableLambda Retriever ---\n",
      "[[Document(id='9dc76707-687c-4ae1-90a0-90b6cda55f72', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.')], [Document(id='e88e6e6b-b762-4231-897a-beb00c6b862b', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')]]\n"
     ]
    }
   ],
   "source": [
    "# Import RunnableLambda for creating a runnable from a function.\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Create a retriever using RunnableLambda.\n",
    "# We wrap `vectorstore.similarity_search` and bind the `k` parameter to 1 (return 1 result).\n",
    "retriever_lambda = RunnableLambda(vectorstore.similarity_search).bind(k=1)\n",
    "\n",
    "# Use the `batch` method on the retriever to get results for multiple queries.\n",
    "# This demonstrates its `Runnable` capabilities.\n",
    "batch_results_lambda = retriever_lambda.batch([\"cat\", \"dog\"])\n",
    "\n",
    "# Print the batched results.\n",
    "print(\"--- Batch Results using RunnableLambda Retriever ---\")\n",
    "print(batch_results_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Retriever from `as_retriever()`\n",
    "\n",
    "Most `VectorStore` implementations in LangChain provide an `as_retriever()` method, which is the idiomatic way to create a `Retriever` from a vector store. This method also allows you to specify search parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch Results using as_retriever() ---\n",
      "[[Document(id='9dc76707-687c-4ae1-90a0-90b6cda55f72', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.')], [Document(id='e88e6e6b-b762-4231-897a-beb00c6b862b', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')]]\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever using the `as_retriever` method of the vector store.\n",
    "# search_type: Specifies the type of search (e.g., \"similarity\", \"mmr\").\n",
    "# search_kwargs: A dictionary of parameters to pass to the underlying search method (e.g., \"k\" for top_k results).\n",
    "retriever_as_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1} # Retrieve only the top 1 most similar document\n",
    ")\n",
    "\n",
    "# Use the `batch` method to test the retriever.\n",
    "batch_results_as_retriever = retriever_as_retriever.batch([\"cat\", \"dog\"])\n",
    "\n",
    "# Print the batched results.\n",
    "print(\"--- Batch Results using as_retriever() ---\")\n",
    "print(batch_results_as_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG Application\n",
    "\n",
    "Now, let's combine all these components to build a basic Retrieval-Augmented Generation (RAG) chain. This chain will:\n",
    "1.  Take a user question.\n",
    "2.  Retrieve relevant documents using our `retriever`.\n",
    "3.  Combine the question and context into a prompt.\n",
    "4.  Send the prompt to the LLM to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RAG Chain Response ---\n",
      "According to the provided context, dogs are great companions, known for their loyalty and friendliness.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary components for building the RAG chain.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define the prompt template for the LLM.\n",
    "# It explicitly tells the LLM to answer \"using the provided context only\".\n",
    "# \"{question}\" is for the user's query.\n",
    "# \"{context}\" is for the retrieved documents.\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "# Build the RAG chain using LCEL.\n",
    "# The chain expects a dictionary input with a \"question\" key.\n",
    "# {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "#   - \"context\" key: Populated by the `retriever` (which takes the \"question\" as input implicitly).\n",
    "#   - \"question\" key: Passed directly from the original input using `RunnablePassthrough()`.\n",
    "# The output of this dict construction is then piped to the `prompt`.\n",
    "# The `prompt`'s output (formatted messages) is then piped to the `llm`.\n",
    "rag_chain = {\"context\": retriever_as_retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "# Invoke the RAG chain with a question.\n",
    "response = rag_chain.invoke(\"tell me about dogs\")\n",
    "\n",
    "# Print the content of the LLM's response.\n",
    "# The answer should be based on the dog document we provided earlier.\n",
    "print(\"--- RAG Chain Response ---\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-bot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
