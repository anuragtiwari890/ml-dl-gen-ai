{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Conversational Chatbot with LangChain\n",
    "\n",
    "This notebook demonstrates how to design and implement a basic LLM-powered chatbot that can maintain a conversation and remember previous interactions. We will focus on building a stateful chatbot using LangChain's message history features.\n",
    "\n",
    "This tutorial covers the fundamentals of building conversational agents, which are essential for more advanced topics like:\n",
    "* **Conversational RAG**: Enabling a chatbot to answer questions based on external data sources while maintaining conversation history.\n",
    "* **Agents**: Building chatbots that can not only converse but also take actions (e.g., call tools, access external APIs).\n",
    "\n",
    "## What You Will Learn:\n",
    "\n",
    "1.  **Basic LLM Interaction**: Sending messages to an LLM and getting responses.\n",
    "2.  **Managing Message History**: Implementing memory for a chatbot using `ChatMessageHistory` and `RunnableWithMessageHistory`.\n",
    "3.  **Session Management**: Handling multiple concurrent conversations using different session IDs.\n",
    "4.  **Integrating Prompt Templates**: Using `ChatPromptTemplate` and `MessagesPlaceholder` for structured conversation prompts.\n",
    "5.  **Adding Dynamic Input**: Extending prompts to include additional variables beyond just chat messages (e.g., target language).\n",
    "6.  **Managing Conversation History Size**: Employing `trim_messages` to prevent context window overflow in long conversations.\n",
    "7.  **Building Complex Chains with LCEL**: Composing multiple components (trimmer, prompt, model) into a robust conversational chain.\n",
    "\n",
    "## Key Concepts:\n",
    "\n",
    "* **Stateful Chatbot**: A chatbot that remembers past turns in a conversation, allowing for more natural and coherent interactions.\n",
    "* **Message History**: The chronological record of messages exchanged between the user and the AI.\n",
    "* **`ChatMessageHistory`**: A LangChain class to store and manage chat messages for a single session.\n",
    "* **`RunnableWithMessageHistory`**: A LangChain Runnable wrapper that automatically manages message history for an underlying chain, handling loading and saving messages.\n",
    "* **Session ID**: A unique identifier for a conversation, allowing the chatbot to maintain separate histories for different users or chats.\n",
    "* **`ChatPromptTemplate`**: Used to structure the input to a chat model, often including system instructions and a placeholder for conversation history.\n",
    "* **`MessagesPlaceholder`**: A component within a `ChatPromptTemplate` that dynamically inserts the current message history into the prompt.\n",
    "* **Context Window**: The maximum amount of text an LLM can process at one time. Managing history size is critical to avoid exceeding this limit.\n",
    "* **`trim_messages`**: A LangChain utility to strategically reduce the number of messages in the history to fit within a `max_tokens` limit.\n",
    "* **LCEL (LangChain Expression Language)**: The intuitive `|` operator for chaining components (`Runnable`s) together, facilitating complex workflows.\n",
    "* **`RunnablePassthrough.assign`**: An LCEL construct that allows you to add or modify keys in the input dictionary before passing them to the next component in a chain.\n",
    "\n",
    "## Setup: API Keys and Environment Variables\n",
    "\n",
    "Ensure your `.env` file is configured with `GROQ_API_KEY`. Optionally, set up `LANGCHAIN_API_KEY`, `LANGCHAIN_TRACING_V2`, and `LANGCHAIN_PROJECT` for LangSmith tracing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Chatbot\n",
    "In this video We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbot that can take actions\n",
    "\n",
    "This video tutorial will cover the basics which will be helpful for those two more advanced topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initialized Groq Model ---\n",
      "client=<groq.resources.chat.completions.Completions object at 0x115e96d50> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x115e97750> model_name='Gemma2-9b-It'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file.\n",
    "load_dotenv()\n",
    "\n",
    "# Set Groq API key from environment variables.\n",
    "# This is crucial for authenticating with Groq's models.\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# --- Optional: LangSmith Tracking Setup ---\n",
    "# Uncomment and set these if you want to use LangSmith for tracing.\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "\n",
    "# Import ChatGroq for using Groq's fast inference models.\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize a ChatGroq model.\n",
    "# We're using \"Gemma2-9b-It\" as our chosen model.\n",
    "# Ensure you have pulled this model locally via `ollama pull gemma:2b`\n",
    "# (if using Ollama and linking it to Groq's API, or simply ensuring the model is available via Groq's cloud).\n",
    "# Note: For Groq, 'gemma:2b' implies the model *served by Groq*, not necessarily locally pulled by Ollama,\n",
    "# unless you're explicitly proxying Ollama through Groq's API. The model name refers to Groq's available models.\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)\n",
    "\n",
    "# Display the initialized model object.\n",
    "print(\"--- Initialized Groq Model ---\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LLM Interaction (Without Memory)\n",
    "\n",
    "Let's start by showing how the LLM responds to individual messages without any built-in memory. It treats each invocation as a fresh start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First Interaction Response (No Memory) ---\n",
      "Hi Krish! It's nice to meet you. \n",
      "\n",
      "That's an impressive title! As a Chief AI Engineer, I imagine you have a lot of exciting projects on your plate. \n",
      "\n",
      "What kind of work are you currently focused on? I'm always eager to learn about new developments in the field of AI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import HumanMessage for user input.\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Invoke the model with a single HumanMessage.\n",
    "response = model.invoke([HumanMessage(content=\"Hi , My name is Krish and I am a Chief AI Engineer\")])\n",
    "\n",
    "# Print the content of the model's response.\n",
    "print(\"--- First Interaction Response (No Memory) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Multi-Turn Interaction Response (Explicit Context) ---\n",
      "You are Krish, and you are a Chief AI Engineer.  \n",
      "\n",
      "Is there anything else you'd like to tell me about yourself or your work? 😊  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import AIMessage for representing AI's past responses.\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Invoke the model with a list of messages representing a short conversation.\n",
    "# The model sees the entire list and responds based on that full context.\n",
    "# However, it doesn't \"remember\" this exchange for *future* independent invocations.\n",
    "response = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is Krish and I am a Chief AI Engineer\"),\n",
    "        AIMessage(content=\"Hello Krish! It's nice to meet you. \\n\\nAs a Chief AI Engineer, what kind of projects are you working on these days? \\n\\nI'm always eager to learn more about the exciting work being done in the field of AI.\\n\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the content of the model's response.\n",
    "print(\"--- Multi-Turn Interaction Response (Explicit Context) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Message History with `RunnableWithMessageHistory`\n",
    "\n",
    "To make the chatbot truly stateful and remember past interactions automatically, we use LangChain's `RunnableWithMessageHistory`. This wrapper handles loading and saving chat messages to a history store for each session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stateful Chat (Session 1, Turn 1) ---\n",
      "Hello Krish,\n",
      "\n",
      "It's nice to meet you!\n",
      "\n",
      "Being a Chief AI Engineer is an exciting role. What kind of projects are you working on these days?  \n",
      "\n",
      "I'm eager to learn more about your work and how I can be helpful.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install langchain_community if not already installed (for ChatMessageHistory)\n",
    "# !pip install langchain_community\n",
    "\n",
    "# Import ChatMessageHistory for storing messages in memory.\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "# Import BaseChatMessageHistory for type hinting.\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "# Import RunnableWithMessageHistory for stateful chains.\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# A simple in-memory store for session histories.\n",
    "# In a real application, this would be a persistent database (e.g., Redis, database).\n",
    "store = {}\n",
    "\n",
    "# Define a function to get the chat history for a given session ID.\n",
    "# If a session ID doesn't exist, it creates a new ChatMessageHistory.\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrap our 'model' (or any chain) with RunnableWithMessageHistory.\n",
    "# This makes the model stateful by automatically managing message history.\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "# Define a configuration for the session.\n",
    "# The 'configurable' key expects a dictionary with session_id.\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}\n",
    "\n",
    "# Invoke the stateful model for the first turn.\n",
    "# The 'config' dictionary tells RunnableWithMessageHistory which session to use.\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My name is Krish and I am a Chief AI Engineer\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the content of the response.\n",
    "print(\"--- Stateful Chat (Session 1, Turn 1) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stateful Chat (Session 1, Turn 2) ---\n",
      "Your name is Krish. \n",
      "\n",
      "I remember that from our introduction! 😊  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Invoke the stateful model again for the same session.\n",
    "# Now, the model should remember the name \"Krish\" from the previous turn,\n",
    "# because RunnableWithMessageHistory automatically prepends the history.\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Print the content of the response.\n",
    "# It should correctly identify the name.\n",
    "print(\"--- Stateful Chat (Session 1, Turn 2) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Multiple Chat Sessions\n",
    "\n",
    "`RunnableWithMessageHistory` allows you to manage separate conversation histories for different users or sessions by simply changing the `session_id` in the `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me your name, I'd be happy to use it! 😊\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config-->session id\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stateful Chat (Session 2, Turn 2 - New Name) ---\n",
      "Hi John! It's nice to meet you. 😊 \n",
      "\n",
      "Is there anything I can help you with today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, in 'chat2', tell the model a new name.\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is John\")],\n",
    "    config=config1\n",
    ")\n",
    "\n",
    "# Print the content of the response.\n",
    "print(\"--- Stateful Chat (Session 2, Turn 2 - New Name) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stateful Chat (Session 2, Turn 3 - Recalls John) ---\n",
      "Your name is John!  \n",
      "\n",
      "I remember you told me earlier. 😊  Anything else I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask \"Whats my name\" again in 'chat2'.\n",
    "# It should now remember \"John\".\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "\n",
    "# Print the content of the response.\n",
    "print(\"--- Stateful Chat (Session 2, Turn 3 - Recalls John) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancing Prompts with `ChatPromptTemplate` and `MessagesPlaceholder`\n",
    "\n",
    "Prompt Templates help transform raw user input and conversation history into a format that the LLM can best utilize. We'll add a system message with instructions and use `MessagesPlaceholder` to insert the conversation history dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chain with Prompt Template (No History Yet) ---\n",
      "Hi Krish, it's nice to meet you!\n",
      "\n",
      "How can I help you today? 😊  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import ChatPromptTemplate and MessagesPlaceholder.\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Define a ChatPromptTemplate.\n",
    "# The system message provides overall instructions.\n",
    "# MessagesPlaceholder tells the LLM where to insert the actual conversation history.\n",
    "# The variable name \"messages\" is conventional for message history.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a simple chain: prompt | model.\n",
    "# This chain now expects a dictionary input with a \"messages\" key.\n",
    "chain = prompt | model\n",
    "\n",
    "# Invoke the chain directly with a new message, formatted as a list of HumanMessages.\n",
    "# Note: At this point, `chain` itself doesn't have memory; we're just testing the prompt.\n",
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"Hi My name is Krish\")]})\n",
    "\n",
    "# Print the response content.\n",
    "print(\"--- Chain with Prompt Template (No History Yet) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stateful Chain with Prompt (Session 3, Turn 1) ---\n",
      "Hi Krish, it's nice to meet you!\n",
      "\n",
      "How can I help you today?  😊  \n",
      "\n",
      "--- Stateful Chain with Prompt (Session 3, Turn 2) ---\n",
      "Your name is Krish. I remember! 😄 \n",
      "\n",
      "Is there anything else I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, wrap this new 'chain' (which includes the prompt) with `RunnableWithMessageHistory`.\n",
    "# This combines the structured prompting with automatic history management.\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
    "\n",
    "# Define a new configuration for session 'chat3'.\n",
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "\n",
    "# First turn in session 'chat3'.\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Krish\")], # Input is still a list of messages here\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the response.\n",
    "print(\"--- Stateful Chain with Prompt (Session 3, Turn 1) ---\")\n",
    "print(response.content)\n",
    "\n",
    "# Second turn in session 'chat3'.\n",
    "# It should now remember \"Krish\".\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Print the response content.\n",
    "print(\"--- Stateful Chain with Prompt (Session 3, Turn 2) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is Krish.  \\n\\nI'm ready for your next question! 😊  \\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding More Complexity to the Prompt (e.g., Language)\n",
    "\n",
    "What if we want to pass *additional* information to the LLM besides just the chat history? For example, controlling the output language. We can add more placeholders to our prompt.\n",
    "\n",
    "When using `RunnableWithMessageHistory` with a chain that has multiple input keys, you need to tell it which key contains the actual chat messages for history management. This is done with the `input_messages_key` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chain with Multiple Prompt Inputs (No History Yet) ---\n",
      "नमस्ते कृष्णा! \n",
      "\n",
      "मुझे बहुत खुशी हो कि आप मुझसे बात कर रहे हैं। मैं आपकी मदद करने के लिए यहाँ हूँ। \n",
      "\n",
      "आप किस बारे में जानना चाहते हैं? \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a more complex prompt template.\n",
    "# Now includes a {language} placeholder in the system message.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # This is where the chat history goes\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the chain: prompt | model.\n",
    "chain = prompt | model\n",
    "\n",
    "# Invoke the chain with both \"messages\" and \"language\" inputs.\n",
    "# Note: This 'chain' still doesn't have history built-in at this stage.\n",
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Krish\")],\"language\":\"Hindi\"})\n",
    "\n",
    "# Print the response content.\n",
    "print(\"--- Chain with Multiple Prompt Inputs (No History Yet) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stateful Chain (Session 4, Turn 1 - Hindi) ---\n",
      "नमस्ते कृष! \n",
      "\n",
      "मेरे लिए खुशी है कि आप मुझसे बात कर रहे हैं। क्या मैं आपकी कोई मदद कर सकता हूँ? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wrap this more complicated chain with RunnableWithMessageHistory.\n",
    "# Crucially, we specify `input_messages_key=\"messages\"` to tell it which input key\n",
    "# holds the actual chat history that needs to be managed.\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\" # This tells the Runnable to look for messages here\n",
    ")\n",
    "\n",
    "# Define a new configuration for session 'chat4'.\n",
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "\n",
    "# First turn in session 'chat4', providing both messages and language.\n",
    "response = with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi,I am Krish\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the response content.\n",
    "print(\"--- Stateful Chain (Session 4, Turn 1 - Hindi) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stateful Chain (Session 4, Turn 2 - Hindi) ---\n",
      "आपका नाम कृष है। 😊  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second turn in session 'chat4'.\n",
    "# It should remember \"Krish\" and continue responding in Hindi.\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Print the response content.\n",
    "print(\"--- Stateful Chain (Session 4, Turn 2 - Hindi) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation History Size (Context Window Management)\n",
    "\n",
    "For long conversations, the list of messages can grow very large, eventually exceeding the LLM's context window and increasing costs. It's essential to implement a strategy to limit the size of the messages passed to the LLM.\n",
    "\n",
    "LangChain provides a `trim_messages` helper to manage history size by token count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pro/python-learning/Machine Learning/16-chat-bot-with-history/chat-bot-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Trimmed Messages (max_tokens=45) ---\n",
      "content=\"you're a good assistant\"\n",
      "content='I like vanilla ice cream'\n",
      "content='nice'\n",
      "content='whats 2 + 2'\n",
      "content='4'\n",
      "content='thanks'\n",
      "content='no problem!'\n",
      "content='having fun?'\n",
      "content='yes!'\n"
     ]
    }
   ],
   "source": [
    "# Import SystemMessage and trim_messages.\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "# Import AIMessage (already imported, but good to show explicitly for context).\n",
    "from langchain_core.messages import AIMessage\n",
    "# Import HumanMessage (already imported).\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# Initialize the trimmer.\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,          # Maximum number of tokens to keep in the history.\n",
    "    strategy=\"last\",        # Keep the most recent messages.\n",
    "    token_counter=model,    # Use the model's tokenizer to count tokens.\n",
    "    include_system=True,    # Always include the system message.\n",
    "    allow_partial=False,    # Do not allow partially included messages (a full message must fit).\n",
    "    start_on=\"human\"        # Start counting from the first human message if system message is always included.\n",
    ")\n",
    "\n",
    "# Create a long list of sample messages.\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "# Invoke the trimmer with the long message list.\n",
    "# This will return a truncated list of messages according to `max_tokens`.\n",
    "trimmed_messages = trimmer.invoke(messages)\n",
    "\n",
    "# Print the trimmed messages.\n",
    "print(\"--- Trimmed Messages (max_tokens=45) ---\")\n",
    "for msg in trimmed_messages:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Response with Trimming (Recalls Ice Cream) ---\n",
      "As an AI, I have no memory of past conversations and don't know your preferences.  \n",
      "\n",
      "What's your favorite flavor of ice cream? 😊🍦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import itemgetter for extracting values from a dictionary.\n",
    "from operator import itemgetter\n",
    "# Import RunnablePassthrough for passing through input.\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Reconstruct the chain to include the trimmer *before* the prompt.\n",
    "# RunnablePassthrough.assign allows us to modify the \"messages\" key in the input dictionary.\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer) # Apply trimmer to messages\n",
    "    | prompt # Then send trimmed messages (and other inputs) to the prompt\n",
    "    | model  # Then send prompt's output to the model\n",
    ")\n",
    "\n",
    "# Invoke the chain with the full message history plus a new question.\n",
    "# The 'trimmer' inside the chain will automatically reduce the messages.\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What ice cream do I like?\")],\n",
    "        \"language\": \"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the response content.\n",
    "print(\"--- Response with Trimming (Recalls Ice Cream) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Response with Trimming (Recalls Math Problem) ---\n",
      "You asked \"What is 2 + 2?\" 😊  \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other math problems you'd like to try!  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain with another question from the trimmed history.\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the response content.\n",
    "# It should still recall the math problem if it fits within the trimmed history.\n",
    "print(\"--- Response with Trimming (Recalls Math Problem) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping the Trimmed Chain in `RunnableWithMessageHistory`\n",
    "\n",
    "Finally, we wrap this more sophisticated chain (which includes the trimmer) with `RunnableWithMessageHistory` to get the full stateful chatbot experience, now with context window management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Full Stateful Chat (Session 5, Turn 1 - Recalls Name) ---\n",
      "As an AI, I have no memory of past conversations and don't know your name. What name would you like me to call you? 😊  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wrap the chain (which now includes the trimmer) with RunnableWithMessageHistory.\n",
    "# This creates a full-fledged stateful chatbot with history management.\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\", # Specify which input key contains the messages for history\n",
    ")\n",
    "\n",
    "# Define a new configuration for session 'chat5'.\n",
    "config = {\"configurable\": {\"session_id\": \"chat5\"}}\n",
    "\n",
    "# First interaction in 'chat5'.\n",
    "# The input 'messages' now includes the initial long history plus a new human message.\n",
    "# The trimmer will ensure only a relevant portion is sent to the LLM.\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Print the response content.\n",
    "# It should recall the name \"bob\" if the system message and \"hi! I'm bob\" fit the context.\n",
    "print(\"--- Full Stateful Chat (Session 5, Turn 1 - Recalls Name) ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Full Stateful Chat (Session 5, Turn 2 - Recalls Math Problem) ---\n",
      "As a helpful assistant, I have no memory of past conversations.  \n",
      "\n",
      "Can you tell me the math problem you'd like help with? 😊  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second interaction in 'chat5'.\n",
    "# Now, the 'messages' input *only* contains the current human message,\n",
    "# because `RunnableWithMessageHistory` prepends the *persisted and trimmed* history automatically.\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Print the response content.\n",
    "# It should recall the math problem if it was part of the history kept by the trimmer.\n",
    "print(\"--- Full Stateful Chat (Session 5, Turn 2 - Recalls Math Problem) ---\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-bot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
