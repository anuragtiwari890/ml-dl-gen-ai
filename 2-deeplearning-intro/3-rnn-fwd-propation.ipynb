{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Forward Propagation in Simple RNN ‚Äì Step-by-Step Breakdown\n",
    "\n",
    "---\n",
    "\n",
    "## üìò Notebook Objective\n",
    "\n",
    "This notebook aims to **demystify forward propagation in a Simple Recurrent Neural Network (RNN)** using a clear and intuitive example. We will walk through every step in how an RNN processes sequences word by word, maintains context through time, and computes outputs at each timestamp.\n",
    "\n",
    "By the end of this notebook, you will have a solid understanding of **how RNNs \"remember\" past words** and why this memory is crucial for sequential tasks like text classification, sentiment analysis, and language modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úçÔ∏è What We Will Learn\n",
    "\n",
    "We will cover the following concepts in detail:\n",
    "\n",
    "### üîπ 1. Introduction to RNNs (Recap)\n",
    "- Understanding the **architecture of Simple RNNs**\n",
    "- Explanation of the **unfolding technique** (how an RNN is \"unrolled\" over time)\n",
    "\n",
    "### üîπ 2. Problem Setup\n",
    "- Input Sentence: `\"The food is good\"`\n",
    "- Vocabulary: [\"the\", \"food\", \"is\", \"good\", \"bad\", \"not\"] ‚Üí Total: 6 unique words\n",
    "- Objective: Build a binary classification model using RNN\n",
    "\n",
    "### üîπ 3. Text Preprocessing\n",
    "- **One-hot encoding** for word vectorization\n",
    "    - Each word becomes a vector of length equal to the vocabulary size\n",
    "    - Only the index corresponding to the word is `1`, all others are `0`\n",
    "\n",
    "### üîπ 4. RNN Architecture\n",
    "- **Input layer**: Sequence of word vectors\n",
    "- **Hidden layer**: Set of neurons (e.g., 3 neurons)\n",
    "- **Output layer**: Prediction of class (e.g., positive or negative sentiment)\n",
    "- **Recurrent feedback loop**: Past hidden states are fed into the next timestamp\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ How Forward Propagation Works\n",
    "\n",
    "For a sentence like `\"The food is good\"`, we break down the operations over **multiple time steps (t=1 to t=4)**:\n",
    "\n",
    "### ‚úÖ Time Step t=1\n",
    "- Input: First word vector `x‚ÇÅ`\n",
    "- Operation:  \n",
    "\n",
    "**h‚ÇÅ = f(x‚ÇÅ ‚Ä¢ W‚Çì + b‚ÇÅ)**\n",
    "\n",
    "- `W‚Çì`: weight matrix for input-to-hidden\n",
    "- `b‚ÇÅ`: bias\n",
    "- `f`: activation function (usually `tanh` or `ReLU`)\n",
    "- Output: Hidden state `h‚ÇÅ`\n",
    "\n",
    "### ‚úÖ Time Step t=2\n",
    "- Input: Next word vector `x‚ÇÇ`\n",
    "- Previous output `h‚ÇÅ` is also fed back\n",
    "- Operation:\n",
    "\n",
    "**h‚ÇÇ = f(x‚ÇÇ ‚Ä¢ W‚Çì + h‚ÇÅ ‚Ä¢ W‚Çï + b‚ÇÅ)**\n",
    "\n",
    "- `W‚Çï`: recurrent weights (hidden-to-hidden)\n",
    "- Adds contextual memory from `h‚ÇÅ` to current input\n",
    "\n",
    "### ‚úÖ Time Steps t=3, t=4 ...\n",
    "- Repeat same pattern:\n",
    "\n",
    "**h‚Çú = f(x‚Çú ‚Ä¢ W‚Çì + h‚Çú‚Çã‚ÇÅ ‚Ä¢ W‚Çï + b‚ÇÅ)**\n",
    "\n",
    "### ‚úÖ Final Output\n",
    "- Once the entire sentence is processed:\n",
    "\n",
    "**≈∑ = sigmoid(h‚ÇÑ ‚Ä¢ W_out + b_out)**\n",
    "- Binary classification ‚Üí use **sigmoid**\n",
    "- For multi-class ‚Üí use **softmax**\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Trainable Parameters ‚Äì How Many?\n",
    "\n",
    "Let‚Äôs calculate the total number of parameters for our RNN with:\n",
    "- **Input vector size**: 5 (one-hot vector length)\n",
    "- **Hidden units**: 3\n",
    "- **Output classes**: 1 (binary)\n",
    "\n",
    "| Parameter Type       | Shape           | Count |\n",
    "|----------------------|------------------|-------|\n",
    "| Input weights        | (5, 3)           | 15    |\n",
    "| Hidden weights       | (3, 3)           | 9     |\n",
    "| Output weights       | (3, 1)           | 3     |\n",
    "| Bias (hidden)        | (3,)             | 3     |\n",
    "| Bias (output)        | (1,)             | 1     |\n",
    "| **Total Parameters** |                  | **31**|\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why Does This Matter?\n",
    "\n",
    "Understanding forward propagation in RNNs is **critical** because:\n",
    "\n",
    "### üîç Sequence Awareness\n",
    "Unlike traditional feedforward networks, RNNs process data **sequentially**, maintaining context via hidden states.\n",
    "\n",
    "### üìö Foundation for Advanced Models\n",
    "Concepts like **context preservation**, **recurrent loops**, and **time-dependent computation** are fundamental for understanding:\n",
    "- LSTM (Long Short-Term Memory)\n",
    "- GRU (Gated Recurrent Unit)\n",
    "- Transformer models (e.g., BERT, GPT)\n",
    "\n",
    "### ‚öôÔ∏è Practical Applications\n",
    "RNNs (and their variants) are used in:\n",
    "- Sentiment analysis\n",
    "- Machine translation\n",
    "- Text summarization\n",
    "- Speech recognition\n",
    "- Time series forecasting\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Additional Notes & Recommendations\n",
    "\n",
    "### üß© Activation Functions\n",
    "- Use `tanh` or `ReLU` in the hidden layers\n",
    "- Use `sigmoid` for binary output; `softmax` for multi-class\n",
    "\n",
    "### üõ†Ô∏è Suggestions for Deepening Learning\n",
    "- **Add implementation**: Include NumPy or PyTorch code for each operation\n",
    "- **Visuals**: Include a diagram showing how hidden states are passed across time steps\n",
    "- **Case Study**: Show prediction on a real dataset (e.g., IMDB reviews)\n",
    "- **Extend**: Introduce Backward Propagation Through Time (BPTT) in the next notebook\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Final Takeaway\n",
    "\n",
    "The forward pass in a Simple RNN is **the gateway to understanding how machines learn from sequences**. It teaches:\n",
    "- How **word context** is built over time\n",
    "- How **weights** and **states** are used to preserve memory\n",
    "- The basic machinery behind many advanced **NLP models**\n",
    "\n",
    "> üß† *\"If feedforward networks are about 'what' the data is, RNNs are about 'when' the data is.\"*\n",
    "\n",
    "Understanding this concept sets a strong foundation for building more intelligent, memory-aware deep learning models.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è≠Ô∏è Coming Next\n",
    "\n",
    "In the next notebook/video, we will cover **Backward Propagation Through Time (BPTT)** ‚Äì the learning algorithm used to train RNNs by minimizing the loss function over sequential data.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
