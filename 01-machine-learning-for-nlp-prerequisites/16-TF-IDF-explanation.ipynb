{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Natural Language Processing - TF-IDF (Term Frequency–Inverse Document Frequency)\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "We're going to explore how TF-IDF works as a method to convert text into numerical vectors.\n",
    "\n",
    "Overview:\n",
    "---------\n",
    "TF-IDF combines two concepts:\n",
    "1. Term Frequency (TF): Measures how often a word appears in a document.\n",
    "2. Inverse Document Frequency (IDF): Penalizes words that occur in many documents (less unique).\n",
    "\n",
    "TF = (Number of times term appears in sentence) / (Total number of words in sentence)\n",
    "\n",
    "IDF = log_e(Total number of sentences / Number of sentences containing the word)\n",
    "\n",
    "TF-IDF = TF * IDF\n",
    "\n",
    "We’ll work through an example with these three sentences (already lowercased and stopwords removed):\n",
    "\n",
    "S1 = \"good boy\"\n",
    "S2 = \"good girl\"\n",
    "S3 = \"boy girl good\"\n",
    "\n",
    "Step 1: Vocabulary\n",
    "------------------\n",
    "From the three sentences, our vocabulary (unique words) is:\n",
    "['boy', 'girl', 'good']\n",
    "\n",
    "Step 2: Term Frequency (TF)\n",
    "---------------------------\n",
    "Calculate TF for each word in each sentence:\n",
    "\n",
    "Sentence 1: \"good boy\"\n",
    "- good: 1/2\n",
    "- boy: 1/2\n",
    "- girl: 0\n",
    "\n",
    "Sentence 2: \"good girl\"\n",
    "- good: 1/2\n",
    "- girl: 1/2\n",
    "- boy: 0\n",
    "\n",
    "Sentence 3: \"boy girl good\"\n",
    "- good: 1/3\n",
    "- boy: 1/3\n",
    "- girl: 1/3\n",
    "\n",
    "Step 3: Inverse Document Frequency (IDF)\n",
    "----------------------------------------\n",
    "Total number of sentences = 3\n",
    "\n",
    "Calculate how many sentences contain each word:\n",
    "- good → 3 sentences → IDF = log(3/3) = 0\n",
    "- boy → 2 sentences → IDF = log(3/2)\n",
    "- girl → 2 sentences → IDF = log(3/2)\n",
    "\n",
    "So:\n",
    "- IDF(good) = 0\n",
    "- IDF(boy) ≈ 0.176\n",
    "- IDF(girl) ≈ 0.176\n",
    "\n",
    "Step 4: TF-IDF Calculation\n",
    "--------------------------\n",
    "TF-IDF = TF * IDF\n",
    "\n",
    "Sentence 1:\n",
    "- good: (1/2) * 0     = 0\n",
    "- boy:  (1/2) * 0.176 ≈ 0.088\n",
    "- girl: 0 * 0.176     = 0\n",
    "\n",
    "Sentence 2:\n",
    "- good: (1/2) * 0     = 0\n",
    "- boy:  0 * 0.176     = 0\n",
    "- girl: (1/2) * 0.176 ≈ 0.088\n",
    "\n",
    "Sentence 3:\n",
    "- good: (1/3) * 0     = 0\n",
    "- boy:  (1/3) * 0.176 ≈ 0.059\n",
    "- girl: (1/3) * 0.176 ≈ 0.059\n",
    "\n",
    "Final TF-IDF Vectors:\n",
    "---------------------\n",
    "S1: [boy=0.088, girl=0,     good=0]\n",
    "S2: [boy=0,     girl=0.088, good=0]\n",
    "S3: [boy=0.059, girl=0.059, good=0]\n",
    "\n",
    "Key Insight:\n",
    "------------\n",
    "- Unlike Bag of Words (BoW), TF-IDF gives **lower weight** to common words (like 'good' here, IDF=0).\n",
    "- TF-IDF improves upon BoW by reducing the impact of common but less meaningful terms.\n",
    "\n",
    "Next Steps:\n",
    "-----------\n",
    "- In the following section, we’ll implement TF-IDF using sklearn.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding TF-IDF in NLP\n",
    "\n",
    "In this notebook, we delve deeper into **TF-IDF (Term Frequency - Inverse Document Frequency)**, one of the most widely used techniques in Natural Language Processing (NLP) for text representation. We’ve already discussed the basic formula and seen introductory examples. Now, we will expand on that foundation by understanding **why TF-IDF is often preferred over Bag of Words (BoW)**, along with its **advantages, disadvantages**, and **impact on model performance**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 What You Will Learn\n",
    "\n",
    "- A recap of the TF-IDF formula (Term Frequency and Inverse Document Frequency).\n",
    "- How TF-IDF differs from and improves upon the Bag of Words (BoW) model.\n",
    "- Key advantages of using TF-IDF:\n",
    "  - Intuitive and straightforward to implement.\n",
    "  - Fixed input size, just like BoW (based on vocabulary size).\n",
    "  - **Captures word importance**, a major differentiator compared to BoW.\n",
    "- Real-world interpretation of how TF-IDF values reflect the importance of words in a paragraph or document.\n",
    "- Understanding how TF-IDF helps improve model performance by emphasizing contextually relevant words.\n",
    "- Disadvantages of TF-IDF:\n",
    "  - **Sparsity**: The presence of many zeroes in feature vectors.\n",
    "  - **Out of Vocabulary (OOV)**: New words in test data not present in training data get ignored.\n",
    "\n",
    "---\n",
    "\n",
    "## ❓ Why Do We Need TF-IDF?\n",
    "\n",
    "While Bag of Words is simple and effective for basic text representation, it treats all words equally—ignoring their actual relevance within and across documents. TF-IDF solves this by:\n",
    "\n",
    "- Penalizing words that appear too frequently across documents (like \"the\", \"is\", \"and\").\n",
    "- Highlighting words that are **unique** and **contextually significant** in a specific document.\n",
    "- Providing a more meaningful numerical representation of textual data for machine learning algorithms.\n",
    "  \n",
    "This allows models to **focus on discriminative features**, improving performance in tasks like classification, clustering, and information retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Conclusion\n",
    "\n",
    "TF-IDF is a crucial step in transforming raw text into a usable format for machine learning models. Unlike Bag of Words, it adds a layer of intelligence by considering the **importance of each word based on its frequency and distribution**. Though it shares some limitations like sparsity and handling OOV words, its strengths make it a better option in most NLP tasks.\n",
    "\n",
    "In the next section, we’ll move on to practical implementation using **Python and NLTK**, and explore how this representation can be applied to real-world datasets.\n",
    "\n",
    "> 🧠 *Tip: Practice on varied datasets to solidify your understanding. Assignments will be provided to help with hands-on learning.*\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
