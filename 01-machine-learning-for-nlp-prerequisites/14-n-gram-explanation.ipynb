{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÅ Natural Language Processing: Understanding N-grams\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "In this section, we explore **N-grams**, a powerful technique in NLP that captures the context between words by grouping them into contiguous sequences. N-grams are especially useful when Bag of Words or TF-IDF fails to distinguish between similar but **contextually opposite sentences**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì Why Do We Need N-grams?\n",
    "\n",
    "- Standard **Bag of Words** and **TF-IDF** treat each word **independently**.\n",
    "- Context is lost in sentences like:\n",
    "  - ‚Äú**Food is good**‚Äù\n",
    "  - ‚Äú**Food is not good**‚Äù\n",
    "- These two are **opposites**, yet BoW may represent them similarly due to word frequency alone.\n",
    "- **N-grams** solve this by preserving **word sequences**, adding **contextual depth**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Example: Sentences\n",
    "\n",
    "```text\n",
    "Sentence 1: \"Food is good\"  \n",
    "Sentence 2: \"Food is not good\"\n",
    "```\n",
    "\n",
    "## Without N-grams (Unigram BoW):\n",
    "\n",
    "| Word | S1 | S2 |\n",
    "| ---- | -- | -- |\n",
    "| food | 1  | 1  |\n",
    "| good | 1  | 1  |\n",
    "| not  | 0  | 1  |\n",
    "\n",
    "Only one word (‚Äúnot‚Äù) differs. Sentences appear similar numerically, but are semantically opposite.\n",
    "\n",
    "## üîó With N-grams (Bigram)\n",
    "### We now consider word pairs:\n",
    "\n",
    "| Bigram    | S1 | S2 |\n",
    "| --------- | -- | -- |\n",
    "| food good | 1  | 0  |\n",
    "| food not  | 0  | 1  |\n",
    "| not good  | 0  | 1  |\n",
    "\n",
    "Bigram vectors now reflect major differences between the sentences.\n",
    "\n",
    "\n",
    "## üìä Types of N-grams\n",
    "\n",
    "| Type    | n-gram Range | Description                   |\n",
    "| ------- | ------------ | ----------------------------- |\n",
    "| Unigram | (1,1)        | Individual words only         |\n",
    "| Bigram  | (2,2)        | Consecutive word pairs        |\n",
    "| Trigram | (3,3)        | Consecutive 3-word sequences  |\n",
    "| Uni+Bi  | (1,2)        | Unigrams and Bigrams combined |\n",
    "| Bi+Tri  | (2,3)        | Bigrams and Trigrams only     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è How to Use in scikit-learn\n",
    "```python\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    // Example for Unigram + Bigram\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "```\n",
    "\n",
    "- ngram_range=(1, 2) ‚Üí includes unigrams and bigrams\n",
    "- ngram_range=(2, 3) ‚Üí includes bigrams and trigrams only\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusion\n",
    "- N-grams are essential for context-aware NLP tasks.\n",
    "- They improve model understanding, especially in sentiment and text classification.\n",
    "- By choosing the right ngram_range, we can preserve phrase-level meaning.\n",
    "- In the next step, we‚Äôll implement N-gram vectorization and test its impact on a model's performance.\n",
    "\n",
    "üîÅ Try experimenting with (1,3) to include unigram, bigram, and trigram together."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
