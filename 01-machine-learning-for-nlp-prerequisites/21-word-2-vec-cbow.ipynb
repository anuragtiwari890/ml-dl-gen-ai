{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Word2Vec with CBOW Architecture\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before diving into Word2Vec, ensure you're familiar with the following:\n",
    "\n",
    "- **Artificial Neural Networks (ANN)**\n",
    "- **Loss Functions**\n",
    "- **Optimizers**\n",
    "\n",
    "---\n",
    "\n",
    "## What is Word2Vec?\n",
    "\n",
    "Word2Vec is a popular word embedding technique that converts words into numerical vector representations. There are two primary architectures:\n",
    "\n",
    "- **CBOW (Continuous Bag of Words)**\n",
    "- **Skip-gram**\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-trained vs. Custom Word2Vec\n",
    "\n",
    "- **Pre-trained** Word2Vec models (e.g., Google’s model trained on 3 billion words) offer fast implementations.\n",
    "- **Custom training** allows us to understand internal workings and tailor embeddings to specific datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## Sample Corpus\n",
    "\n",
    "Let's use a simple sentence for demonstration:\n",
    "\n",
    "\"I neuron company is related to data science\"\n",
    "\n",
    "\n",
    "This corpus consists of 7 words.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Defining Window Size\n",
    "\n",
    "Let’s use a **window size of 5** (an odd number to identify a clear center word).\n",
    "\n",
    "From the phrase:\n",
    "\n",
    "I neuron company is related to\n",
    "\n",
    "\n",
    "- **Input**: `I`, `neuron`, `company`, `related`, `to`\n",
    "- **Center Word (Output)**: `is`\n",
    "\n",
    "Here’s how input-output pairs are generated using a sliding window:\n",
    "\n",
    "1. **Input**: `I neuron company related to` → **Output**: `is`\n",
    "2. **Input**: `neuron company is to data` → **Output**: `related`\n",
    "3. **Input**: `company is related data science` → **Output**: `to`\n",
    "4. *(and so on, based on corpus length)*\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: One-Hot Encoding\n",
    "\n",
    "Each word in the vocabulary is represented as a **one-hot encoded vector**:\n",
    "\n",
    "Example Vocabulary:\n",
    "\n",
    "['I', 'neuron', 'company', 'is', 'related', 'to', 'data', 'science']\n",
    "\n",
    "\n",
    "If vocabulary size = `V = 8`, each word will be encoded as a `1 x 8` vector:\n",
    "\n",
    "- `neuron`: `[0, 1, 0, 0, 0, 0, 0, 0]`\n",
    "- `company`: `[0, 0, 1, 0, 0, 0, 0, 0]`\n",
    "- `is`: `[0, 0, 0, 1, 0, 0, 0, 0]`\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: CBOW Architecture Using ANN\n",
    "\n",
    "CBOW is implemented using a **fully connected neural network (ANN)**.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "1. **Input Layer**:\n",
    "    - Input size = number of context words (`4`)\n",
    "    - Each word = one-hot vector of size `V`\n",
    "    - Total input = `4 x V`\n",
    "\n",
    "2. **Hidden Layer**:\n",
    "    - Size = Embedding dimensions (`N`, e.g., 5)\n",
    "    - Each word is projected to a `1 x N` vector\n",
    "    - Hidden layer output = average of context word embeddings\n",
    "\n",
    "3. **Output Layer**:\n",
    "    - Outputs a vector of size `V`\n",
    "    - Softmax is applied to generate probability distribution\n",
    "    - Target is the center word’s one-hot vector\n",
    "\n",
    "### Visualization:\n",
    "\n",
    "Input Words (one-hot) → Hidden Layer (averaging) → Output Word (Softmax)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Training the CBOW Model\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "- Each context word is converted to its one-hot vector.\n",
    "- These vectors are multiplied by a weight matrix `W1` (`V x N`) to get word embeddings.\n",
    "- These embeddings are averaged to form the hidden layer representation.\n",
    "- Hidden layer output is multiplied with `W2` (`N x V`) to get output logits.\n",
    "- Apply **Softmax** to get prediction `ŷ`.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Use **cross-entropy loss** between predicted vector `ŷ` and actual one-hot vector `y`.\n",
    "\n",
    "\\[\n",
    "\\mathcal{L} = -\\sum_{i=1}^{V} y_i \\log(\\hat{y}_i)\n",
    "\\]\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "- Calculate gradients using the loss\n",
    "- Update `W1` and `W2` using an **optimizer** like SGD or Adam\n",
    "- Repeat until the loss converges\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Generating Word Vectors\n",
    "\n",
    "Once training completes:\n",
    "\n",
    "- Each word has an embedding (vector) of size `N`\n",
    "- For example:\n",
    "    - `\"neuron\"` → `[0.94, 0.32, 0.56, 0.21, 0.88]`\n",
    "    - `\"company\"` → `[0.65, 0.22, 0.77, 0.13, 0.43]`\n",
    "\n",
    "These vectors capture **semantic relationships** between words.\n",
    "\n",
    "---\n",
    "\n",
    "## Recap\n",
    "\n",
    "- CBOW predicts a center word using surrounding context words.\n",
    "- It uses a basic feedforward neural network with:\n",
    "    - One-hot encoded inputs\n",
    "    - Hidden layer with reduced dimensionality\n",
    "    - Output layer with Softmax\n",
    "- Final embeddings are stored in `W1` matrix (input to hidden)\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of CBOW\n",
    "\n",
    "- Faster to train than Skip-gram\n",
    "- Works well for frequent words\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "- May not perform well with rare words\n",
    "\n",
    "---\n",
    "\n",
    "## What’s Next?\n",
    "\n",
    "In the next lesson, we will discuss the **Skip-gram** architecture and compare it with CBOW.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
