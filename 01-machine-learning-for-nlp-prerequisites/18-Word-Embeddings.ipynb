{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Word Embeddings in NLP – Introduction & Overview\n",
    "\n",
    "In this notebook, we dive into **Word Embeddings**, a core concept in Natural Language Processing (NLP) that enables machines to understand the semantic meaning of words by representing them as dense vectors. This lesson marks a transition from traditional word vectorization methods to more advanced, deep learning–based techniques like **Word2Vec**.\n",
    "\n",
    "---\n",
    "\n",
    "## ❓ Why Do We Need Word Embeddings?\n",
    "\n",
    "Traditional word vectorization methods such as **One-Hot Encoding**, **Bag of Words**, and **TF-IDF** are simple and effective but come with significant limitations:\n",
    "\n",
    "| Method           | Vector Type       | Limitations                                                  |\n",
    "|------------------|-------------------|---------------------------------------------------------------|\n",
    "| One-Hot Encoding | Sparse, Binary    | No semantic meaning, very high dimensionality                |\n",
    "| Bag of Words     | Sparse Frequency  | Ignores word order, no context awareness, large vocab size   |\n",
    "| TF-IDF           | Weighted Sparse   | No semantic relationships, sparsity remains                  |\n",
    "\n",
    "### 🔍 Key Issues with These Methods\n",
    "- Words are treated independently without understanding their meaning\n",
    "- Vectors are sparse (mostly zeros), leading to inefficient storage and processing\n",
    "- No relationship captured between similar or related words (e.g., “happy” and “excited”)\n",
    "\n",
    "### ✅ What Word Embeddings Solve\n",
    "- Convert words into **dense, real-valued vectors**\n",
    "- Preserve **semantic similarity** (e.g., similar words have similar vector representations)\n",
    "- Make downstream machine learning models **more accurate and efficient**\n",
    "- Eliminate sparsity issues common with traditional vectorization\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 What You Will Learn\n",
    "\n",
    "- What **word embeddings** are and why they’re needed in NLP\n",
    "- How word meanings are preserved in vector space\n",
    "- Overview of two types of embedding techniques:\n",
    "  1. **Frequency-based techniques**\n",
    "     - One-Hot Encoding\n",
    "     - Bag of Words\n",
    "     - TF-IDF\n",
    "  2. **Deep learning–based embeddings**\n",
    "     - **Word2Vec**: a more powerful and context-aware approach\n",
    "- The two architectures within Word2Vec:\n",
    "  - **CBOW (Continuous Bag of Words)**: Predicts a word from its context\n",
    "  - **Skip-Gram**: Predicts context words from a single word\n",
    "- Brief on **pre-trained embeddings** (like Google’s 1.5GB Word2Vec model)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Conceptual Analogy: Semantic Proximity Without a Graph\n",
    "\n",
    "Think of the following relationships between words:\n",
    "\n",
    "- **\"happy\"** and **\"excited\"** → have similar meanings → should have **similar vectors**\n",
    "- **\"happy\"** and **\"angry\"** → opposite meanings → should have **dissimilar vectors**\n",
    "\n",
    "In a well-trained word embedding model, these semantic relationships are encoded in such a way that **the vector distance reflects meaning** — closer vectors for similar words, farther vectors for unrelated or opposite words.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Word2Vec: A Glimpse\n",
    "\n",
    "| Architecture | Predicts              | Works Best For             | Training Style             |\n",
    "|--------------|-----------------------|-----------------------------|-----------------------------|\n",
    "| **CBOW**     | Target word from context | Frequent words, faster training | Averages surrounding words |\n",
    "| **Skip-Gram**| Context words from target | Rare words, deeper learning | One word to many contexts  |\n",
    "\n",
    "Word2Vec embeddings are **learned representations** trained on large corpora. They retain meaningful relationships like:\n",
    "- **king - man + woman ≈ queen**\n",
    "- **Paris - France + Italy ≈ Rome**\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Conclusion\n",
    "\n",
    "Word Embeddings revolutionized NLP by transforming words into **dense, context-aware vectors** that preserve both **syntactic** and **semantic** relationships. This overcomes the key drawbacks of traditional vectorization methods like Bag of Words or TF-IDF.\n",
    "\n",
    "In the next lessons, we will:\n",
    "- Explore how **Word2Vec** works in detail\n",
    "- Understand how it solves sparsity and context issues\n",
    "- Load and use **pre-trained Word2Vec models** to apply these concepts\n",
    "\n",
    "> 💡 *Every vectorization technique you've learned so far falls under the umbrella of word embeddings. What sets Word2Vec apart is its ability to embed **meaning** into vectors, not just presence or frequency.*\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
