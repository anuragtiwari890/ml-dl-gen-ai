{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Word2Vec â€“ A Deep Dive\n",
    "\n",
    "Welcome back! Today, we continue our journey in **Natural Language Processing (NLP)** with an in-depth discussion about **Word2Vec** â€” a powerful word embedding technique based on deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Word2Vec?\n",
    "\n",
    "- Word2Vec is a **deep learningâ€“based word embedding** technique.\n",
    "- It converts words into **dense vectors** while maintaining their **semantic meaning**.\n",
    "- Words with similar meanings get vectors **close to each other**.\n",
    "- Words that are opposites or unrelated have vectors that are **far apart**.\n",
    "- Word2Vec helps detect **synonyms**, suggests additional words for partial sentences, and captures meaningful word relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## Background and Importance\n",
    "\n",
    "- Word2Vec was introduced by **Google in 2013**.\n",
    "- It uses a **neural network** to learn word associations from large text corpora.\n",
    "- Unlike sparse representations like One-Hot Encoding or TF-IDF, Word2Vec creates **dense vector representations**.\n",
    "- These vectors encode rich linguistic features allowing machines to understand nuances between words.\n",
    "\n",
    "---\n",
    "\n",
    "## How Does Word2Vec Represent Words?\n",
    "\n",
    "### Vocabulary and Feature Representation\n",
    "\n",
    "- Vocabulary: The set of **unique words** in a text corpus.\n",
    "- Each word in the vocabulary is mapped to a **feature vector**.\n",
    "- The vector dimensions (e.g., 100, 300) represent **abstract latent features** learned during training.\n",
    "- These features could relate to semantic concepts like **gender, royalty, age, or food categories** â€” though in large models, these features arenâ€™t explicitly known but inferred.\n",
    "\n",
    "### Example of Feature Representation (Intuitive)\n",
    "\n",
    "| Word  | Gender | Royalty | Age  | Food  | ... | Dimension 300 |\n",
    "|--------|--------|---------|------|-------|-----|----------------|\n",
    "| Boy    | -1     | 0.01    | 0.03 | 0.00  | ... | Vector of 300 dims |\n",
    "| Girl   | +1     | 0.02    | 0.01 | 0.00  | ... | Vector of 300 dims |\n",
    "| King   | -0.92  | 0.95    | 0.75 | 0.00  | ... | Vector of 300 dims |\n",
    "| Queen  | +0.93  | 0.96    | 0.68 | 0.00  | ... | Vector of 300 dims |\n",
    "| Apple  | 0.01   | 0.00    | 0.20 | 0.91  | ... | Vector of 300 dims |\n",
    "\n",
    "- Values close to zero indicate little or no relationship.\n",
    "- Opposite words have opposing signs on certain features (e.g., Boy vs. Girl in gender).\n",
    "- Similar words have vectors close in the feature space (e.g., King and Queen in royalty).\n",
    "\n",
    "---\n",
    "\n",
    "## Semantic Vector Arithmetic with Word2Vec\n",
    "\n",
    "A famous example from Googleâ€™s Word2Vec research shows how vectors encode meaning:\n",
    "\n",
    "king - man + woman â‰ˆ queen\n",
    "\n",
    "\n",
    "- Subtracting \"man\" vector from \"king\" removes the male attribute.\n",
    "- Adding \"woman\" vector adds the female attribute.\n",
    "- Resulting vector points close to \"queen\".\n",
    "\n",
    "Similarly:\n",
    "\n",
    "king - boy + queen â‰ˆ girl\n",
    "\n",
    "\n",
    "This demonstrates the **power of vector arithmetic** in semantic space.\n",
    "\n",
    "---\n",
    "\n",
    "## Dimensionality and Vector Space\n",
    "\n",
    "- Common Word2Vec vectors have **100 to 300 dimensions**.\n",
    "- These dimensions capture complex linguistic and semantic relationships.\n",
    "- Example with simplified 2D vectors for visualization:\n",
    "\n",
    "| Word  | Dim 1 | Dim 2 |\n",
    "|--------|-------|-------|\n",
    "| King   | 0.95  | 0.96  |\n",
    "| Queen  | 0.96  | 0.97  |\n",
    "| Man    | 0.95  | 0.98  |\n",
    "| Human  | 0.94  | 0.96  |\n",
    "\n",
    "Using vector arithmetic, King - Man + Queen results near Human in this 2D space.\n",
    "\n",
    "---\n",
    "\n",
    "## Measuring Similarity: Cosine Similarity\n",
    "\n",
    "- To find how similar two word vectors are, **cosine similarity** is used.\n",
    "- Cosine similarity measures the **angle between two vectors**:\n",
    "\n",
    "\\[\n",
    "\\text{cosine similarity} = \\cos(\\theta) = \\frac{A \\cdot B}{||A|| \\times ||B||}\n",
    "\\]\n",
    "\n",
    "- Similar vectors have angles close to 0Â°, cosine similarity close to 1.\n",
    "- Dissimilar vectors have angles close to 90Â°, cosine similarity close to 0.\n",
    "- **Distance between vectors = 1 - cosine similarity**.\n",
    "\n",
    "### Example:\n",
    "\n",
    "- Angle between vectors = 45Â°\n",
    "- Cosine 45Â° â‰ˆ 0.707\n",
    "- Distance = 1 - 0.707 = 0.293 â†’ fairly similar words.\n",
    "\n",
    "If vectors are orthogonal (90Â°):\n",
    "\n",
    "- Cosine 90Â° = 0\n",
    "- Distance = 1 â†’ completely different words.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Implications\n",
    "\n",
    "- Word2Vec vectors enable **recommendation systems** and **semantic search**.\n",
    "- For example, in a movie recommender system:\n",
    "  - \"Avengers\" vector is close to \"Iron Man\" vector because of shared features like genre (action, comic).\n",
    "- This analogy helps generalize Word2Vec beyond just words to any domain where features and similarity matter.\n",
    "\n",
    "---\n",
    "\n",
    "## Whatâ€™s Next?\n",
    "\n",
    "- Upcoming videos will explain:\n",
    "  - How Word2Vec is **trained from scratch** using neural networks.\n",
    "  - The **architectures behind Word2Vec**: CBOW and Skip-Gram.\n",
    "  - How **loss functions and optimizers** are used.\n",
    "  - Practical implementations with popular datasets and pre-trained models.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Word2Vec is a **game-changing NLP technique** converting words into dense vectors encoding rich semantic relationships.\n",
    "- Each word is represented as a **feature vector** in a high-dimensional space.\n",
    "- Vector operations reveal meaningful relationships between words.\n",
    "- Cosine similarity measures closeness of word meanings.\n",
    "- Understanding Word2Vec lays the foundation for many modern NLP applications.\n",
    "\n",
    "---\n",
    "\n",
    "# Keep exploring! ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
