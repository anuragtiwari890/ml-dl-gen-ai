{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture Title: Tokenization and Basic NLP Terminologies\n",
    "\n",
    "#### Lecture Summary:\n",
    "\n",
    "In this foundational lecture on Natural Language Processing (NLP), the instructor introduces key terminologies crucial for understanding and working with textual data. These concepts form the building blocks for more advanced topics in NLP.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Concepts Covered:\n",
    "\n",
    "1. **Corpus**  \n",
    "   - A **corpus** is a large collection of text, often referred to as a paragraph or a body of text used for analysis.  \n",
    "   - Example: A paragraph with multiple sentences can be treated as a corpus.\n",
    "\n",
    "2. **Documents/Sentences**  \n",
    "   - A **document** refers to individual sentences or text units within a corpus.  \n",
    "   - These are the smaller components extracted from a corpus during preprocessing.\n",
    "\n",
    "3. **Vocabulary**  \n",
    "   - The **vocabulary** of a corpus consists of all **unique words** present in the text.  \n",
    "   - It is similar to a dictionary where every distinct word is counted once.\n",
    "\n",
    "4. **Words**  \n",
    "   - Words are the basic units of text data.  \n",
    "   - Every word (including repeated ones) in the corpus is considered in NLP preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "#### What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units called **tokens**. These tokens can be:  \n",
    "- **Sentences**: Tokenizing a paragraph into individual sentences.  \n",
    "- **Words**: Tokenizing sentences into individual words.\n",
    "\n",
    "##### Example Flow:\n",
    "- Paragraph → Sentence Tokens  \n",
    "  _e.g., \"I like to drink apple juice. My friend likes mango juice.\"_  \n",
    "  → [\"I like to drink apple juice\", \"My friend likes mango juice\"]\n",
    "\n",
    "- Sentence → Word Tokens  \n",
    "  _e.g., \"I like to drink apple juice\"_  \n",
    "  → [\"I\", \"like\", \"to\", \"drink\", \"apple\", \"juice\"]\n",
    "\n",
    "Tokenization helps in:  \n",
    "- Preprocessing text before applying machine learning models.  \n",
    "- Converting words into numerical vectors for computation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Importance in NLP:\n",
    "\n",
    "Tokenization is one of the **first and most essential steps** in text preprocessing. It enables machines to understand and manipulate human language by segmenting it into manageable pieces.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
