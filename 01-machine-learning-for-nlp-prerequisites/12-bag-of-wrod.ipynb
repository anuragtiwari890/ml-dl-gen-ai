{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò NLP Notes - Bag of Words (BoW) üî°\n",
    "\n",
    "## üìç Lecture Overview\n",
    "\n",
    "- ‚úÖ Review: One-Hot Encoding  \n",
    "- üöÄ Introduction to Bag of Words  \n",
    "- üõ†Ô∏è Step-by-step BoW implementation  \n",
    "- üìä Binary BoW vs Count BoW  \n",
    "- ‚úÖ Advantages & ‚ùå Disadvantages  \n",
    "- üí° Real-world considerations  \n",
    "- üß† Semantic issues & limitations  \n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Recap: From One-Hot Encoding to Bag of Words\n",
    "\n",
    "In **One-Hot Encoding**, we represented **each word** with a binary vector of size equal to the **vocabulary**. For each **word**, we created a separate vector.\n",
    "\n",
    "**Limitations we faced:**\n",
    "- Sparse matrices\n",
    "- Variable-length input\n",
    "- No semantic meaning\n",
    "- Out-of-vocabulary issues\n",
    "\n",
    "So now, we move to a **better method for entire text/sentences**: **Bag of Words (BoW)**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Step-by-Step: How Bag of Words Works\n",
    "\n",
    "### ‚úèÔ∏è Sample Sentences\n",
    "\n",
    "We start with a simple dataset:\n",
    "\n",
    "- S1: He is a good boy\n",
    "- S2: She is a good girl\n",
    "- S3: Boy and girl are good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### üßæ Step 2: Build Vocabulary\n",
    "\n",
    "From the cleaned sentences, we extract unique words:\n",
    "\n",
    "Vocabulary = ['good', 'boy', 'girl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We also note **frequencies**:\n",
    "\n",
    "| Word  | Frequency |\n",
    "|-------|-----------|\n",
    "| good  | 3         |\n",
    "| boy   | 2         |\n",
    "| girl  | 2         |\n",
    "\n",
    "This is useful to **prioritize features** or to **limit vocabulary** (e.g., top 10 frequent words).\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Step 3: Convert Sentences to Vectors\n",
    "\n",
    "We now build **BoW vectors** based on vocabulary order.\n",
    "\n",
    "#### üìÑ Sentence 1: good boy ‚Üí Vector: [1, 1, 0]  \n",
    "#### üìÑ Sentence 2: good girl ‚Üí Vector: [1, 0, 1]  \n",
    "#### üìÑ Sentence 3: boy girl good ‚Üí Vector: [1, 1, 1]\n",
    "\n",
    "Each vector represents **presence (1) or absence (0)** of each word in the sentence.\n",
    "\n",
    "> This is also known as **Binary Bag of Words**\n",
    "\n",
    "---\n",
    "\n",
    "## üÜö Count BoW vs Binary BoW\n",
    "\n",
    "Let‚Äôs take this sentence:\n",
    "\n",
    "good girl girl\n",
    "\n",
    "### üßÆ Count BoW:\n",
    "\n",
    "Vector ‚Üí [1, 0, 2] # good (1), boy (0), girl (2)\n",
    "\n",
    "### ‚ö° Binary BoW:\n",
    "\n",
    "Vector ‚Üí [1, 0, 1] # Regardless of repetitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So,\n",
    "- **Binary BoW**: Uses 1/0 (present or not)\n",
    "- **Count BoW**: Uses actual **frequency count**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Advantages of Bag of Words\n",
    "\n",
    "1. **Simple & Intuitive**\n",
    "   - Easy to implement (e.g., using `CountVectorizer` from `sklearn`).\n",
    "\n",
    "2. **Fixed-Length Vectors**\n",
    "   - Unlike one-hot, BoW provides fixed-size input for all documents, matching vocabulary size.\n",
    "\n",
    "3. **Effective for Text Classification**\n",
    "   - Works well for spam/ham detection, sentiment analysis, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Disadvantages of Bag of Words\n",
    "\n",
    "1. **Still a Sparse Matrix**\n",
    "   - For large vocabularies (e.g. 50,000 words), most entries will still be 0 ‚Üí inefficient.\n",
    "\n",
    "2. **No Semantic Understanding**\n",
    "   - ‚Äúgreat‚Äù and ‚Äúawesome‚Äù are treated as different, unrelated words.\n",
    "\n",
    "3. **Out-of-Vocabulary (OOV)**\n",
    "   - New words not seen during training (e.g., ‚Äúschool‚Äù) are ignored during test time.\n",
    "\n",
    "4. **Loss of Word Order**\n",
    "   - \"The food is good\" vs \"The food is not good\" ‚Üí both may seem similar in BoW.\n",
    "   - Can lead to wrong interpretations (e.g., missing negation).\n",
    "\n",
    "5. **Misleading Similarity**\n",
    "   - Cosine similarity between:\n",
    "     - ‚ÄúThe food is good‚Äù\n",
    "     - ‚ÄúThe food is not good‚Äù  \n",
    "     May be **high**, even though the **sentiments are opposite**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary Table\n",
    "\n",
    "| Feature                   | Bag of Words      |\n",
    "|--------------------------|-------------------|\n",
    "| Easy to Implement         | ‚úÖ Yes            |\n",
    "| Sparse Matrix             | ‚ö†Ô∏è Yes            |\n",
    "| Fixed Input Size          | ‚úÖ Yes            |\n",
    "| Semantic Understanding    | ‚ùå No             |\n",
    "| Word Order Preserved      | ‚ùå No             |\n",
    "| Handles OOV               | ‚ùå Poorly         |\n",
    "| Frequency-Based Options   | ‚úÖ Yes (Count BoW) |\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Python Example: Using `CountVectorizer`\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample sentences\n",
    "docs = [\n",
    "    \"He is a good boy\",\n",
    "    \"She is a good girl\",\n",
    "    \"Boy and girl are good\"\n",
    "]\n",
    "\n",
    "# Preprocess: lowercase and remove stopwords manually if needed\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "X = cv.fit_transform(docs)\n",
    "\n",
    "# Convert to array\n",
    "print(cv.get_feature_names_out())\n",
    "print(X.toarray())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
