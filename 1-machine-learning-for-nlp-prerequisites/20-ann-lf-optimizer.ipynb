{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundational Concepts for Word2Vec\n",
    "\n",
    "To truly understand how models like Word2Vec work under the hood (especially CBOW and Skip-gram), it's essential to grasp these three foundational components:\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 1. Artificial Neural Networks (ANN)\n",
    "\n",
    "### What is an ANN?\n",
    "An **Artificial Neural Network** is a computational model inspired by the human brain. It's made up of layers of **neurons** (also called nodes) that process input data and learn to make predictions.\n",
    "\n",
    "### Basic Structure\n",
    "Input Layer → Hidden Layer(s) → Output Layer\n",
    "\n",
    "\n",
    "Each **layer** is made of neurons, and each neuron performs:\n",
    "- Weighted sum of its inputs\n",
    "- Applies a **non-linear activation function**\n",
    "\n",
    "### Components:\n",
    "- **Input layer**: Takes in feature data (e.g., one-hot vectors for words).\n",
    "- **Hidden layer(s)**: Perform computations to learn patterns/features.\n",
    "- **Output layer**: Produces final prediction (e.g., probability of a word).\n",
    "\n",
    "### Example:\n",
    "For CBOW:\n",
    "- Input = context words (one-hot vectors)\n",
    "- Output = center word prediction\n",
    "\n",
    "---\n",
    "\n",
    "## 📉 2. Loss Function\n",
    "\n",
    "### What is a Loss Function?\n",
    "\n",
    "A **loss function** measures how wrong the model's prediction is compared to the true value.\n",
    "\n",
    "> It guides the model on how to adjust its internal weights during training.\n",
    "\n",
    "### Common Loss Functions:\n",
    "\n",
    "#### ✅ Cross-Entropy Loss (used in classification)\n",
    "\\[\n",
    "\\mathcal{L} = -\\sum_{i=1}^{V} y_i \\log(\\hat{y}_i)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y \\) = true label (one-hot encoded)\n",
    "- \\( \\hat{y} \\) = predicted probabilities from the model\n",
    "\n",
    "### Why Important?\n",
    "Lower loss = better performance. The goal is to minimize the loss as much as possible.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 3. Optimizers\n",
    "\n",
    "### What is an Optimizer?\n",
    "\n",
    "An **optimizer** adjusts the neural network's weights to reduce the loss.\n",
    "\n",
    "> Think of it as the engine that drives the learning process in ANN.\n",
    "\n",
    "### How it works:\n",
    "- Uses gradients (from **backpropagation**) to update weights\n",
    "- Applies a **learning rate** to control step size\n",
    "\n",
    "### Common Optimizers:\n",
    "\n",
    "| Optimizer | Description |\n",
    "|----------|-------------|\n",
    "| **SGD** (Stochastic Gradient Descent) | Updates weights using one sample at a time |\n",
    "| **Adam** (Adaptive Moment Estimation) | Combines momentum + adaptive learning rate. Efficient and popular |\n",
    "| **RMSProp** | Adapts learning rate based on recent gradients |\n",
    "\n",
    "### Update Rule (SGD example):\n",
    "\n",
    "\\[\n",
    "w := w - \\eta \\cdot \\nabla \\mathcal{L}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( w \\): weight\n",
    "- \\( \\eta \\): learning rate\n",
    "- \\( \\nabla \\mathcal{L} \\): gradient of loss w.r.t weights\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Component     | Role                                                                 |\n",
    "|---------------|----------------------------------------------------------------------|\n",
    "| **ANN**        | Learns to map input to output through layers of neurons              |\n",
    "| **Loss Function** | Tells the model **how wrong** it is                               |\n",
    "| **Optimizer**     | Helps the model **improve** by adjusting weights                  |\n",
    "\n",
    "These three together form the foundation of deep learning models like Word2Vec."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
