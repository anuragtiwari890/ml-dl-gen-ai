{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Natural Language Processing - TF-IDF (Term Frequencyâ€“Inverse Document Frequency)\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "We're going to explore how TF-IDF works as a method to convert text into numerical vectors.\n",
    "\n",
    "Overview:\n",
    "---------\n",
    "TF-IDF combines two concepts:\n",
    "1. Term Frequency (TF): Measures how often a word appears in a document.\n",
    "2. Inverse Document Frequency (IDF): Penalizes words that occur in many documents (less unique).\n",
    "\n",
    "TF = (Number of times term appears in sentence) / (Total number of words in sentence)\n",
    "\n",
    "IDF = log_e(Total number of sentences / Number of sentences containing the word)\n",
    "\n",
    "TF-IDF = TF * IDF\n",
    "\n",
    "Weâ€™ll work through an example with these three sentences (already lowercased and stopwords removed):\n",
    "\n",
    "S1 = \"good boy\"\n",
    "S2 = \"good girl\"\n",
    "S3 = \"boy girl good\"\n",
    "\n",
    "Step 1: Vocabulary\n",
    "------------------\n",
    "From the three sentences, our vocabulary (unique words) is:\n",
    "['boy', 'girl', 'good']\n",
    "\n",
    "Step 2: Term Frequency (TF)\n",
    "---------------------------\n",
    "Calculate TF for each word in each sentence:\n",
    "\n",
    "Sentence 1: \"good boy\"\n",
    "- good: 1/2\n",
    "- boy: 1/2\n",
    "- girl: 0\n",
    "\n",
    "Sentence 2: \"good girl\"\n",
    "- good: 1/2\n",
    "- girl: 1/2\n",
    "- boy: 0\n",
    "\n",
    "Sentence 3: \"boy girl good\"\n",
    "- good: 1/3\n",
    "- boy: 1/3\n",
    "- girl: 1/3\n",
    "\n",
    "Step 3: Inverse Document Frequency (IDF)\n",
    "----------------------------------------\n",
    "Total number of sentences = 3\n",
    "\n",
    "Calculate how many sentences contain each word:\n",
    "- good â†’ 3 sentences â†’ IDF = log(3/3) = 0\n",
    "- boy â†’ 2 sentences â†’ IDF = log(3/2)\n",
    "- girl â†’ 2 sentences â†’ IDF = log(3/2)\n",
    "\n",
    "So:\n",
    "- IDF(good) = 0\n",
    "- IDF(boy) â‰ˆ 0.176\n",
    "- IDF(girl) â‰ˆ 0.176\n",
    "\n",
    "Step 4: TF-IDF Calculation\n",
    "--------------------------\n",
    "TF-IDF = TF * IDF\n",
    "\n",
    "Sentence 1:\n",
    "- good: (1/2) * 0     = 0\n",
    "- boy:  (1/2) * 0.176 â‰ˆ 0.088\n",
    "- girl: 0 * 0.176     = 0\n",
    "\n",
    "Sentence 2:\n",
    "- good: (1/2) * 0     = 0\n",
    "- boy:  0 * 0.176     = 0\n",
    "- girl: (1/2) * 0.176 â‰ˆ 0.088\n",
    "\n",
    "Sentence 3:\n",
    "- good: (1/3) * 0     = 0\n",
    "- boy:  (1/3) * 0.176 â‰ˆ 0.059\n",
    "- girl: (1/3) * 0.176 â‰ˆ 0.059\n",
    "\n",
    "Final TF-IDF Vectors:\n",
    "---------------------\n",
    "S1: [boy=0.088, girl=0,     good=0]\n",
    "S2: [boy=0,     girl=0.088, good=0]\n",
    "S3: [boy=0.059, girl=0.059, good=0]\n",
    "\n",
    "Key Insight:\n",
    "------------\n",
    "- Unlike Bag of Words (BoW), TF-IDF gives **lower weight** to common words (like 'good' here, IDF=0).\n",
    "- TF-IDF improves upon BoW by reducing the impact of common but less meaningful terms.\n",
    "\n",
    "Next Steps:\n",
    "-----------\n",
    "- In the following section, weâ€™ll implement TF-IDF using sklearn.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding TF-IDF in NLP\n",
    "\n",
    "In this notebook, we delve deeper into **TF-IDF (Term Frequency - Inverse Document Frequency)**, one of the most widely used techniques in Natural Language Processing (NLP) for text representation. Weâ€™ve already discussed the basic formula and seen introductory examples. Now, we will expand on that foundation by understanding **why TF-IDF is often preferred over Bag of Words (BoW)**, along with its **advantages, disadvantages**, and **impact on model performance**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š What You Will Learn\n",
    "\n",
    "- A recap of the TF-IDF formula (Term Frequency and Inverse Document Frequency).\n",
    "- How TF-IDF differs from and improves upon the Bag of Words (BoW) model.\n",
    "- Key advantages of using TF-IDF:\n",
    "  - Intuitive and straightforward to implement.\n",
    "  - Fixed input size, just like BoW (based on vocabulary size).\n",
    "  - **Captures word importance**, a major differentiator compared to BoW.\n",
    "- Real-world interpretation of how TF-IDF values reflect the importance of words in a paragraph or document.\n",
    "- Understanding how TF-IDF helps improve model performance by emphasizing contextually relevant words.\n",
    "- Disadvantages of TF-IDF:\n",
    "  - **Sparsity**: The presence of many zeroes in feature vectors.\n",
    "  - **Out of Vocabulary (OOV)**: New words in test data not present in training data get ignored.\n",
    "\n",
    "---\n",
    "\n",
    "## â“ Why Do We Need TF-IDF?\n",
    "\n",
    "While Bag of Words is simple and effective for basic text representation, it treats all words equallyâ€”ignoring their actual relevance within and across documents. TF-IDF solves this by:\n",
    "\n",
    "- Penalizing words that appear too frequently across documents (like \"the\", \"is\", \"and\").\n",
    "- Highlighting words that are **unique** and **contextually significant** in a specific document.\n",
    "- Providing a more meaningful numerical representation of textual data for machine learning algorithms.\n",
    "  \n",
    "This allows models to **focus on discriminative features**, improving performance in tasks like classification, clustering, and information retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "\n",
    "TF-IDF is a crucial step in transforming raw text into a usable format for machine learning models. Unlike Bag of Words, it adds a layer of intelligence by considering the **importance of each word based on its frequency and distribution**. Though it shares some limitations like sparsity and handling OOV words, its strengths make it a better option in most NLP tasks.\n",
    "\n",
    "In the next section, weâ€™ll move on to practical implementation using **Python and NLTK**, and explore how this representation can be applied to real-world datasets.\n",
    "\n",
    "> ðŸ§  *Tip: Practice on varied datasets to solidify your understanding. Assignments will be provided to help with hands-on learning.*\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
