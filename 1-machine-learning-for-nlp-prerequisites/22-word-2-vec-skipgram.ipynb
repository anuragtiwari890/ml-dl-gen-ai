{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Skip-Gram Architecture in Word2Vec\n",
    "\n",
    "## üîç What You'll Learn\n",
    "\n",
    "In this notebook, we continue our journey into **Natural Language Processing (NLP)** by exploring the **Skip-Gram architecture**, a key component of the Word2Vec model.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Revisit how **CBOW (Continuous Bag of Words)** works.\n",
    "- Understand the **differences** between CBOW and Skip-Gram.\n",
    "- Learn how Skip-Gram is structured in terms of:\n",
    "  - Input and Output layers\n",
    "  - Hidden layer and weight matrices\n",
    "  - Forward and Backward propagation\n",
    "- Visualize how word vectors are trained and extracted.\n",
    "- Learn when to use CBOW vs. Skip-Gram depending on the dataset size.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Skip-Gram vs CBOW ‚Äì Core Concept\n",
    "\n",
    "- **CBOW** predicts the **target word** from a given **context**.\n",
    "- **Skip-Gram** predicts the **context words** given a **target word**.\n",
    "\n",
    "In other words:\n",
    "- **CBOW:** Context ‚Üí Target\n",
    "- **Skip-Gram:** Target ‚Üí Context\n",
    "\n",
    "Both models use the same underlying neural network architecture but reverse the input-output mapping.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Architecture Breakdown\n",
    "\n",
    "- We use a window size (e.g., `window = 5`) to determine the number of context words.\n",
    "- The vocabulary size determines the input/output dimensions.\n",
    "- We initialize weight matrices:\n",
    "  - **Input to Hidden Layer:** `V x N` matrix (e.g., `7x5`)\n",
    "  - **Hidden to Output Layer:** `N x V` matrix (e.g., `5x7`)\n",
    "- One-hot encoding is used for the input word.\n",
    "- The **output** is processed with **softmax**, and **loss** is computed using cross-entropy.\n",
    "- We train the model using **forward and backward propagation**.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Example: Enron and Data Science\n",
    "\n",
    "Using the sentence:  \n",
    "> ‚ÄúEnron company is related to data science.‚Äù\n",
    "\n",
    "- Vocabulary size = 7\n",
    "- Window size = 5\n",
    "- Input word (e.g., ‚Äúis‚Äù) ‚Üí One-hot vector ‚Üí Network predicts surrounding context words.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Training & Performance Tips\n",
    "\n",
    "To improve Word2Vec (CBOW or Skip-Gram):\n",
    "\n",
    "1. **Increase training data** ‚Äì more text helps capture better semantic relationships.\n",
    "2. **Increase window size** ‚Äì larger context window provides more learning signals.\n",
    "3. **Tune vector dimensions** ‚Äì higher-dimensional embeddings can encode more nuanced meanings.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† When to Use CBOW vs Skip-Gram?\n",
    "\n",
    "| Model      | Best Used For       |\n",
    "|------------|---------------------|\n",
    "| **CBOW**   | Smaller datasets     |\n",
    "| **Skip-Gram** | Larger datasets / better for rare words |\n",
    "\n",
    "According to research findings, Skip-Gram performs better with **large corpora** and **rare words**, whereas CBOW is faster and more efficient on **smaller datasets**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç What‚Äôs Next?\n",
    "\n",
    "In the next notebook, we will:\n",
    "\n",
    "- Use a **pre-trained Google Word2Vec model** trained on **3 billion words** from Google News.\n",
    "- Each word will be represented as a **300-dimensional vector**.\n",
    "- Implement Word2Vec using the **Gensim** library.\n",
    "- Also, learn how to train a Word2Vec model from **scratch**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why Do We Need This?\n",
    "\n",
    "Understanding Skip-Gram and CBOW is essential because:\n",
    "\n",
    "- These are foundational models in NLP for creating **dense word embeddings**.\n",
    "- Word2Vec allows machines to understand **semantic similarity** between words.\n",
    "- These embeddings are used in downstream tasks like **text classification**, **machine translation**, **chatbots**, and **semantic search**.\n",
    "\n",
    "---\n",
    "\n",
    "## üí¨ Final Thoughts\n",
    "\n",
    "The Skip-Gram model is incredibly powerful when working with **large datasets** and looking to capture **fine-grained semantic details** between words. While CBOW is faster, Skip-Gram generally provides **better representations** for infrequent or rare words.\n",
    "\n",
    "Having a solid understanding of these architectures‚Äîand how they translate words into meaningful vectors‚Äîis crucial as you move forward in building NLP models or working with tools like **Gensim**, **spaCy**, or **transformers**.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
