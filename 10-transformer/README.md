# Transformer Models

This folder contains notebooks and resources focused on the Transformer architecture, including self-attention, multi-head attention, positional encoding, and encoder-decoder models.

## Topics Covered
- Transformer basics and architecture
- Self-attention and multi-head attention
- Positional encoding
- Encoder and decoder implementation
- Layer normalization

## Environment Setup
- Python 3.8+ or 3.11 recommended
- Create a virtual environment if needed:
  ```bash
  python3 -m venv transformer-env
  source transformer-env/bin/activate
  pip install -r requirements.txt  # if available
  ```
- If using Jupyter, install `notebook`, `numpy`, `torch`, and other dependencies as needed.

## How to Use
Open the notebooks in Jupyter or VS Code and follow the explanations and code examples to learn about Transformers.
