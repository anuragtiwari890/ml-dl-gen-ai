{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization in Action: A Step-by-Step Computational Walkthrough\n",
    "\n",
    "## Overview\n",
    "\n",
    "This Jupyter Notebook serves as a practical, computational companion to our theoretical understanding of **Layer Normalization** within the Transformer architecture. Following our discussions on Self-Attention, Multi-Head Attention, and Positional Encoding, this session is dedicated to a **step-by-step numerical example** of how Layer Normalization is applied to a single token's embedding, demonstrating its critical role in stabilizing activations and facilitating robust training in deep neural networks.\n",
    "\n",
    "Layer Normalization is a cornerstone of Transformer stability, especially when combined with **Residual Connections** (the \"Add\" part of \"Add & Normalize\"). It addresses the challenge of internal covariate shift and allows for deeper, more stable models. This notebook will concretely illustrate the calculation of mean, variance, normalization using epsilon for stability, and the application of the crucial **learnable scale ($\\gamma$) and shift ($\\beta$) parameters** that give the model fine-grained control over the normalized distributions.\n",
    "\n",
    "## Detailed Breakdown of Key Concepts & Computational Example\n",
    "\n",
    "### 1. Contextualizing Layer Normalization in the Transformer\n",
    "\n",
    "Recall the Transformer's Encoder block structure:\n",
    "`Input (Embedding + Positional Encoding)` $\\rightarrow$ `Multi-Head Attention` $\\rightarrow$ **`Add & Normalize`** $\\rightarrow$ `Feed-Forward Network` $\\rightarrow$ **`Add & Normalize`** $\\rightarrow$ `Output for Next Layer`\n",
    "\n",
    "The \"Add & Normalize\" block is applied twice within each Encoder/Decoder layer: once after the Multi-Head Attention sub-layer and once after the Position-wise Feed-Forward Network. Our focus here is on the **Normalization** part of this block.\n",
    "\n",
    "### 2. The Core Mechanism of Layer Normalization: Per-Sample Normalization\n",
    "\n",
    "Unlike Batch Normalization, which normalizes features across a batch, Layer Normalization normalizes activations **across the feature dimension for each individual sample (or token)**. This makes it ideal for Transformer's parallel processing of diverse, variable-length sequences.\n",
    "\n",
    "For a given input vector $X = [x_1, x_2, \\dots, x_D]$ (where D is $d_{model}$), Layer Normalization computes:\n",
    "\n",
    "* **Mean ($\\mu$)**: The average of all elements within that single input vector:\n",
    "    $$ \\mu = \\frac{1}{D} \\sum_{i=1}^{D} x_i $$\n",
    "* **Variance ($\\sigma^2$)**: The spread of the elements within that single input vector:\n",
    "    $$ \\sigma^2 = \\frac{1}{D} \\sum_{i=1}^{D} (x_i - \\mu)^2 $$\n",
    "* **Standard Deviation ($\\sigma$)**: The square root of the variance.\n",
    "* **Normalized Value ($\\hat{x_i}$)**: Each element is normalized using the calculated mean and standard deviation for *its own vector*:\n",
    "    $$ \\hat{x_i} = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} $$\n",
    "    * **Epsilon ($\\epsilon$)**: A very small constant (e.g., $1e^{-5}$) is added to the variance before taking the square root. This prevents **division by zero** in cases where the standard deviation might be zero (e.g., if all elements in the input vector are identical).\n",
    "\n",
    "### 3. The Power of Learnable Scale ($\\gamma$) and Shift ($\\beta$) Parameters\n",
    "\n",
    "A critical addition to the basic normalization is the introduction of two learnable parameters per layer:\n",
    "\n",
    "* **Gamma ($\\gamma$)**: A **scaling factor**.\n",
    "* **Beta ($\\beta$)**: A **shifting (or bias)** factor.\n",
    "\n",
    "After normalization, the final output $Y$ is computed as:\n",
    "$$ Y_i = \\gamma_i \\cdot \\hat{x_i} + \\beta_i $$\n",
    "* **Role and Benefits**: These parameters are learned during the training process. They provide the network with the flexibility to *partially or entirely undo* the normalization if the model determines that the original distribution (or a modified one) is more beneficial for learning. If $\\gamma$ learns to approximate the original standard deviation and $\\beta$ approximates the original mean, the layer can effectively bypass the normalization effect, allowing the model to adapt optimally to complex data distributions and potentially preserve important variations in activation magnitudes. They are typically initialized such that $\\gamma=1$ and $\\beta=0$ for all dimensions.\n",
    "\n",
    "### 4. Step-by-Step Computational Example (Token Embedding: `[2.0, 4.0, 6.0, 8.0]`)\n",
    "\n",
    "This section mirrors the lecture's detailed calculation for a single token's embedding to illustrate the process concretely:\n",
    "\n",
    "**Given:**\n",
    "* Input token embedding $X = [2.0, 4.0, 6.0, 8.0]$ (where $D=4$)\n",
    "* Initial $\\gamma = [1.0, 1.0, 1.0, 1.0]$\n",
    "* Initial $\\beta = [0.0, 0.0, 0.0, 0.0]$\n",
    "* $\\epsilon = 1e^{-5}$\n",
    "\n",
    "**Step-by-Step Calculation:**\n",
    "\n",
    "1.  **Compute Mean ($\\mu$)**:\n",
    "    $$ \\mu = \\frac{1}{4} (2.0 + 4.0 + 6.0 + 8.0) = \\frac{20.0}{4} = 5.0 $$\n",
    "\n",
    "2.  **Compute Variance ($\\sigma^2$)**:\n",
    "    $$ \\sigma^2 = \\frac{1}{4} [ (2.0 - 5.0)^2 + (4.0 - 5.0)^2 + (6.0 - 5.0)^2 + (8.0 - 5.0)^2 ] $$\n",
    "    $$ \\sigma^2 = \\frac{1}{4} [ (-3.0)^2 + (-1.0)^2 + (1.0)^2 + (3.0)^2 ] $$\n",
    "    $$ \\sigma^2 = \\frac{1}{4} [ 9.0 + 1.0 + 1.0 + 9.0 ] = \\frac{20.0}{4} = 5.0 $$\n",
    "\n",
    "3.  **Compute Standard Deviation ($\\sigma$) with Epsilon**:\n",
    "    $$ \\sigma = \\sqrt{5.0 + 1e^{-5}} \\approx \\sqrt{5.00001} \\approx 2.23607 $$\n",
    "\n",
    "4.  **Normalize Each Input Element ($\\hat{x_i}$)**:\n",
    "    * $\\hat{x_1} = (2.0 - 5.0) / 2.23607 = -3.0 / 2.23607 \\approx -1.3416$\n",
    "    * $\\hat{x_2} = (4.0 - 5.0) / 2.23607 = -1.0 / 2.23607 \\approx -0.4472$\n",
    "    * $\\hat{x_3} = (6.0 - 5.0) / 2.23607 = 1.0 / 2.23607 \\approx 0.4472$\n",
    "    * $\\hat{x_4} = (8.0 - 5.0) / 2.23607 = 3.0 / 2.23607 \\approx 1.3416$\n",
    "    * Normalized vector: $\\hat{X} \\approx [-1.3416, -0.4472, 0.4472, 1.3416]$\n",
    "\n",
    "5.  **Apply Scale ($\\gamma$) and Shift ($\\beta$)**:\n",
    "    Since initial $\\gamma = [1.0, 1.0, 1.0, 1.0]$ and $\\beta = [0.0, 0.0, 0.0, 0.0]$:\n",
    "    * $Y_i = 1.0 \\cdot \\hat{x_i} + 0.0 = \\hat{x_i}$\n",
    "    * Final output $Y \\approx [-1.3416, -0.4472, 0.4472, 1.3416]$\n",
    "\n",
    "This output $Y$ is the result of the Layer Normalization for that specific token, which then proceeds to the next sub-layer or the Feed-Forward Network.\n",
    "\n",
    "### 5. Benefits of Layer Normalization in Transformers\n",
    "\n",
    "* **Improved Training Stability**: Keeps activations within a stable range, preventing vanishing/exploding gradients in deep networks.\n",
    "* **Faster Convergence**: Creates a smoother optimization landscape, allowing the model to converge more quickly.\n",
    "* **Robustness to Batch Size**: Its per-sample normalization makes it insensitive to batch size, ideal for varying input loads.\n",
    "* **Better Generalization**: Helps the model generalize well by reducing internal covariate shift.\n",
    "* **Facilitates Residual Connections**: Works in tandem with residual connections to create a powerful highway for information flow, crucial for effectively training very deep Transformer architectures.\n",
    "\n",
    "This notebook provides both the theoretical justification and the practical computation of Layer Normalization, an indispensable technique for building robust and high-performing Transformer models. In subsequent discussions, we will integrate these components into the complete Transformer Encoder architecture.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
