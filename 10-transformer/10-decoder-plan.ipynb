{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer Decoder: Unraveling its Generative Architecture and Masked Attention\n",
    "\n",
    "## Overview\n",
    "\n",
    "This Jupyter Notebook embarks on the next critical phase of our Transformer journey: dissecting the **Decoder architecture**. Having thoroughly explored the Encoder's role in creating rich contextual representations, we now turn our attention to the Decoder, the component solely responsible for **generating output sequences one token at a time**. This session will provide a strategic plan of action for understanding the Decoder, focusing initially on its unique first sub-layer: **Masked Multi-Head Self-Attention**.\n",
    "\n",
    "The Decoder fundamentally differs from the Encoder in its generative nature. While the Encoder processes its entire input sequence in parallel, the Decoder must operate sequentially to predict the next token, ensuring that its predictions only rely on previously generated tokens and the full encoded input. This constraint is meticulously enforced through the mechanism of \"masking,\" a key innovation that enables the Transformer's powerful sequence generation capabilities.\n",
    "\n",
    "## Deconstructing the Decoder: Core Components and Distinctive Operation\n",
    "\n",
    "The Transformer Decoder is a sophisticated generative model that leverages the contextual understanding provided by the Encoder. A full Transformer, mirroring its Encoder stack, typically features **six identical Decoder layers**.\n",
    "\n",
    "### 1. The Generative Nature: Sequential Output Generation\n",
    "\n",
    "A fundamental distinction between the Encoder and Decoder lies in their processing paradigm:\n",
    "\n",
    "* **Encoder**: Processes the entire input sequence **all at once** (in parallel), creating contextualized embeddings for every token simultaneously.\n",
    "* **Decoder**: Generates the output sequence **one token at a time** (sequentially). At each time step, it predicts the next word based on the Encoder's output and all previously generated words. This sequential generation is crucial for tasks like machine translation or text summarization.\n",
    "\n",
    "### 2. Main Components of a Single Decoder Layer\n",
    "\n",
    "Each Decoder layer is composed of three primary sub-layers, interspersed with **Residual Connections** and **Layer Normalization** (following the \"Add & Normalize\" pattern seen in the Encoder):\n",
    "\n",
    "#### a. Masked Multi-Head Self-Attention\n",
    "\n",
    "* **Role**: This is the first and a highly distinctive self-attention sub-layer within the Decoder. It allows the decoder to attend to the *previously generated parts* of the output sequence.\n",
    "* **The Crucial \"Mask\"**: This attention mechanism is **masked** to ensure that the prediction for a given position `i` can *only* depend on the known outputs at positions `1` through `i-1`. During training, this mask prevents the model from \"cheating\" by looking at future tokens in the target sequence. This is typically implemented by setting the attention scores for future (or padded) positions to negative infinity, effectively zeroing out their contribution after the softmax.\n",
    "* **Detailed Steps (aligned with Encoder's Self-Attention, but with a critical addition)**:\n",
    "    1.  **Input Embedding & Positional Embedding**: The previously generated output tokens (or the start-of-sequence token during training) are converted into `d_model` (512-dimensional) embeddings, which are then combined with positional encodings.\n",
    "    2.  **Linear Projections (Q, K, V)**: These input representations are linearly transformed into Query (Q), Key (K), and Value (V) vectors for each attention head (with `d_k = d_v = 64` dimensions, as in the Encoder).\n",
    "    3.  **Scaled Dot-Product Attention**: The core attention mechanism computes dot products between Queries and Keys, scales them by $\\sqrt{d_k}$, and applies a softmax function.\n",
    "    4.  **Mask Application (The NEW Step)**: *Before* the softmax, a mask is applied to set attention scores for future positions to negative infinity. This is the defining difference from the Encoder's self-attention.\n",
    "    5.  **Multi-Head Mechanism**: The masked attention is performed across multiple heads (e.g., 8 heads) in parallel.\n",
    "    6.  **Concatenation & Final Linear Projection**: The outputs from all heads are concatenated and projected back to `d_model` dimensions.\n",
    "    7.  **Residual Connection & Layer Normalization**: The input to this sub-layer is added to its output, followed by layer normalization for stability and efficient gradient flow.\n",
    "\n",
    "#### b. Multi-Head Attention (Encoder-Decoder Attention)\n",
    "\n",
    "* **Role**: This is the second attention sub-layer in the Decoder. It's often called **Encoder-Decoder Attention** because it allows the Decoder to attend to the *output of the Encoder stack*.\n",
    "* **Mechanism**: The Queries (Q) for this attention mechanism come from the *previous* Masked Self-Attention sub-layer's output in the Decoder. The Keys (K) and Values (V), however, come from the *output of the Encoder*. This effectively allows the Decoder to \"look at\" and \"understand\" the entire source sentence's contextual representation while generating the target sentence.\n",
    "* **Output**: The output of this layer is a `d_model`-dimensional contextualized vector that combines information from both the target sequence (via Q from masked attention) and the source sequence (via K and V from the Encoder).\n",
    "* **Residual Connection & Layer Normalization**: Again, wrapped in \"Add & Normalize\" for robustness.\n",
    "\n",
    "#### c. Position-wise Feed-Forward Network (FFN)\n",
    "\n",
    "* **Role**: Identical in structure and function to the FFN in the Encoder. It applies a two-layer, point-wise linear transformation (with ReLU activation) to each position independently.\n",
    "* **Purpose**: Adds non-linearity and allows the model to learn richer, more complex transformations on the contextual information derived from both attention layers.\n",
    "* **Residual Connection & Layer Normalization**: The final sub-layer also benefits from the \"Add & Normalize\" mechanism.\n",
    "\n",
    "### 3. Decoder Operation: Training vs. Inference\n",
    "\n",
    "The Decoder behaves differently during training and inference to manage its generative nature:\n",
    "\n",
    "* **During Training**:\n",
    "    * The Decoder is provided with the *entire target sequence* (shifted right, meaning the \"start-of-sentence\" token is the first input).\n",
    "    * **Crucially, the Masked Multi-Head Self-Attention ensures that at any decoding step, the model only sees the tokens *up to that point* in the target sequence.** This simulates the sequential generation process even though the computation can be parallelized during training.\n",
    "    * The model learns to predict the next token based on the previous ground truth tokens.\n",
    "* **During Inference (Generation)**:\n",
    "    * The Decoder operates truly sequentially, generating one token at a time.\n",
    "    * It starts with a \"start-of-sequence\" token.\n",
    "    * At each step, the newly predicted token is added to the input sequence for the *next* step.\n",
    "    * The Masked Self-Attention naturally only sees the tokens that have *already been generated*.\n",
    "    * The process continues until an \"end-of-sequence\" token is predicted or a maximum length is reached.\n",
    "\n",
    "This notebook will focus on a detailed walkthrough of the **Masked Multi-Head Self-Attention** sub-layer, emphasizing the critical role of masking in enabling the Transformer Decoder's powerful generative capabilities during training and inference.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
