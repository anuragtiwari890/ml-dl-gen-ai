{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer Decoder: Unraveling its Generative Architecture and Masked Attention (Elaborated)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to a comprehensive, in-depth exploration of the **Transformer Decoder**! This Jupyter Notebook combines and expands upon our previous discussions, providing an exceptionally detailed understanding of how the Decoder functions, particularly highlighting its core components and the indispensable role of **masking** in enabling its powerful generative capabilities.\n",
    "\n",
    "The Transformer architecture revolutionized sequence modeling by introducing attention mechanisms. While the Encoder part efficiently processes and transforms an input sequence into a rich, dense contextual representation, the Decoder takes on the equally crucial task of **generating output sequences, token by token**. This fundamental distinction—the Decoder's **autoregressive**, sequential nature—necessitates unique design elements, most notably the **Masked Multi-Head Self-Attention**. This mechanism is ingeniously crafted to ensure that the model's predictions at any given step are based *exclusively* on previously generated tokens and the full context extracted from the Encoder, strictly preventing it from \"peeking\" into future information.\n",
    "\n",
    "Throughout this notebook, we will meticulously walk through the architectural flow of a Decoder block, delve into the step-by-step computations, and dissect the critical **padding** and **look-ahead masking** techniques. Understanding these nuances is paramount, as they define the Decoder's behavior during both the training phase (where it learns from entire target sequences) and the inference phase (where it generates new sequences one word at a time).\n",
    "\n",
    "## Deconstructing the Decoder: Core Components and Distinctive Operation\n",
    "\n",
    "The Transformer Decoder is a sophisticated generative model that takes the contextual understanding derived from the Encoder and translates it into a new sequence. Mirroring its Encoder counterpart, a full Transformer model typically features a stack of **six identical Decoder layers**, connected sequentially, where the output of one layer feeds into the next.\n",
    "\n",
    "### 1. The Generative Nature: Sequential Output Generation\n",
    "\n",
    "A fundamental and perhaps the most significant distinction between the Encoder and Decoder lies in their processing paradigm:\n",
    "\n",
    "* **Encoder**: Operates on the entire input sequence **all at once** (in parallel). It receives all tokens (e.g., \"The cat sat on the mat\") simultaneously and computes contextualized embeddings for each, understanding the full context of the sentence in one go. This allows for efficient training and deep understanding of the input.\n",
    "* **Decoder**: In contrast, the Decoder generates the output sequence **one token at a time** (sequentially). Imagine typing out a sentence: you type the first word, then the second, then the third, and so on. The Decoder works similarly. At each time step, it predicts the next word based on the Encoder's comprehensive understanding of the source language and all words it has *already* generated in the target language. This sequential generation is absolutely vital for tasks like machine translation, text summarization, or chatbots, where the output is built piece by piece.\n",
    "\n",
    "### 2. Main Components of a Single Decoder Layer\n",
    "\n",
    "Each Decoder layer is a robust unit composed of three primary sub-layers, consistently benefiting from the \"Add & Normalize\" pattern:\n",
    "\n",
    "* **Residual Connections**: These allow gradients to flow directly through the network, mitigating the vanishing gradient problem and enabling the effective training of very deep architectures.\n",
    "* **Layer Normalization**: Applied after the residual connection, it normalizes the activations across the feature dimension for *each individual token*, stabilizing the training process and allowing for higher learning rates.\n",
    "\n",
    "Now, let's explore the three distinct sub-layers within a Decoder block:\n",
    "\n",
    "#### a. Masked Multi-Head Self-Attention\n",
    "\n",
    "This is the first and most unique self-attention sub-layer within the Decoder. Its primary function is to allow the decoder to attend to the *previously generated parts* of the output sequence while strictly preventing it from accessing future tokens.\n",
    "\n",
    "**i. Input Preparation during Training:**\n",
    "\n",
    "During training, to teach the Decoder how to generate sequentially, it's provided with the **ground truth target output sequence**, but with a crucial modification:\n",
    "\n",
    "* **\"Output Shifted Right\"**: This is an ingenious trick! Instead of giving the Decoder the full target sentence (e.g., \"मैं घर हूँ\"), we provide it with a version where a special \"Start of Sentence\" (`<SOS>`) token is prepended, and the last actual token is omitted: `[<SOS>, मैं, घर]`. This means that when the Decoder is trying to predict \"मैं\", it only sees `<SOS>`. When it tries to predict \"घर\", it sees `<SOS>, मैं`. This process forces the model to learn to predict the *next* token based *only* on the *previous* tokens, perfectly mimicking the inference scenario.\n",
    "* **Padding**: In practical batch processing, sequences often have varying lengths. To make all sequences in a batch uniform in length, **padding tokens (e.g., zeros, or a special `<PAD>` token)** are added to the end of shorter sequences. For example, if the target sentence is \"मैं घर हूँ\" (3 tokens) and our maximum sequence length is 4, the input to the decoder might be `[<SOS>, मैं, घर, <PAD>]`.\n",
    "* **Input Embedding & Positional Encoding**: As with the Encoder, these shifted-right and padded tokens are first converted into dense numerical **embeddings** (with `d_model` dimensions, typically **512** in the original paper). These embeddings are then summed with **positional encodings** to imbue the model with information about the order and relative positions of tokens in the sequence, which attention alone cannot inherently capture.\n",
    "\n",
    "    * *Conceptual Example for input `[<SOS>, Token1, Token2, <PAD>]`:*\n",
    "        ```\n",
    "        Input_Embeddings = \n",
    "        [[embed_SOS_dim1, ..., embed_SOS_dim512],  # Embedding for <SOS>\n",
    "         [embed_T1_dim1,  ..., embed_T1_dim512],   # Embedding for Token1\n",
    "         [embed_T2_dim1,  ..., embed_T2_dim512],   # Embedding for Token2\n",
    "         [embed_PAD_dim1, ..., embed_PAD_dim512]]  # Embedding for <PAD> (often zero vector)\n",
    "        ```\n",
    "\n",
    "**ii. Computational Steps within Masked Self-Attention (Detailed):**\n",
    "\n",
    "1.  **Linear Projections (Q, K, V)**: The combined input embeddings (embeddings + positional encodings) are first linearly transformed into **Query (Q), Key (K), and Value (V)** matrices. This is done by multiplying the input matrix by three distinct, *learned* weight matrices: $W^Q$, $W^K$, and $W^V$. These projections allow the model to learn different \"perspectives\" or \"representations\" of the input for the attention mechanism.\n",
    "    * For each attention head, the dimensionality of Q, K, and V vectors (`d_k` and `d_v`) is typically `d_model / number_of_heads`. For `d_model = 512` and 8 heads, `d_k = d_v = 64`.\n",
    "    * *Illustrative Example (simplified, assuming identity matrices for WQ, WK, WV for initial intuition):*\n",
    "        ```\n",
    "        Q = K = V = \n",
    "        [[0.1, 0.2, 0.3, 0.4],  # Row corresponding to <SOS>\n",
    "         [0.5, 0.6, 0.7, 0.8],  # Row corresponding to Token1\n",
    "         [0.9, 1.0, 1.1, 1.2],  # Row corresponding to Token2\n",
    "         [0.0, 0.0, 0.0, 0.0]]  # Row corresponding to <PAD>\n",
    "        ```\n",
    "2.  **Scaled Dot-Product Attention Calculation**: The raw attention scores are computed by taking the dot product of the **Query matrix (Q)** and the **transpose of the Key matrix (K^T)**. This dot product essentially calculates how \"related\" each query token is to every key token. To prevent the dot products from growing too large (which can push the softmax function into regions with tiny gradients, hindering learning), the scores are then divided by the square root of the key dimension ($\\sqrt{d_k}$).\n",
    "    * *Formula*: $Scores = Q \\cdot K^T / \\sqrt{d_k}$\n",
    "    * For `d_k = 4` (in our simple 4D example), $\\sqrt{d_k} = \\sqrt{4} = 2$.\n",
    "    * *Resulting Scores Matrix (Illustrative Example):*\n",
    "        ```\n",
    "        Scores = \n",
    "        [[0.3, 0.7, 1.1, 0.0],  # Query for <SOS> attending to all keys\n",
    "         [0.7, 1.9, 3.1, 0.0],  # Query for Token1 attending to all keys\n",
    "         [1.1, 3.1, 5.1, 0.0],  # Query for Token2 attending to all keys\n",
    "         [0.0, 0.0, 0.0, 0.0]]  # Query for <PAD> attending to all keys\n",
    "        ```\n",
    "        *(Note: Actual dot products would yield specific numerical values based on the embeddings. The example values are for conceptual understanding.)*\n",
    "3.  **Mask Application**: This is the pivotal step that differentiates Decoder's self-attention. Before the softmax function is applied to the `Scores` matrix, two types of masks are combined and applied to strategically modify these scores:\n",
    "    * **Padding Mask**:\n",
    "        * **Purpose**: To ensure that **padding tokens** (e.g., `<PAD>` or zeros) do not influence the attention mechanism. If the model were to learn to use padding tokens, it would lead to **incorrect or biased predictions**, as these tokens carry no meaningful information relevant to the actual sequence content. This is crucial for training robustness.\n",
    "        * **Mechanism**: A 1D boolean mask is first created, identifying real tokens (e.g., `True` or `1`) and padding tokens (e.g., `False` or `0`). This 1D mask is then extended to a 2D matrix, where each row is a copy of the 1D mask. This 2D mask indicates which *key* positions are valid for attention for *any* query. For instance, if the last token is padding, the last column in this 2D mask would be all `0`s. Additionally, if the query itself is a padding token, its entire row in the mask would be `0`s.\n",
    "            * *Example 2D Padding Mask (for input `[<SOS>, Token1, Token2, <PAD>]`):*\n",
    "                ```\n",
    "                Padding_Mask = \n",
    "                [[1, 1, 1, 0],  # For Query <SOS> (can attend to <SOS>, T1, T2; ignores <PAD>)\n",
    "                 [1, 1, 1, 0],  # For Query Token1 (can attend to <SOS>, T1, T2; ignores <PAD>)\n",
    "                 [1, 1, 1, 0],  # For Query Token2 (can attend to <SOS>, T1, T2; ignores <PAD>)\n",
    "                 [0, 0, 0, 0]]  # For Query <PAD> (itself a padding token, attends to nothing)\n",
    "                ```\n",
    "    * **Look-Ahead Mask (Autoregressive Mask)**:\n",
    "        * **Purpose**: To enforce the **autoregressive property** during training. This is fundamental for generative models. It ensures that when the model is predicting a token at position `t`, it can *only* attend to tokens at positions `1` to `t` (including itself) and *not* to any future tokens (`t+1`, `t+2`, etc.) in the target sequence. This prevents the model from \"cheating\" by simply looking at the actual ground truth future tokens, forcing it to learn to truly predict sequentially. Imagine predicting \"apple\" in \"I like green apple\". The model should only use \"I like green\", not \"apple\" itself or any words after it.\n",
    "        * **Mechanism**: A lower-triangular matrix mask is constructed. All elements in the upper triangle (representing future positions relative to the current query) are set to `0` (or `False`). This ensures that queries cannot \"look ahead.\"\n",
    "            * *Example Look-Ahead Mask (for a 4-token sequence):*\n",
    "                ```\n",
    "                Look_Ahead_Mask = \n",
    "                [[1, 0, 0, 0],  # Query <SOS> can only attend to <SOS>\n",
    "                 [1, 1, 0, 0],  # Query Token1 can attend to <SOS>, Token1\n",
    "                 [1, 1, 1, 0],  # Query Token2 can attend to <SOS>, Token1, Token2\n",
    "                 [1, 1, 1, 1]]  # Query <PAD> can attend to all (but padding mask will take precedence)\n",
    "                ```\n",
    "    * **Combined Mask**: These two masks (`Padding_Mask` and `Look_Ahead_Mask`) are combined, typically via **element-wise multiplication** (or logical AND for boolean masks). This ensures that a position is masked if *either* the padding mask *or* the look-ahead mask dictates it. The \"masking\" effect is cumulative.\n",
    "        * *Example Combined Mask (`Padding_Mask * Look_Ahead_Mask`):*\n",
    "            ```\n",
    "            Combined_Mask = \n",
    "            [[1, 0, 0, 0],  # <SOS>: Real token, no future tokens, no padding\n",
    "             [1, 1, 0, 0],  # Token1: Real token, attends to <SOS>, Token1, no future/padding\n",
    "             [1, 1, 1, 0],  # Token2: Real token, attends to <SOS>, T1, T2, no future/padding\n",
    "             [0, 0, 0, 0]]  # <PAD>: Padding token, completely masked out\n",
    "            ```\n",
    "    * **Applying the Combined Mask to Scores**: Wherever the `Combined_Mask` has a `0` (or `False`), the corresponding attention score in the `Scores` matrix is overwritten and set to a very large **negative infinity (`-inf`)**. This is a crucial step for controlling attention flow.\n",
    "        * *Example Resulting Masked Scores:*\n",
    "            ```\n",
    "            Masked_Scores = \n",
    "            [[score_SOS_SOS,  -inf,           -inf,           -inf],\n",
    "             [score_T1_SOS,   score_T1_T1,    -inf,           -inf],\n",
    "             [score_T2_SOS,   score_T2_T1,    score_T2_T2,    -inf],\n",
    "             [-inf,           -inf,           -inf,           -inf]]\n",
    "            ```\n",
    "        * **Why -inf?**: This trick works because when the `softmax` function is applied to a value of `-inf`, the output probability becomes effectively **zero**. This brilliantly ensures that **masked positions do not contribute any weight** to the attention mechanism. It completely removes their influence, making the attention weights (and thus the resulting contextual vector) solely dependent on the allowed (unmasked) positions.\n",
    "4.  **Softmax on Masked Scores**: The `softmax` function is then applied row-wise to these `Masked_Scores`. This converts the raw scores into probabilities (attention weights) that sum to 1 for each query, indicating how much attention each query token pays to other valid key tokens.\n",
    "    * *Crucial Outcome*: Due to the `-inf` values, the probabilities corresponding to masked positions will become **exactly zero**.\n",
    "    * *Example Softmax Scores:*\n",
    "        ```\n",
    "        Softmax_Scores = Softmax(Masked_Scores) =\n",
    "        [[1.0, 0.0, 0.0, 0.0],  # <SOS> attends only to itself\n",
    "         [0.3, 0.7, 0.0, 0.0],  # Token1 attends to <SOS> and Token1 (illustrative relative weights)\n",
    "         [0.0, 0.0, 1.0, 0.0],  # Token2 attends to <SOS>, T1, T2 (illustrative values)\n",
    "         [0.0, 0.0, 0.0, 0.0]]  # <PAD> token attends to nothing at all\n",
    "        ```\n",
    "5.  **Weighted Sum of Values**: The final attention output for this sub-layer is computed by taking the `Softmax_Scores` (the attention weights) and multiplying them element-wise by the `Value (V)` matrix. The results are then summed row-wise. This produces the highly contextualized representation for each output token. Critically, because of the masking, each output token's representation is a weighted sum of *only* the Value vectors from allowed previous (and self) positions.\n",
    "\n",
    "#### b. Multi-Head Attention (Encoder-Decoder Attention)\n",
    "\n",
    "* **Role**: This is the second attention sub-layer within the Decoder, often referred to as **Encoder-Decoder Attention** or **Cross-Attention**. Its primary purpose is to create a **bridge** between the Encoder's understanding of the source sentence and the Decoder's current state of generating the target sentence. It allows the Decoder to selectively attend to the **output of the Encoder stack**.\n",
    "* **Mechanism**: This sub-layer has a unique flow of Query, Key, and Value matrices:\n",
    "    * The **Queries (Q)** for this attention mechanism come from the *output of the preceding Masked Self-Attention sub-layer in the Decoder*. These queries represent the Decoder's current understanding of the part of the target sequence generated so far.\n",
    "    * However, the **Keys (K) and Values (V)** for this layer come from the *output of the Encoder*. This means the Decoder's queries are \"asking questions\" about the Encoder's comprehensive representation of the source sentence.\n",
    "    * This setup allows the Decoder to \"look at\" and \"understand\" relevant information from the entire source sentence's contextual representation to guide the generation of the next token in the target sentence. For example, if translating \"The cat sat\" to \"बिल्ली बैठी थी\", while generating \"बैठी\", the Decoder might query the Encoder's understanding of \"sat\" and \"cat\".\n",
    "* **Output**: Produces a `d_model`-dimensional contextualized vector for each token in the target sequence, enriched with information from both the target (via Q) and source (via K, V) sequences.\n",
    "* **Residual Connection & Layer Normalization**: As with all sub-layers, this layer is wrapped in the \"Add & Normalize\" pattern for robustness and efficient training.\n",
    "\n",
    "#### c. Position-wise Feed-Forward Network (FFN)\n",
    "\n",
    "* **Role**: This is the third and final sub-layer in a Decoder block. It is structurally and functionally **identical to the FFN found in the Encoder**. It applies a two-layer, point-wise linear transformation (with a non-linear activation function, typically ReLU, in between) to *each position independently*. This means the same FFN is applied to every token's representation, but the operations for each token are independent of other tokens at this stage.\n",
    "* **Purpose**:\n",
    "    * **Adds Non-Linearity**: Introduces crucial non-linearity into the model, enabling it to learn and approximate complex, non-linear relationships within the data that cannot be captured by linear transformations alone.\n",
    "    * **Refines Representations**: Allows the model to further transform and refine the contextual information derived from both attention layers (masked self-attention and encoder-decoder attention). This helps in extracting higher-level features for each token.\n",
    "    * **Adds Depth and Capacity**: Contributes to the overall depth and parameter count of the model, increasing its capacity to learn more intricate patterns and representations from the data. For instance, the intermediate layer of the FFN often expands the dimensionality (e.g., from 512 to 2048) before projecting back down (to 512).\n",
    "* **Residual Connection & Layer Normalization**: This sub-layer also benefits from the \"Add & Normalize\" mechanism for stability and effective gradient flow.\n",
    "\n",
    "### 4. Decoder Operation: Training vs. Inference\n",
    "\n",
    "The Decoder's mode of operation subtly but significantly shifts between the training and inference phases:\n",
    "\n",
    "* **During Training**:\n",
    "    * The Decoder is provided with the **entire ground truth target sequence** (after being \"shifted right\" and padded).\n",
    "    * **Crucially, the Masked Multi-Head Self-Attention ensures that for any given prediction step, the model *only* \"sees\" the tokens *up to that point* in the target sequence.** This powerful masking mechanism simulates the sequential generation process, even though the overall computation for a batch of sequences can be parallelized for efficiency during training. This forces the model to learn to predict tokens based only on valid, prior context.\n",
    "    * The model learns to predict the next token by comparing its prediction with the actual ground truth token at the next position.\n",
    "* **During Inference (Generation)**:\n",
    "    * The Decoder operates **truly sequentially**, generating one token at a time in an iterative loop.\n",
    "    * It begins with a special \"start-of-sequence\" token (`<SOS>`) as its initial input.\n",
    "    * At each step:\n",
    "        1.  The Decoder produces a probability distribution over the vocabulary for the next token.\n",
    "        2.  A token is selected (e.g., via argmax for greedy decoding, or sampling for more diverse output).\n",
    "        3.  The newly predicted token is then **appended** to the input sequence for the *next* step of decoding.\n",
    "    * During this process, the Masked Self-Attention naturally and automatically only sees the tokens that have *already been generated*, perfectly respecting the autoregressive property without needing any explicit \"shifting.\"\n",
    "    * The generation continues until an \"end-of-sequence\" token (`<EOS>`) is predicted or a pre-defined maximum sequence length is reached.\n",
    "\n",
    "This comprehensive understanding of masking within the Masked Multi-Head Self-Attention, alongside its interplay with Encoder-Decoder Attention and Feed-Forward Networks, is fundamental to appreciating the powerful and adaptable generative capabilities of the Transformer Decoder. We have meticulously detailed how precise computational steps and architectural choices ensure the model learns to predict sequences correctly and efficiently.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
