{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding LSTM Architecture: A Deep Dive into Memory Cells and Gates\n",
    "\n",
    "### This Jupyter Notebook delves into the detailed architecture of Long Short-Term Memory (LSTM) networks, building upon the introductory concepts from the previous lecture. The primary goal is to break down the components of an LSTM cell and understand the fundamental operations that enable its long-term memory capabilities.\n",
    "\n",
    "## high level understanding - \n",
    "\n",
    "![Alt text for the image](images/lstm-architecture.png)\n",
    "\n",
    "## Key Learning Objectives:\n",
    "\n",
    "### 1. **Overall LSTM Architecture Overview**\n",
    "* **Inputs**:\n",
    "    * $X_t$: Current input word/vector at time step $t$.\n",
    "    * $H_{t-1}$: Hidden state (short-term memory) from the previous time step $t-1$.\n",
    "    * $C_{t-1}$: Cell state (long-term memory) from the previous time step $t-1$.\n",
    "* **Outputs**:\n",
    "    * $H_t$: Current hidden state at time step $t$.\n",
    "    * $C_t$: Current cell state at time step $t$.\n",
    "* **Three Main Gates**: A single LSTM unit is composed of three crucial \"gates\" that regulate the flow of information:\n",
    "    1.  **Forget Gate**: Decides what information from the previous cell state ($C_{t-1}$) should be discarded.\n",
    "    2.  **Input Gate & Candidate Memory**: Decides what new information from the current input ($X_t$) and previous hidden state ($H_{t-1}$) should be stored in the cell state.\n",
    "    3.  **Output Gate**: Decides what part of the cell state ($C_t$) should be output as the current hidden state ($H_t$).\n",
    "\n",
    "### 2. **Understanding LSTM Notation and Operations**\n",
    "\n",
    "The lecture explains the visual symbols commonly used in LSTM diagrams:\n",
    "\n",
    "* **Neural Network Layer (Yellow Box with Sigmoid/Tanh)**:\n",
    "    * Represents a layer of neurons (typically a `Dense` layer in Keras) followed by an activation function (sigmoid or tanh).\n",
    "    * It takes combined inputs (e.g., concatenated $X_t$ and $H_{t-1}$), performs a linear transformation, and then applies the activation.\n",
    "    * **Concatenation (Stacked Boxes)**: This symbol means combining two vectors or tensors by placing them end-to-end (e.g., $X_t$ and $H_{t-1}$ are concatenated before being fed into a gate's neural network layer).\n",
    "* **Point-wise Operations (Pink Symbols)**: These operations apply element-wise to vectors:\n",
    "    * $\\times$ (Circle with an 'x'): **Point-wise Multiplication**. Multiplies corresponding elements of two vectors.\n",
    "        * Example: $[1, 2, 3] \\times [4, 5, 6] = [1 \\times 4, 2 \\times 5, 3 \\times 6] = [4, 10, 18]$.\n",
    "    * $+$ (Circle with a '+'): **Point-wise Addition**. Adds corresponding elements of two vectors.\n",
    "        * Example: $[1, 2, 3] + [4, 5, 6] = [1+4, 2+5, 3+6] = [5, 7, 9]$.\n",
    "    * $\\tanh$ (Circle with 'tanh'): **Point-wise Tanh Activation**. Applies the tanh function to each element of a vector.\n",
    "        * Example: $\\tanh([1, 2, 3]) = [\\tanh(1), \\tanh(2), \\tanh(3)]$.\n",
    "* **Vector Transfer (Solid Lines/Arrows)**: Simply indicates the flow of a vector or information from one point to another.\n",
    "* **Copy (Branching Lines)**: Shows that a vector's value is duplicated and sent to multiple destinations.\n",
    "\n",
    "### 3. **The Memory Cell (Cell State - $C_t$)**\n",
    "* This is the horizontal line at the very top of the LSTM diagram.\n",
    "* It serves as the **long-term memory** of the network, designed to store and propagate relevant information across many time steps.\n",
    "* $C_{t-1}$: Memory cell state from the previous time step.\n",
    "* $C_t$: Updated memory cell state for the current time step, after information has been selectively forgotten and added.\n",
    "\n",
    "### Next Steps:\n",
    "The lecture concludes by setting the stage for the next video, which will delve into the **Forget Gate** in detail, explaining its purpose and the mathematical operations involved."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
