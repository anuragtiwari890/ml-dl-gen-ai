{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissecting the LSTM Output Gate: How LSTMs Produce Their Final Hidden State\n",
    "\n",
    "### This Jupyter Notebook concludes the detailed breakdown of the Long Short-Term Memory (LSTM) architecture by focusing on the **Output Gate**. This gate is responsible for deciding what part of the *updated cell state* (long-term memory) should be exposed as the *current hidden state* (short-term memory) for the next time step and for making predictions.\n",
    "\n",
    "## high level understanding - \n",
    "\n",
    "![Alt text for the image](images/output_gate.png)\n",
    "\n",
    "## Key Learning Objectives:\n",
    "\n",
    "### 1. **Recap: Previous Gates' Functions**\n",
    "* **Forget Gate**: Decides what information to **forget** from the previous cell state ($C_{t-1}$) based on context.\n",
    "* **Input Gate & Candidate Memory**: Decide what **new information** to **add** to the cell state based on context.\n",
    "* The combination of these two results in the updated cell state, $C_t = (f_t \\times C_{t-1}) + (i_t \\times \\tilde{C}_t)$. This $C_t$ represents the refined long-term memory.\n",
    "\n",
    "### 2. **Focus: The Output Gate's Role**\n",
    "\n",
    "* The **Output Gate** determines what portion of the filtered long-term memory ($C_t$) will be used to generate the current hidden state ($H_t$). The hidden state is what's passed to the next LSTM unit and also typically used for predictions (e.g., the next word in a sequence).\n",
    "* **Why not just output $C_t$ directly?**: The cell state can contain a vast amount of information, much of which might not be relevant for the *immediate* output or the short-term context. The Output Gate acts as a filter to extract only the pertinent information for the current hidden state.\n",
    "\n",
    "### 3. **Step-by-Step Breakdown of the Output Gate Operation**\n",
    "\n",
    "#### **3.1. Output Gate Filter (Sigmoid Layer)**\n",
    "* **Inputs**: It takes the concatenated $H_{t-1}$ (previous hidden state) and $X_t$ (current input).\n",
    "* **Neural Network Layer + Sigmoid Activation**: This combined input is passed through a neural network layer (with its own weights $W_o$ and bias $b_o$) and then through a sigmoid activation function.\n",
    "    * **Mathematical Representation**: $o_t = \\sigma(W_o \\cdot [H_{t-1}, X_t] + b_o)$\n",
    "    * The output, $o_t$, is a vector of values between 0 and 1. This vector acts as a filter, deciding which parts of the (transformed) cell state are relevant for the current hidden state.\n",
    "\n",
    "#### **3.2. Cell State Transformation (Tanh Activation)**\n",
    "* The **updated cell state** ($C_t$) is passed through a **tanh activation function**.\n",
    "    * **Mathematical Representation**: $\\tanh(C_t)$\n",
    "    * **Purpose**: This scales the values of the cell state between -1 and 1, preparing it for the final filtering.\n",
    "\n",
    "#### **3.3. Point-wise Multiplication to Get Hidden State**\n",
    "* The output of the sigmoid from the Output Gate ($o_t$) is then **point-wise multiplied** with the transformed cell state ($\\tanh(C_t)$).\n",
    "* **Mathematical Representation**: $H_t = o_t \\times \\tanh(C_t)$\n",
    "* **Purpose**: This multiplication determines the final value of the current hidden state ($H_t$).\n",
    "    * If an element in $o_t$ is 0, the corresponding element in $\\tanh(C_t)$ is zeroed out, effectively ignoring that part of the cell state for the hidden state.\n",
    "    * If an element in $o_t$ is 1, the corresponding element in $\\tanh(C_t)$ is fully passed to the hidden state.\n",
    "    * Values between 0 and 1 allow for partial inclusion.\n",
    "\n",
    "### 4. **Outputs of the LSTM Unit for the Next Time Step**\n",
    "* **$H_t$ (Current Hidden State)**: This is the **short-term memory** of the LSTM. It is passed to the next LSTM unit in the sequence ($H_{t+1}$) and is also typically used as the output for prediction at the current time step.\n",
    "* **$C_t$ (Current Cell State)**: This is the **long-term memory**. It is passed directly to the next LSTM unit ($C_{t+1}$) to retain relevant information over extended periods.\n",
    "\n",
    "### 5. **Summary of LSTM Weights and Backpropagation**\n",
    "* The lecture highlights that all the weight matrices ($W_f, W_i, W_c, W_o$) and bias vectors ($b_f, b_i, b_c, b_o$) within each gate are learned during the training process (forward and backward propagation) to effectively manage the flow of information through the LSTM.\n",
    "\n",
    "### Conclusion:\n",
    "The Output Gate provides the final level of control in an LSTM, allowing the model to selectively extract relevant information from its refined long-term memory ($C_t$) to form the current hidden state ($H_t$). This detailed control over information flow (forgetting, adding, and outputting) is what empowers LSTMs to effectively learn and leverage long-term dependencies in sequential data, addressing the limitations of simple RNNs.\n",
    "\n",
    "**Next Video**: The next lecture will introduce **Gated Recurrent Units (GRUs)**, another variant of RNNs that offer a simplified architecture compared to LSTMs while still addressing the vanishing gradient problem."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
