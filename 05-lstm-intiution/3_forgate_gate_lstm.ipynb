{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissecting the LSTM Forget Gate: How LSTMs Decide What to Remember\n",
    "\n",
    "### This Jupyter Notebook focuses on the first critical component of an LSTM cell: the Forget Gate. We will break down its internal workings, the mathematical operations involved, and its role in managing the long-term memory of the network.\n",
    "\n",
    "## high level understanding - \n",
    "\n",
    "![Alt text for the image](images/forget_gate.png)\n",
    "\n",
    "## Key Learning Objectives:\n",
    "\n",
    "### 1. **Recall: Overall LSTM Architecture**\n",
    "* Briefly review the overall LSTM architecture with its three main gates: Forget Gate, Input Gate & Candidate Memory, and Output Gate.\n",
    "* Reiterate the roles of $X_t$ (current input), $H_{t-1}$ (previous hidden state/short-term memory), and $C_{t-1}$ (previous cell state/long-term memory).\n",
    "\n",
    "### 2. **Focus: The Forget Gate's Role**\n",
    "* The **Forget Gate** is responsible for deciding what information from the *previous cell state* ($C_{t-1}$) is no longer relevant and should be \"forgotten\" (i.e., discarded or reduced in importance).\n",
    "\n",
    "### 3. **Step-by-Step Breakdown of the Forget Gate Operation**\n",
    "\n",
    "#### **3.1. Input Preparation (Concatenation)**\n",
    "* **Inputs**: The forget gate takes two main inputs:\n",
    "    * The current input vector, $X_t$ (e.g., a word embedding, 4-dimensional in the example).\n",
    "    * The previous hidden state vector, $H_{t-1}$ (e.g., 3-dimensional in the example, the same dimension as $C_{t-1}$ and the output of the gate).\n",
    "* **Concatenation**: $H_{t-1}$ and $X_t$ are **concatenated** (combined end-to-end) to form a single input vector.\n",
    "    * Example: If $H_{t-1}$ is 3-dim and $X_t$ is 4-dim, the concatenated vector will be 7-dim.\n",
    "\n",
    "#### **3.2. Neural Network Layer (Linear Transformation + Sigmoid Activation)**\n",
    "* The concatenated input vector ($[H_{t-1}, X_t]$) is fed into a neural network layer.\n",
    "* This layer involves:\n",
    "    * **Weight Matrix Multiplication**: The input vector is multiplied by a weight matrix ($W_f$) associated with the forget gate.\n",
    "        * Example: If input is 1x7 and output of this layer is desired to be 1x3 (matching $C_{t-1}$'s dimension), then $W_f$ will be 7x3.\n",
    "    * **Bias Addition**: A bias vector ($b_f$) is added to the result.\n",
    "    * **Sigmoid Activation Function ($\\sigma$)**: The result is passed through a sigmoid activation function.\n",
    "        * The sigmoid function squashes the values between 0 and 1. This output, denoted as $f_t$, is the **forget gate vector**.\n",
    "        * **Mathematical Representation**: $f_t = \\sigma(W_f \\cdot [H_{t-1}, X_t] + b_f)$\n",
    "        * **Significance**: Each element in $f_t$ represents a \"forget factor\" for the corresponding element in the previous cell state $C_{t-1}$. A value close to 0 means \"forget this part completely,\" while a value close to 1 means \"keep this part completely.\"\n",
    "\n",
    "#### **3.3. Point-wise Multiplication with Previous Cell State**\n",
    "* The output of the sigmoid function, $f_t$, is then **point-wise multiplied** with the previous cell state, $C_{t-1}$.\n",
    "* **Operation**: $f_t \\times C_{t-1}$\n",
    "* **Purpose**: This multiplication selectively scales down or eliminates information from $C_{t-1}$:\n",
    "    * If an element in $f_t$ is 0, the corresponding element in $C_{t-1}$ becomes 0, effectively \"forgetting\" that piece of information.\n",
    "    * If an element in $f_t$ is 1, the corresponding element in $C_{t-1}$ remains unchanged, effectively \"remembering\" that piece of information.\n",
    "    * If an element in $f_t$ is between 0 and 1 (e.g., 0.5), the corresponding element in $C_{t-1}$ is partially scaled, reducing its influence.\n",
    "\n",
    "### 4. **Illustrative Examples of Forget Gate Behavior:**\n",
    "* **Scenario 1: Complete Forgetting**\n",
    "    * If $f_t = [0, 0, 0]$ (all zeros), and $C_{t-1} = [6, 8, 9]$, then $f_t \\times C_{t-1} = [0, 0, 0]$. All previous context is removed. This happens when the context of the sentence completely changes.\n",
    "* **Scenario 2: Complete Remembering**\n",
    "    * If $f_t = [1, 1, 1]$ (all ones), and $C_{t-1} = [6, 8, 9]$, then $f_t \\times C_{t-1} = [6, 8, 9]$. No information is removed; all previous context is retained.\n",
    "* **Scenario 3: Partial Forgetting**\n",
    "    * If $f_t = [0.5, 1, 0.5]$, and $C_{t-1} = [6, 8, 9]$, then $f_t \\times C_{t-1} = [3, 8, 4.5]$. Some parts of the context are reduced in importance, while others are fully retained. This allows for fine-grained control over what information from the past cell state is passed forward.\n",
    "\n",
    "### Conclusion:\n",
    "The Forget Gate, through its sigmoid activation and point-wise multiplication, provides the LSTM with a crucial mechanism to selectively forget or retain information from its long-term memory ($C_{t-1}$) based on the current input ($X_t$) and the previous short-term memory ($H_{t-1}$). This is a key reason why LSTMs can handle long-term dependencies effectively, as they can \"decide\" when old information is no longer relevant.\n",
    "\n",
    "**Next Video**: The next lecture will discuss the **Input Gate** and **Candidate Memory**, which are responsible for deciding what *new* information gets added to the cell state."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
