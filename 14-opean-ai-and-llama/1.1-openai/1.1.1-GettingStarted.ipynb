{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with LangChain and OpenAI: A Quickstart Guide\n",
    "\n",
    "This notebook provides a quick introduction to building applications with LangChain, focusing on its core components and integrating with OpenAI models. We'll also touch upon LangSmith for application tracing and observability, and LangServe for deploying your applications.\n",
    "\n",
    "## What You Will Learn:\n",
    "\n",
    "1.  **LangChain Setup**: How to configure your environment with LangChain, OpenAI, and LangSmith.\n",
    "2.  **Core LangChain Components**: Understanding and using:\n",
    "    * **Prompt Templates**: Structured ways to define inputs for LLMs.\n",
    "    * **Models**: Interacting with Large Language Models (LLMs) like OpenAI's `gpt-4o`.\n",
    "    * **Output Parsers**: Extracting structured information from LLM responses.\n",
    "3.  **Building Simple Chains**: Combining components to create basic LLM applications.\n",
    "4.  **LangSmith Tracing**: How to observe and debug your LangChain application's execution.\n",
    "5.  **LangServe (Conceptual)**: An introduction to serving your LangChain applications.\n",
    "\n",
    "## Key Concepts:\n",
    "\n",
    "* **LangChain**: A framework for developing applications powered by language models. It provides tools to chain together different components, making it easier to build complex LLM workflows.\n",
    "* **Prompt Templates**: Predefined recipes for generating prompts to LLMs. They allow for dynamic insertion of input variables.\n",
    "* **Models**: The core LLMs (e.g., OpenAI's GPT models, Google's Gemini, Anthropic's Claude) that process text and generate responses.\n",
    "* **Output Parsers**: Tools to structure the free-form text output from an LLM into a more usable format (e.g., string, JSON, specific data structures).\n",
    "* **Chains (`Runnable`s)**: Sequences of components (prompts, models, parsers, etc.) linked together to perform a specific task. LangChain's \"Runnable\" protocol makes these components easily composable using the `|` operator.\n",
    "* **LangSmith**: An observability and evaluation platform for LLM applications. It helps in debugging, testing, and monitoring your LangChain applications by providing detailed traces of each step.\n",
    "* **LangServe**: A library for deploying LangChain runnables as REST APIs, making it easy to expose your LLM applications.\n",
    "\n",
    "## Setup: Environment Variables\n",
    "\n",
    "We will load API keys for OpenAI and LangChain (LangSmith) from a `.env` file for security and ease of management. Ensure you have your `.env` file correctly set up as described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file.\n",
    "# This makes environment variables defined in .env accessible in the script.\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key from environment variables.\n",
    "# This is crucial for authentication with OpenAI's models.\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# --- LangSmith Tracking Setup ---\n",
    "# Set LangChain API key for LangSmith.\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "# Enable LangSmith tracing. Setting this to \"true\" automatically sends traces to LangSmith.\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# Define the project name under which traces will be organized in LangSmith.\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "# Import ChatOpenAI from LangChain's OpenAI integration\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the ChatOpenAI model.\n",
    "# We're using \"gpt-4o\" which is a powerful, multimodal model.\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Print the LLM object to confirm its initialization.\n",
    "print(\"--- Initialized LLM ---\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the LLM Directly\n",
    "\n",
    "The simplest way to interact with an LLM is to invoke it directly with a prompt string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Direct LLM Invocation Result ---\n",
      "content='Generative AI refers to a category of artificial intelligence systems designed to generate new content or data. This involves creating models that can produce text, images, music, or other media based on input data they have been trained on. Generative AI utilizes machine learning techniques, particularly deep learning, to understand patterns in the input data and create novel outputs that resemble the training data.\\n\\nOne of the most well-known types of generative AI models are Generative Adversarial Networks (GANs), which consist of two competing networks—a generator that creates new data instances, and a discriminator that evaluates them for authenticity. Other notable techniques include Variational Autoencoders (VAEs) and transformer models like GPT (Generative Pre-trained Transformer), which excel in generating human-like text.\\n\\nGenerative AI has numerous applications, including creating realistic images, generating art, composing music, writing articles, building virtual environments, and even assisting in drug discovery. However, it also raises ethical concerns regarding authenticity, copyright, and the potential for misuse in creating misleading information or deepfakes.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 211, 'prompt_tokens': 13, 'total_tokens': 224, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BosjljHjuC9eVckJFngAVN3MO6aJV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--2a65a777-4833-40da-85aa-dfd740808af2-0' usage_metadata={'input_tokens': 13, 'output_tokens': 211, 'total_tokens': 224, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "--- Content of the LLM Response ---\n",
      "Generative AI refers to a category of artificial intelligence systems designed to generate new content or data. This involves creating models that can produce text, images, music, or other media based on input data they have been trained on. Generative AI utilizes machine learning techniques, particularly deep learning, to understand patterns in the input data and create novel outputs that resemble the training data.\n",
      "\n",
      "One of the most well-known types of generative AI models are Generative Adversarial Networks (GANs), which consist of two competing networks—a generator that creates new data instances, and a discriminator that evaluates them for authenticity. Other notable techniques include Variational Autoencoders (VAEs) and transformer models like GPT (Generative Pre-trained Transformer), which excel in generating human-like text.\n",
      "\n",
      "Generative AI has numerous applications, including creating realistic images, generating art, composing music, writing articles, building virtual environments, and even assisting in drug discovery. However, it also raises ethical concerns regarding authenticity, copyright, and the potential for misuse in creating misleading information or deepfakes.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the LLM with a simple question.\n",
    "# The 'invoke' method sends the prompt to the LLM and returns its response.\n",
    "result = llm.invoke(\"What is generative AI?\")\n",
    "\n",
    "# Print the entire response object received from the LLM.\n",
    "# This object contains not just the content but also metadata like token usage, stop reasons, etc.\n",
    "print(\"--- Direct LLM Invocation Result ---\")\n",
    "print(result)\n",
    "\n",
    "# Accessing just the content of the response.\n",
    "print(\"\\n--- Content of the LLM Response ---\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Prompt Template\n",
    "\n",
    "For more structured and robust interactions, especially with chat models, `ChatPromptTemplate` is used. It allows you to define messages with specific roles (system, user, assistant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chat Prompt Template ---\n",
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Import ChatPromptTemplate for creating structured chat prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a ChatPromptTemplate.\n",
    "# It consists of a list of message tuples: (role, content).\n",
    "# \"{input}\" is a placeholder that will be filled in when the prompt is invoked.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the prompt template object.\n",
    "print(\"--- Chat Prompt Template ---\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Chain (Prompt | Model)\n",
    "\n",
    "LangChain's core power lies in chaining components together. Here, we create a simple chain by piping the `prompt` to the `llm` using the `|` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chain Invocation Result (Raw Response) ---\n",
      "content='Langsmith is a tool associated with LangChain, providing capabilities for tracing and debugging in the context of working with language models. It allows developers to better understand and refine the interactions between components in applications that use large language models. By visualizing and analyzing how data flows through the system, Langsmith helps identify inefficiencies, optimize performance, and ensure that the application behaves as expected. Langsmith’s features make it a useful tool for developers looking to enhance the reliability and efficiency of language model applications.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 33, 'total_tokens': 132, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BoslL5WC0fWoe0sCansc0HA5IreUd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--756895a3-9191-41d6-b875-72f96f77331d-0' usage_metadata={'input_tokens': 33, 'output_tokens': 99, 'total_tokens': 132, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Type of response: <class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "# Create a chain by piping the prompt template to the LLM.\n",
    "# This means the output of the prompt (formatted messages) becomes the input to the LLM.\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke the chain with the input for the placeholder defined in the prompt.\n",
    "# The result is a standard LLM response object.\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "\n",
    "# Print the full response object.\n",
    "print(\"--- Chain Invocation Result (Raw Response) ---\")\n",
    "print(response)\n",
    "\n",
    "# Print the type of the response, which is typically a LangChain message object.\n",
    "print(f\"\\nType of response: {type(response)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an Output Parser (StrOutputParser)\n",
    "\n",
    "LLMs return full response objects, which include content and metadata. Often, you just need the generated text content. `StrOutputParser` extracts this content as a simple string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chain Invocation Result (String Output) ---\n",
      "Langsmith is a suite of developer tools created by LangChain, designed to enhance the development, testing, and deployment of applications built with large language models (LLMs). It offers features for tracing, debugging, testing, monitoring, and evaluating LLM-based applications. Langsmith helps developers understand how their language model applications work, by providing insights and detailed analytics about application behavior.\n",
      "\n",
      "This toolset is particularly useful for businesses and developers who need to train and run their own custom models or applications using LLM services. Langsmith emphasizes ensuring these applications are reliable, performant, and capable of delivering consistent results. It also includes an evaluation framework with both quantitative and qualitative measures to check the performance of LLMs or chains, focused on producing effective and accurate outcomes.\n",
      "\n",
      "Type of response: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Import StrOutputParser to extract the string content from LLM responses\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the string output parser.\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create a new chain: prompt -> llm -> output_parser.\n",
    "# The output parser now processes the LLM's response to give a simple string.\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Invoke the new chain with the same input.\n",
    "# The 'response' variable will now directly contain the string output.\n",
    "response_string = chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "\n",
    "# Print the response, which is now a plain string.\n",
    "print(\"--- Chain Invocation Result (String Output) ---\")\n",
    "print(response_string)\n",
    "\n",
    "# Verify the type of the response.\n",
    "print(f\"\\nType of response: {type(response_string)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-ai-llama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
