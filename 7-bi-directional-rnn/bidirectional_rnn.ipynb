{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Recurrent Neural Networks (BiRNNs): Leveraging Future Context\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This Jupyter Notebook provides a detailed explanation of Bidirectional Recurrent Neural Networks (BiRNNs), building upon previous discussions of simple RNNs, LSTMs, and GRUs. We'll explore why BiRNNs are necessary, their architectural differences, and how they overcome the limitations of unidirectional models by incorporating both past and future context for sequence understanding and prediction.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1.  **Recap of Previous Concepts**\n",
    "2.  **Introduction to Bidirectional RNNs (BiRNNs)**\n",
    "3.  **Types of RNN Architectures (Input-Output Relationships)**\n",
    "    * One-to-One\n",
    "    * One-to-Many\n",
    "    * Many-to-One\n",
    "    * Many-to-Many\n",
    "4.  **Motivation for BiRNNs: The Need for Future Context**\n",
    "5.  **Architecture of Bidirectional RNNs**\n",
    "6.  **Advantages of BiRNNs**\n",
    "7.  **Assignment: Deriving Forward Propagation Equations**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recap of Previous Concepts\n",
    "\n",
    "Before diving into Bidirectional RNNs, let's briefly revisit what we've covered:\n",
    "\n",
    "* **Simple RNNs**: The foundational concept of recurrent neural networks, designed to process sequential data by maintaining a hidden state that carries information from previous time steps. We've seen their basic structure, unfolding over time, and a full practical implementation including deployment.\n",
    "* **Embedding Layer**: A crucial component in NLP that transforms discrete word tokens into dense, continuous vector representations (embeddings). This allows the model to capture semantic relationships between words.\n",
    "* **LSTM and GRU Variants**: We discussed the limitations of simple RNNs (like vanishing/exploding gradients and difficulty capturing long-term dependencies) and introduced LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) as more sophisticated variants that use \"gates\" to control information flow and mitigate these issues. Practical implementations of these variants were also explored.\n",
    "\n",
    "This lecture now moves to another important variant: **Bidirectional RNNs**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introduction to Bidirectional RNNs (BiRNNs)\n",
    "\n",
    "A Bidirectional RNN (BiRNN) is an extension of the traditional (unidirectional) RNN architecture. While it can theoretically be built with simple RNN cells, it's most commonly implemented using more advanced recurrent units like **LSTMs** (resulting in **Bidirectional LSTMs or BiLSTMs**) or **GRUs** (resulting in **Bidirectional GRUs or BiGRUs**).\n",
    "\n",
    "The core idea behind bidirectionality is to allow the model to process sequential information not only from past to future but also from future to past.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Types of RNN Architectures (Input-Output Relationships)\n",
    "\n",
    "Before delving into the \"why\" of BiRNNs, it's helpful to understand the different ways RNNs can be configured based on their input and output structures. These \"types\" of RNNs categorize problem statements:\n",
    "\n",
    "* ### **One-to-One RNN**\n",
    "    * **Description**: A single input produces a single output.\n",
    "    * **Analogy**: This is like a traditional feedforward neural network.\n",
    "    * **Example**: Image classification (input: one image, output: one class label).\n",
    "\n",
    "* ### **One-to-Many RNN**\n",
    "    * **Description**: A single input produces multiple outputs in sequence.\n",
    "    * **Example**:\n",
    "        * **Image Captioning**: Input is a single image, and the output is a sequence of words forming a descriptive caption (e.g., \"A dog and a cat playing in the grass\").\n",
    "        * **Music Generation**: Input could be a single seed note or a genre, and the output is a sequence of notes forming a melody.\n",
    "\n",
    "* ### **Many-to-One RNN**\n",
    "    * **Description**: Multiple sequential inputs produce a single output.\n",
    "    * **Example**:\n",
    "        * **Sentiment Analysis**: Input is a sequence of words (a sentence or paragraph), and the output is a single sentiment label (e.g., positive, negative, neutral).\n",
    "        * **Text Classification**: Input is a document, and the output is its category.\n",
    "        * **Image Search (Text Query)**: Input is a sequence of words (e.g., \"a cat eating food\"), and the output is a relevant image.\n",
    "\n",
    "* ### **Many-to-Many RNN**\n",
    "    * **Description**: Multiple sequential inputs produce multiple sequential outputs. This category has two common sub-types:\n",
    "        * **Equal Length (Synchronous)**: Input and output sequences have the same length, with an output produced at each time step.\n",
    "            * **Example**: **Video Classification (frame-by-frame)**: Input is a sequence of video frames, and the output at each time step is the classification of that specific frame.\n",
    "        * **Unequal Length (Asynchronous / Sequence-to-Sequence)**: Input and output sequences can have different lengths.\n",
    "            * **Example**: **Machine Translation**: Input is a sentence in one language, and the output is its translation in another language (e.g., \"Je suis\" $\\rightarrow$ \"I am\"). The number of words in the input and output can differ.\n",
    "\n",
    "Our focus for understanding BiRNNs will primarily be on tasks where outputs are generated at each time step, often falling under the \"Many-to-Many (Equal Length)\" category, or any task where understanding the full context (past and future) of an input element is crucial for its corresponding output.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Motivation for BiRNNs: The Need for Future Context\n",
    "\n",
    "Let's consider a practical problem that highlights the limitation of unidirectional RNNs (including LSTMs and GRUs):\n",
    "\n",
    "**Problem Example:** Predicting a missing word in a sentence.\n",
    "\n",
    "Sentence: \"Krish eats **[BLANK]** in Bangalore.\"\n",
    "\n",
    "If we use a standard, unidirectional RNN (which processes text from left to right):\n",
    "* When the RNN processes \"Krish\", then \"eats\", it tries to predict the blank word.\n",
    "* However, its prediction is only based on the preceding words (\"Krish eats\"). It has no information about the words that come *after* the blank (\"in Bangalore\").\n",
    "\n",
    "**The Critical Flaw:** The word \"Bangalore\" is crucial for predicting the blank word. If the sentence were \"Krish eats **[BLANK]** in Paris,\" the predicted word might change (e.g., \"dosa\" for Bangalore, \"pizza\" for Paris). A unidirectional RNN cannot account for this \"future\" context.\n",
    "\n",
    "This problem applies to many NLP tasks where the meaning of a word, or the prediction related to it, depends not only on the words that came before it but also on the words that come after it in the sequence. Examples include:\n",
    "\n",
    "* **Named Entity Recognition (NER)**: Identifying names of persons, organizations, locations. The word \"Spring\" might be a season or a town, depending on the words that follow it.\n",
    "* **Part-of-Speech (POS) Tagging**: \"Read\" can be a verb (present tense) or past tense depending on context.\n",
    "* **Machine Translation**: Understanding the full context of a word in the source sentence requires looking both ways to get an accurate translation.\n",
    "\n",
    "**Solution:** Bidirectional RNNs are designed to address this exact limitation by allowing the model to incorporate information from both directions of the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Architecture of Bidirectional RNNs\n",
    "\n",
    "A Bidirectional RNN consists of two separate and independent recurrent networks (RNN, LSTM, or GRU cells) processing the input sequence:\n",
    "\n",
    "1.  **Forward RNN**: This network processes the input sequence from left-to-right (from $X_1$ to $X_N$). It computes a sequence of forward hidden states: $\\vec{h}_1, \\vec{h}_2, ..., \\vec{h}_N$.\n",
    "2.  **Backward RNN**: This network processes the *same* input sequence, but from right-to-left (from $X_N$ to $X_1$). It computes a sequence of backward hidden states: $\\overleftarrow{h}_N, \\overleftarrow{h}_{N-1}, ..., \\overleftarrow{h}_1$.\n",
    "\n",
    "**How they combine to form the output:**\n",
    "\n",
    "* At each time step $t$, the hidden state from the forward RNN ($\\vec{h}_t$) and the hidden state from the backward RNN ($\\overleftarrow{h}_t$) are combined. The most common way to combine them is by **concatenation**:\n",
    "    $h_t = [\\vec{h}_t; \\overleftarrow{h}_t]$\n",
    "* This combined hidden state ($h_t$) then contains information about both the past context (from $\\vec{h}_t$) and the future context (from $\\overleftarrow{h}_t$) relative to the current time step $t$.\n",
    "* This combined hidden state ($h_t$) is then fed into an output layer (e.g., a Dense layer with softmax for classification) to produce the output for that time step ($Y_t$).\n",
    "\n",
    "**Visual Representation (Conceptual):**\n",
    "\n",
    "Input:    X1 ----> X2 ----> X3 ----> X4 ----> X5 (Sentence: Krish eats _ in Bangalore)\n",
    "\n",
    "Forward RNN:  (h_f1)--> (h_f2)--> (h_f3)--> (h_f4)--> (h_f5)\n",
    "/       /       /       /       /\n",
    "/       /       /       /       /\n",
    "Backward RNN: (h_b1) <-- (h_b2) <-- (h_b3) <-- (h_b4) <-- (h_b5)\n",
    "|         |         |         |         |\n",
    "V         V         V         V         V\n",
    "Combined:     [h_f1;h_b1] [h_f2;h_b2] [h_f3;h_b3] [h_f4;h_b4] [h_f5;h_b5]\n",
    "|         |         |         |         |\n",
    "V         V         V         V         V\n",
    "Output:       Y1        Y2        Y3        Y4        Y5\n",
    "\n",
    "\n",
    "```python\n",
    "# Bidirectional Recurrent Neural Networks (BiRNNs): Leveraging Future Context\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This Jupyter Notebook provides a detailed explanation of Bidirectional Recurrent Neural Networks (BiRNNs), building upon previous discussions of simple RNNs, LSTMs, and GRUs. We'll explore why BiRNNs are necessary, their architectural differences, and how they overcome the limitations of unidirectional models by incorporating both past and future context for sequence understanding and prediction.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1.  **Recap of Previous Concepts**\n",
    "2.  **Introduction to Bidirectional RNNs (BiRNNs)**\n",
    "3.  **Types of RNN Architectures (Input-Output Relationships)**\n",
    "    * One-to-One\n",
    "    * One-to-Many\n",
    "    * Many-to-One\n",
    "    * Many-to-Many\n",
    "4.  **Motivation for BiRNNs: The Need for Future Context**\n",
    "5.  **Architecture of Bidirectional RNNs**\n",
    "6.  **Advantages of BiRNNs**\n",
    "7.  **Assignment: Deriving Forward Propagation Equations**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recap of Previous Concepts\n",
    "\n",
    "Before diving into Bidirectional RNNs, let's briefly revisit what we've covered:\n",
    "\n",
    "* **Simple RNNs**: The foundational concept of recurrent neural networks, designed to process sequential data by maintaining a hidden state that carries information from previous time steps. We've seen their basic structure, unfolding over time, and a full practical implementation including deployment.\n",
    "* **Embedding Layer**: A crucial component in NLP that transforms discrete word tokens into dense, continuous vector representations (embeddings). This allows the model to capture semantic relationships between words.\n",
    "* **LSTM and GRU Variants**: We discussed the limitations of simple RNNs (like vanishing/exploding gradients and difficulty capturing long-term dependencies) and introduced LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) as more sophisticated variants that use \"gates\" to control information flow and mitigate these issues. Practical implementations of these variants were also explored.\n",
    "\n",
    "This lecture now moves to another important variant: **Bidirectional RNNs**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introduction to Bidirectional RNNs (BiRNNs)\n",
    "\n",
    "A Bidirectional RNN (BiRNN) is an extension of the traditional (unidirectional) RNN architecture. While it can theoretically be built with simple RNN cells, it's most commonly implemented using more advanced recurrent units like **LSTMs** (resulting in **Bidirectional LSTMs or BiLSTMs**) or **GRUs** (resulting in **Bidirectional GRUs or BiGRUs**).\n",
    "\n",
    "The core idea behind bidirectionality is to allow the model to process sequential information not only from past to future but also from future to past.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Types of RNN Architectures (Input-Output Relationships)\n",
    "\n",
    "Before delving into the \"why\" of BiRNNs, it's helpful to understand the different ways RNNs can be configured based on their input and output structures. These \"types\" of RNNs categorize problem statements:\n",
    "\n",
    "* ### **One-to-One RNN**\n",
    "    * **Description**: A single input produces a single output.\n",
    "    * **Analogy**: This is like a traditional feedforward neural network.\n",
    "    * **Example**: Image classification (input: one image, output: one class label).\n",
    "\n",
    "* ### **One-to-Many RNN**\n",
    "    * **Description**: A single input produces multiple outputs in sequence.\n",
    "    * **Example**:\n",
    "        * **Image Captioning**: Input is a single image, and the output is a sequence of words forming a descriptive caption (e.g., \"A dog and a cat playing in the grass\").\n",
    "        * **Music Generation**: Input could be a single seed note or a genre, and the output is a sequence of notes forming a melody.\n",
    "\n",
    "* ### **Many-to-One RNN**\n",
    "    * **Description**: Multiple sequential inputs produce a single output.\n",
    "    * **Example**:\n",
    "        * **Sentiment Analysis**: Input is a sequence of words (a sentence or paragraph), and the output is a single sentiment label (e.g., positive, negative, neutral).\n",
    "        * **Text Classification**: Input is a document, and the output is its category.\n",
    "        * **Image Search (Text Query)**: Input is a sequence of words (e.g., \"a cat eating food\"), and the output is a relevant image.\n",
    "\n",
    "* ### **Many-to-Many RNN**\n",
    "    * **Description**: Multiple sequential inputs produce multiple sequential outputs. This category has two common sub-types:\n",
    "        * **Equal Length (Synchronous)**: Input and output sequences have the same length, with an output produced at each time step.\n",
    "            * **Example**: **Video Classification (frame-by-frame)**: Input is a sequence of video frames, and the output at each time step is the classification of that specific frame.\n",
    "        * **Unequal Length (Asynchronous / Sequence-to-Sequence)**: Input and output sequences can have different lengths.\n",
    "            * **Example**: **Machine Translation**: Input is a sentence in one language, and the output is its translation in another language (e.g., \"Je suis\" $\\rightarrow$ \"I am\"). The number of words in the input and output can differ.\n",
    "\n",
    "Our focus for understanding BiRNNs will primarily be on tasks where outputs are generated at each time step, often falling under the \"Many-to-Many (Equal Length)\" category, or any task where understanding the full context (past and future) of an input element is crucial for its corresponding output.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Motivation for BiRNNs: The Need for Future Context\n",
    "\n",
    "Let's consider a practical problem that highlights the limitation of unidirectional RNNs (including LSTMs and GRUs):\n",
    "\n",
    "**Problem Example:** Predicting a missing word in a sentence.\n",
    "\n",
    "Sentence: \"Krish eats **[BLANK]** in Bangalore.\"\n",
    "\n",
    "If we use a standard, unidirectional RNN (which processes text from left to right):\n",
    "* When the RNN processes \"Krish\", then \"eats\", it tries to predict the blank word.\n",
    "* However, its prediction is only based on the preceding words (\"Krish eats\"). It has no information about the words that come *after* the blank (\"in Bangalore\").\n",
    "\n",
    "**The Critical Flaw:** The word \"Bangalore\" is crucial for predicting the blank word. If the sentence were \"Krish eats **[BLANK]** in Paris,\" the predicted word might change (e.g., \"dosa\" for Bangalore, \"pizza\" for Paris). A unidirectional RNN cannot account for this \"future\" context.\n",
    "\n",
    "This problem applies to many NLP tasks where the meaning of a word, or the prediction related to it, depends not only on the words that came before it but also on the words that come after it in the sequence. Examples include:\n",
    "\n",
    "* **Named Entity Recognition (NER)**: Identifying names of persons, organizations, locations. The word \"Spring\" might be a season or a town, depending on the words that follow it.\n",
    "* **Part-of-Speech (POS) Tagging**: \"Read\" can be a verb (present tense) or past tense depending on context.\n",
    "* **Machine Translation**: Understanding the full context of a word in the source sentence requires looking both ways to get an accurate translation.\n",
    "\n",
    "**Solution:** Bidirectional RNNs are designed to address this exact limitation by allowing the model to incorporate information from both directions of the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Architecture of Bidirectional RNNs\n",
    "\n",
    "A Bidirectional RNN consists of two separate and independent recurrent networks (RNN, LSTM, or GRU cells) processing the input sequence:\n",
    "\n",
    "1.  **Forward RNN**: This network processes the input sequence from left-to-right (from $X_1$ to $X_N$). It computes a sequence of forward hidden states: $\\vec{h}_1, \\vec{h}_2, ..., \\vec{h}_N$.\n",
    "2.  **Backward RNN**: This network processes the *same* input sequence, but from right-to-left (from $X_N$ to $X_1$). It computes a sequence of backward hidden states: $\\overleftarrow{h}_N, \\overleftarrow{h}_{N-1}, ..., \\overleftarrow{h}_1$.\n",
    "\n",
    "**How they combine to form the output:**\n",
    "\n",
    "* At each time step $t$, the hidden state from the forward RNN ($\\vec{h}_t$) and the hidden state from the backward RNN ($\\overleftarrow{h}_t$) are combined. The most common way to combine them is by **concatenation**:\n",
    "    $h_t = [\\vec{h}_t; \\overleftarrow{h}_t]$\n",
    "* This combined hidden state ($h_t$) then contains information about both the past context (from $\\vec{h}_t$) and the future context (from $\\overleftarrow{h}_t$) relative to the current time step $t$.\n",
    "* This combined hidden state ($h_t$) is then fed into an output layer (e.g., a Dense layer with softmax for classification) to produce the output for that time step ($Y_t$).\n",
    "\n",
    "**Visual Representation (Conceptual):**\n",
    "\n",
    "```\n",
    "\n",
    "Input:    X1 ----\\> X2 ----\\> X3 ----\\> X4 ----\\> X5 (Sentence: Krish eats \\_ in Bangalore)\n",
    "\n",
    "Forward RNN:  (h\\_f1)--\\> (h\\_f2)--\\> (h\\_f3)--\\> (h\\_f4)--\\> (h\\_f5)\n",
    "/       /       /       /       /\n",
    "/       /       /       /       /\n",
    "Backward RNN: (h\\_b1) \\<-- (h\\_b2) \\<-- (h\\_b3) \\<-- (h\\_b4) \\<-- (h\\_b5)\n",
    "|         |         |         |         |\n",
    "V         V         V         V         V\n",
    "Combined:     [h\\_f1;h\\_b1] [h\\_f2;h\\_b2] [h\\_f3;h\\_b3] [h\\_f4;h\\_b4] [h\\_f5;h\\_b5]\n",
    "|         |         |         |         |\n",
    "V         V         V         V         V\n",
    "Output:       Y1        Y2        Y3        Y4        Y5\n",
    "\n",
    "```\n",
    "\n",
    "For our \"Krish eats **[BLANK]** in Bangalore\" example, when predicting `Y3` (for the blank word):\n",
    "* The forward RNN's hidden state ($\\vec{h}_3$) captures context from \"Krish eats\".\n",
    "* The backward RNN's hidden state ($\\overleftarrow{h}_3$) captures context from \"in Bangalore\" (since it processed \"Bangalore\" then \"in\" to reach this point).\n",
    "* By concatenating $\\vec{h}_3$ and $\\overleftarrow{h}_3$, the model has the full context to accurately predict \"dosa\" (if trained on such examples).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Advantages of BiRNNs\n",
    "\n",
    "The primary advantages of Bidirectional RNNs stem directly from their ability to process sequences in both directions:\n",
    "\n",
    "* **Comprehensive Context**: They capture a richer representation of each element in the sequence by considering both its past and future context. This is crucial for tasks where context from both sides is necessary for accurate understanding or prediction.\n",
    "* **Improved Performance**: For many sequence labeling and prediction tasks (like NER, POS tagging, machine translation, speech recognition), BiRNNs consistently outperform their unidirectional counterparts because they have more complete information.\n",
    "* **Robustness**: By having two independent paths, BiRNNs can be more robust to noise or ambiguities that might only be resolved by looking ahead or behind.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Assignment: Deriving Forward Propagation Equations\n",
    "\n",
    "To solidify your understanding of Bidirectional RNNs, an assignment is given:\n",
    "\n",
    "**Task**: Derive the **forward propagation equations** for a Bidirectional RNN (you can choose simple RNN, LSTM, or GRU cells, but focusing on simple RNN cells might be a good starting point for understanding the concatenation).\n",
    "\n",
    "**Considerations**:\n",
    "* How is the hidden state for the forward pass ($\\vec{h}_t$) calculated?\n",
    "* How is the hidden state for the backward pass ($\\overleftarrow{h}_t$) calculated? (Remember the inputs are processed in reverse order for this path).\n",
    "* How are these two hidden states combined to form the final hidden state ($h_t$) at time step $t$?\n",
    "* How is the final output ($Y_t$) at time step $t$ derived from $h_t$?\n",
    "\n",
    "**Hint**: Look into research papers and resources that discuss the mathematical formulations of BiRNNs. Pay close attention to how weights are applied for each direction and how the hidden states are combined. This exercise will deepen your understanding of the underlying mechanics.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Bidirectional RNNs are a powerful architectural enhancement that addresses the limitations of unidirectional models by enabling the network to learn dependencies from both past and future contexts within a sequence. This comprehensive contextual understanding leads to significantly improved performance on a wide range of NLP and sequence-based tasks.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
